{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T09:51:06.442277Z",
     "start_time": "2025-03-18T09:51:06.439829Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from itertools import product\n",
    "import joblib"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:09.709607Z",
     "start_time": "2025-03-18T09:02:09.707217Z"
    }
   },
   "cell_type": "code",
   "source": "path = '/Users/binit/PycharmProjects/ML_AIT_A3/pythonProject/ProjectA3/Data/Out_287.csv'",
   "id": "fe516404610dee9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:11.979855Z",
     "start_time": "2025-03-18T09:02:11.970803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(path)\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ],
   "id": "3237e24f1c4f9f4b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T04:03:54.418706Z",
     "start_time": "2025-03-18T04:03:54.413295Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(5)",
   "id": "42d328e3b5614aa1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  brand  year  selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0   BMW  2017        3200000      13663  Diesel      Dealer    Automatic   \n",
       "1   BMW  2012         900000     155000  Diesel  Individual    Automatic   \n",
       "2   BMW  2017        2950000      39000  Diesel      Dealer    Automatic   \n",
       "3   BMW  2009        1100000      60000  Diesel  Individual    Automatic   \n",
       "4   BMW  2012        1100000      80000  Diesel  Individual    Automatic   \n",
       "\n",
       "   owner  mileage  engine  max_power  seats  \n",
       "0      1    22.69  1995.0     190.00    5.0  \n",
       "1      2    16.07  1995.0     181.00    4.0  \n",
       "2      1    19.59  1995.0     187.74    5.0  \n",
       "3      2    16.07  1995.0     181.00    5.0  \n",
       "4      2    16.07  1995.0     181.00    5.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>3200000</td>\n",
       "      <td>13663</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>1</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>900000</td>\n",
       "      <td>155000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>2950000</td>\n",
       "      <td>39000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>1</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2009</td>\n",
       "      <td>1100000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>1100000</td>\n",
       "      <td>80000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:14.632851Z",
     "start_time": "2025-03-18T09:02:14.617628Z"
    }
   },
   "cell_type": "code",
   "source": "df.describe()",
   "id": "1590c97e91bade66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              year  selling_price      km_driven        owner      mileage  \\\n",
       "count  6663.000000   6.663000e+03    6663.000000  6663.000000  6663.000000   \n",
       "mean   2013.588924   5.243708e+05   73044.716644     1.499475    19.508243   \n",
       "std       3.898769   5.090416e+05   48214.360087     0.733034     3.913635   \n",
       "min    1994.000000   2.999900e+04    1000.000000     1.000000     9.000000   \n",
       "25%    2011.000000   2.500000e+05   38000.000000     1.000000    16.800000   \n",
       "50%    2014.000000   4.200000e+05   69123.000000     1.000000    19.500000   \n",
       "75%    2017.000000   6.500000e+05  100000.000000     2.000000    22.540000   \n",
       "max    2020.000000   1.000000e+07  500000.000000     4.000000    42.000000   \n",
       "\n",
       "            engine    max_power        seats  \n",
       "count  6663.000000  6663.000000  6663.000000  \n",
       "mean   1433.287858    87.871039     5.442593  \n",
       "std     492.908839    31.622824     0.988814  \n",
       "min     624.000000    34.200000     4.000000  \n",
       "25%    1197.000000    68.000000     5.000000  \n",
       "50%    1248.000000    81.860000     5.000000  \n",
       "75%    1498.000000   100.000000     5.000000  \n",
       "max    3604.000000   400.000000    14.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6.663000e+03</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2013.588924</td>\n",
       "      <td>5.243708e+05</td>\n",
       "      <td>73044.716644</td>\n",
       "      <td>1.499475</td>\n",
       "      <td>19.508243</td>\n",
       "      <td>1433.287858</td>\n",
       "      <td>87.871039</td>\n",
       "      <td>5.442593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.898769</td>\n",
       "      <td>5.090416e+05</td>\n",
       "      <td>48214.360087</td>\n",
       "      <td>0.733034</td>\n",
       "      <td>3.913635</td>\n",
       "      <td>492.908839</td>\n",
       "      <td>31.622824</td>\n",
       "      <td>0.988814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1994.000000</td>\n",
       "      <td>2.999900e+04</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>624.000000</td>\n",
       "      <td>34.200000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2011.000000</td>\n",
       "      <td>2.500000e+05</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>1197.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>4.200000e+05</td>\n",
       "      <td>69123.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>1248.000000</td>\n",
       "      <td>81.860000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>6.500000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.540000</td>\n",
       "      <td>1498.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3604.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:17.762373Z",
     "start_time": "2025-03-18T09:02:17.759887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns_drop = ['seats','owner']\n",
    "df.drop(columns=columns_drop, inplace=True)"
   ],
   "id": "ff9f4f8c83e32d74",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:20.625219Z",
     "start_time": "2025-03-18T09:02:20.620330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selling_price = df['selling_price']  # Use your dataset\n",
    "bins = [0, 250000, 500000, 750000, df['selling_price'].max()]  # Define bin edges\n",
    "labels = [0, 1, 2, 3]  # Assign category labels\n",
    "\n",
    "df['price_category_bins'] = pd.cut(df['selling_price'], bins=bins, labels=labels, include_lowest=True)\n",
    "print(df['price_category_bins'].value_counts())"
   ],
   "id": "422e418174dd8fce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_category_bins\n",
      "1    2335\n",
      "0    1719\n",
      "2    1559\n",
      "3    1050\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:46:50.527987Z",
     "start_time": "2025-03-17T11:46:50.525130Z"
    }
   },
   "cell_type": "code",
   "source": "dff = df.loc[:, df.columns.drop('selling_price')]",
   "id": "6d62e0c30337da78",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:22.745596Z",
     "start_time": "2025-03-18T09:02:22.743532Z"
    }
   },
   "cell_type": "code",
   "source": "dff = df.copy()",
   "id": "ead44e63c474af2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:24.785790Z",
     "start_time": "2025-03-18T09:02:24.773268Z"
    }
   },
   "cell_type": "code",
   "source": "dff.query(\"selling_price > 750000 and selling_price < 10000000\").describe()",
   "id": "4a57110480872ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              year  selling_price      km_driven      mileage       engine  \\\n",
       "count  1049.000000   1.049000e+03    1049.000000  1049.000000  1049.000000   \n",
       "mean   2016.457579   1.287957e+06   56991.452812    18.200114  1877.421354   \n",
       "std       2.223307   8.239498e+05   44162.316900     4.508758   529.265577   \n",
       "min    2005.000000   7.510000e+05    1000.000000     9.000000   998.000000   \n",
       "25%    2015.000000   8.500000e+05   25000.000000    15.100000  1493.000000   \n",
       "50%    2017.000000   9.750000e+05   48000.000000    17.100000  1798.000000   \n",
       "75%    2018.000000   1.350000e+06   77000.000000    22.000000  2179.000000   \n",
       "max    2020.000000   7.200000e+06  426000.000000    28.400000  3604.000000   \n",
       "\n",
       "         max_power  \n",
       "count  1049.000000  \n",
       "mean    127.896273  \n",
       "std      39.454969  \n",
       "min      62.100000  \n",
       "25%      98.600000  \n",
       "50%     120.000000  \n",
       "75%     147.940000  \n",
       "max     282.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1.049000e+03</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1049.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2016.457579</td>\n",
       "      <td>1.287957e+06</td>\n",
       "      <td>56991.452812</td>\n",
       "      <td>18.200114</td>\n",
       "      <td>1877.421354</td>\n",
       "      <td>127.896273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.223307</td>\n",
       "      <td>8.239498e+05</td>\n",
       "      <td>44162.316900</td>\n",
       "      <td>4.508758</td>\n",
       "      <td>529.265577</td>\n",
       "      <td>39.454969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>7.510000e+05</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>62.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>8.500000e+05</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>98.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>9.750000e+05</td>\n",
       "      <td>48000.000000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>1.350000e+06</td>\n",
       "      <td>77000.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2179.000000</td>\n",
       "      <td>147.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>7.200000e+06</td>\n",
       "      <td>426000.000000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>3604.000000</td>\n",
       "      <td>282.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:26.671523Z",
     "start_time": "2025-03-18T09:02:26.657518Z"
    }
   },
   "cell_type": "code",
   "source": "dff[dff['price_category_bins']==3].describe()",
   "id": "9d5d22f9b51b4407",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              year  selling_price      km_driven      mileage       engine  \\\n",
       "count  1050.000000   1.050000e+03    1050.000000  1050.000000  1050.000000   \n",
       "mean   2016.458095   1.296254e+06   56965.746667    18.222781  1877.508571   \n",
       "std       2.222310   8.663322e+05   44149.120843     4.566068   529.020795   \n",
       "min    2005.000000   7.510000e+05    1000.000000     9.000000   998.000000   \n",
       "25%    2015.000000   8.500000e+05   25000.000000    15.100000  1493.750000   \n",
       "50%    2017.000000   9.750000e+05   48000.000000    17.100000  1877.000000   \n",
       "75%    2018.000000   1.350000e+06   76784.750000    22.000000  2179.000000   \n",
       "max    2020.000000   1.000000e+07  426000.000000    42.000000  3604.000000   \n",
       "\n",
       "         max_power  \n",
       "count  1050.000000  \n",
       "mean    128.155419  \n",
       "std      40.320284  \n",
       "min      62.100000  \n",
       "25%      98.600000  \n",
       "50%     120.000000  \n",
       "75%     147.985000  \n",
       "max     400.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1.050000e+03</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2016.458095</td>\n",
       "      <td>1.296254e+06</td>\n",
       "      <td>56965.746667</td>\n",
       "      <td>18.222781</td>\n",
       "      <td>1877.508571</td>\n",
       "      <td>128.155419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.222310</td>\n",
       "      <td>8.663322e+05</td>\n",
       "      <td>44149.120843</td>\n",
       "      <td>4.566068</td>\n",
       "      <td>529.020795</td>\n",
       "      <td>40.320284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>7.510000e+05</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>62.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>8.500000e+05</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>1493.750000</td>\n",
       "      <td>98.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>9.750000e+05</td>\n",
       "      <td>48000.000000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>1877.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>1.350000e+06</td>\n",
       "      <td>76784.750000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2179.000000</td>\n",
       "      <td>147.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>426000.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3604.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:59:41.835543Z",
     "start_time": "2025-03-18T03:59:41.832220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dff[\"km_driven\"] = np.log1p(dff[\"km_driven\"])  # \n",
    "dff[\"max_power\"] = np.log1p(dff[\"max_power\"])"
   ],
   "id": "4717c01c4f788c7e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:31.355873Z",
     "start_time": "2025-03-18T09:02:31.351148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the mean selling price for each brand\n",
    "brand_mean_price = dff.groupby(\"brand\")[\"selling_price\"].mean().to_dict()\n",
    "\n",
    "# Replace brand names with their average selling price\n",
    "dff[\"brand_encoded\"] = dff[\"brand\"].map(brand_mean_price)"
   ],
   "id": "cce84c653f501cd2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:37:48.123947Z",
     "start_time": "2025-03-18T09:37:48.119470Z"
    }
   },
   "cell_type": "code",
   "source": "dff.head(3)",
   "id": "b615f5be05c22efc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  brand  year  selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0   BMW  2017        3200000      13663  Diesel      Dealer    Automatic   \n",
       "1   BMW  2012         900000     155000  Diesel  Individual    Automatic   \n",
       "2   BMW  2017        2950000      39000  Diesel      Dealer    Automatic   \n",
       "\n",
       "   mileage  engine  max_power price_category_bins  brand_encoded  \n",
       "0    22.69  1995.0     190.00                   3   2.826222e+06  \n",
       "1    16.07  1995.0     181.00                   3   2.826222e+06  \n",
       "2    19.59  1995.0     187.74                   3   2.826222e+06  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>price_category_bins</th>\n",
       "      <th>brand_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>3200000</td>\n",
       "      <td>13663</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>900000</td>\n",
       "      <td>155000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>2950000</td>\n",
       "      <td>39000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T04:04:34.901619Z",
     "start_time": "2025-03-18T04:04:34.898421Z"
    }
   },
   "cell_type": "code",
   "source": "dff['seller_type'].value_counts()",
   "id": "a2e1a829ed36bf1d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seller_type\n",
       "Individual          5977\n",
       "Dealer               659\n",
       "Trustmark Dealer      27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:22:36.200351Z",
     "start_time": "2025-03-18T09:22:36.196324Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(2)",
   "id": "456392856378e8ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  brand  year  selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0   BMW  2017        3200000      13663  Diesel      Dealer    Automatic   \n",
       "1   BMW  2012         900000     155000  Diesel  Individual    Automatic   \n",
       "\n",
       "   mileage  engine  max_power price_category_bins  \n",
       "0    22.69  1995.0      190.0                   3  \n",
       "1    16.07  1995.0      181.0                   3  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>price_category_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>3200000</td>\n",
       "      <td>13663</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>900000</td>\n",
       "      <td>155000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:51:44.604973Z",
     "start_time": "2025-03-18T09:51:44.587774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, ytrain, ytest = train_test_split(\n",
    "    dff.drop(columns=['price_category_bins','selling_price', 'brand', 'fuel','mileage']),\n",
    "    dff['price_category_bins'],\n",
    "    test_size=0.2,\n",
    "    stratify=dff['price_category_bins'],  # Ensures equal class proportions\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "columns_to_encode = ['seller_type','transmission']\n",
    "scaler = MinMaxScaler()\n",
    "oh = OneHotEncoder(handle_unknown='ignore')\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', oh, columns_to_encode),  # One-hot encoding for categorical columns\n",
    "    ('scaling', scaler, ['year', 'km_driven','engine','max_power',  'brand_encoded'])  # Scaling numerical columns\n",
    "], remainder='passthrough')\n",
    "\n",
    "X_train_trf = preprocessor.fit_transform(X_train)\n",
    "X_test_trf = preprocessor.transform(X_test)\n"
   ],
   "id": "a64c4abe03b40890",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:37:54.164078Z",
     "start_time": "2025-03-18T09:37:54.162161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "joblib.dump(brand_mean_price, 'brand_means.pkl')\n",
    "print(\"brand_means.pkl has been saved.\")"
   ],
   "id": "c18f7dc51f0c739a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand_means.pkl has been saved.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:41.739805Z",
     "start_time": "2025-03-18T09:02:41.737496Z"
    }
   },
   "cell_type": "code",
   "source": "feature_names = ['seller_type_Individual', 'seller_type_Dealer','seller_type_Trustmark_Dealer','transmission_Automatic', 'transmission_Manual','year','km_driven','engine', 'max_power', 'brand_encoded']",
   "id": "49a803830c029a53",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:19:26.182351Z",
     "start_time": "2025-03-18T09:19:26.177421Z"
    }
   },
   "cell_type": "code",
   "source": "X_train",
   "id": "233a85b0c3b49e0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year  km_driven seller_type transmission  engine  max_power  \\\n",
       "4506  2008      80000  Individual       Manual  1086.0      63.00   \n",
       "2325  2012      81632      Dealer       Manual  1399.0      68.00   \n",
       "2404  2015     130000  Individual    Automatic  2982.0     168.50   \n",
       "5353  2012     123000  Individual       Manual  1197.0      85.80   \n",
       "943   2015      62000      Dealer       Manual  1498.0      74.96   \n",
       "...    ...        ...         ...          ...     ...        ...   \n",
       "5746  2012      60000  Individual       Manual  1598.0     103.60   \n",
       "5090  2018      15000  Individual       Manual  1197.0      81.80   \n",
       "3947  2012      50000  Individual    Automatic  2143.0     170.00   \n",
       "1174  2016      70000  Individual       Manual  1248.0      88.50   \n",
       "4605  2015      90000  Individual       Manual  2179.0     120.00   \n",
       "\n",
       "      brand_encoded  \n",
       "4506   4.720181e+05  \n",
       "2325   5.081614e+05  \n",
       "2404   9.094595e+05  \n",
       "5353   3.949493e+05  \n",
       "943    6.236416e+05  \n",
       "...             ...  \n",
       "5746   4.737470e+05  \n",
       "5090   3.949493e+05  \n",
       "3947   2.242136e+06  \n",
       "1174   3.949493e+05  \n",
       "4605   6.236416e+05  \n",
       "\n",
       "[5330 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>brand_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4506</th>\n",
       "      <td>2008</td>\n",
       "      <td>80000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>63.00</td>\n",
       "      <td>4.720181e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>2012</td>\n",
       "      <td>81632</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>5.081614e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>2015</td>\n",
       "      <td>130000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2982.0</td>\n",
       "      <td>168.50</td>\n",
       "      <td>9.094595e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>2012</td>\n",
       "      <td>123000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>85.80</td>\n",
       "      <td>3.949493e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>2015</td>\n",
       "      <td>62000</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>74.96</td>\n",
       "      <td>6.236416e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5746</th>\n",
       "      <td>2012</td>\n",
       "      <td>60000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>103.60</td>\n",
       "      <td>4.737470e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>2018</td>\n",
       "      <td>15000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>81.80</td>\n",
       "      <td>3.949493e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>2012</td>\n",
       "      <td>50000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>170.00</td>\n",
       "      <td>2.242136e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>2016</td>\n",
       "      <td>70000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>88.50</td>\n",
       "      <td>3.949493e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4605</th>\n",
       "      <td>2015</td>\n",
       "      <td>90000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>2179.0</td>\n",
       "      <td>120.00</td>\n",
       "      <td>6.236416e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5330 rows Ã— 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:51:55.974753Z",
     "start_time": "2025-03-18T09:51:55.972001Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_trf[:5]",
   "id": "9541244a59bc6344",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.53846154, 0.15831663, 0.15503356, 0.07873155, 0.09977443],\n",
       "       [1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.69230769, 0.16158717, 0.26006711, 0.09240022, 0.1089036 ],\n",
       "       [0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.80769231, 0.25851703, 0.79127517, 0.36714051, 0.21026424],\n",
       "       [0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.69230769, 0.24448898, 0.19228188, 0.14106069, 0.08030826],\n",
       "       [1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.80769231, 0.12224449, 0.29328859, 0.11142701, 0.1380718 ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:32:49.212038Z",
     "start_time": "2025-03-18T09:32:49.200635Z"
    }
   },
   "cell_type": "code",
   "source": "dff[dff['brand']=='BMW']",
   "id": "4352072bde6911d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     brand  year  selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0      BMW  2017        3200000      13663  Diesel      Dealer    Automatic   \n",
       "1      BMW  2012         900000     155000  Diesel  Individual    Automatic   \n",
       "2      BMW  2017        2950000      39000  Diesel      Dealer    Automatic   \n",
       "3      BMW  2009        1100000      60000  Diesel  Individual    Automatic   \n",
       "4      BMW  2012        1100000      80000  Diesel  Individual    Automatic   \n",
       "5      BMW  2016        2150000      30000  Diesel      Dealer    Automatic   \n",
       "6      BMW  2012        1300000     140000  Diesel  Individual    Automatic   \n",
       "7      BMW  2010        1100000     102000  Diesel      Dealer    Automatic   \n",
       "8      BMW  2017        2600000      44000  Diesel      Dealer    Automatic   \n",
       "9      BMW  2013        1300000     140000  Diesel  Individual    Automatic   \n",
       "10     BMW  2015        1650000      74000  Diesel  Individual    Automatic   \n",
       "11     BMW  2007         480000     110000  Diesel  Individual    Automatic   \n",
       "12     BMW  2011        1500000      84925  Diesel  Individual    Automatic   \n",
       "13     BMW  2016        2950000      60000  Diesel      Dealer    Automatic   \n",
       "14     BMW  2008        1100000     122000  Diesel  Individual    Automatic   \n",
       "15     BMW  2016        2900000      12000  Diesel      Dealer    Automatic   \n",
       "16     BMW  2019        5200000      10000  Diesel      Dealer    Automatic   \n",
       "17     BMW  2016        3200000      40000  Diesel  Individual    Automatic   \n",
       "18     BMW  2018        3790000      29500  Diesel      Dealer    Automatic   \n",
       "19     BMW  2018        3900000      17100  Diesel      Dealer    Automatic   \n",
       "20     BMW  2018        4000000      10000  Diesel      Dealer    Automatic   \n",
       "21     BMW  2009         975000      80000  Diesel      Dealer    Automatic   \n",
       "22     BMW  2010         975000      72200  Petrol      Dealer    Automatic   \n",
       "23     BMW  2010        1000000      60000  Diesel  Individual    Automatic   \n",
       "24     BMW  2013        2000000      60000  Diesel  Individual    Automatic   \n",
       "25     BMW  2018        6000000      28156  Diesel      Dealer    Automatic   \n",
       "26     BMW  2018        5500000      22000  Diesel  Individual    Automatic   \n",
       "27     BMW  2018        6000000      27000  Diesel      Dealer    Automatic   \n",
       "28     BMW  2018        5830000      30000  Diesel  Individual    Automatic   \n",
       "29     BMW  2007         750000      60000  Diesel  Individual    Automatic   \n",
       "30     BMW  2010        2000000      90000  Diesel  Individual    Automatic   \n",
       "6267   BMW  2018        2850000      27000  Diesel      Dealer    Automatic   \n",
       "6268   BMW  2011        1000000      50000  Diesel  Individual    Automatic   \n",
       "6269   BMW  2011        1075000      42545  Diesel      Dealer    Automatic   \n",
       "6270   BMW  2018        3350000      18000  Diesel      Dealer    Automatic   \n",
       "6271   BMW  2012         925000      96000  Diesel      Dealer    Automatic   \n",
       "6272   BMW  2016        2500000      30000  Petrol  Individual    Automatic   \n",
       "6273   BMW  2012        1800000      50000  Diesel  Individual    Automatic   \n",
       "6274   BMW  2019        5800000       7500  Diesel      Dealer    Automatic   \n",
       "6275   BMW  2019        5400000       7500  Diesel      Dealer    Automatic   \n",
       "6276   BMW  2019        5500000       8500  Diesel      Dealer    Automatic   \n",
       "6277   BMW  2019        5800000      21000  Diesel      Dealer    Automatic   \n",
       "6278   BMW  2008         830000      60000  Diesel  Individual    Automatic   \n",
       "6279   BMW  2013        3750000      56000  Diesel      Dealer    Automatic   \n",
       "6280   BMW  2020        7200000       5000  Diesel  Individual    Automatic   \n",
       "\n",
       "      mileage  engine  max_power price_category_bins  brand_encoded  \n",
       "0       22.69  1995.0     190.00                   3   2.826222e+06  \n",
       "1       16.07  1995.0     181.00                   3   2.826222e+06  \n",
       "2       19.59  1995.0     187.74                   3   2.826222e+06  \n",
       "3       16.07  1995.0     181.00                   3   2.826222e+06  \n",
       "4       16.07  1995.0     181.00                   3   2.826222e+06  \n",
       "5       22.69  1995.0     190.00                   3   2.826222e+06  \n",
       "6       18.88  1995.0     184.00                   3   2.826222e+06  \n",
       "7       19.62  1995.0     187.74                   3   2.826222e+06  \n",
       "8       22.69  1995.0     190.00                   3   2.826222e+06  \n",
       "9       18.88  1995.0     184.00                   3   2.826222e+06  \n",
       "10      18.88  1995.0     184.00                   3   2.826222e+06  \n",
       "11      16.07  1995.0     181.00                   1   2.826222e+06  \n",
       "12      16.07  1995.0     181.00                   3   2.826222e+06  \n",
       "13      21.76  1995.0     188.00                   3   2.826222e+06  \n",
       "14      18.48  1995.0     177.00                   3   2.826222e+06  \n",
       "15      18.12  1995.0     190.00                   3   2.826222e+06  \n",
       "16      18.12  1995.0     190.00                   3   2.826222e+06  \n",
       "17      18.12  1995.0     190.00                   3   2.826222e+06  \n",
       "18      22.48  1995.0     187.74                   3   2.826222e+06  \n",
       "19      22.48  1995.0     187.74                   3   2.826222e+06  \n",
       "20      22.48  1995.0     187.74                   3   2.826222e+06  \n",
       "21      22.48  1995.0     187.74                   3   2.826222e+06  \n",
       "22      10.80  2497.0     150.00                   3   2.826222e+06  \n",
       "23      16.73  1995.0     218.00                   3   2.826222e+06  \n",
       "24      16.20  2993.0     235.00                   3   2.826222e+06  \n",
       "25      17.09  2993.0     261.40                   3   2.826222e+06  \n",
       "26      17.09  2993.0     261.40                   3   2.826222e+06  \n",
       "27      17.09  2993.0     261.40                   3   2.826222e+06  \n",
       "28      17.09  2993.0     261.40                   3   2.826222e+06  \n",
       "29      14.49  2993.0     258.00                   2   2.826222e+06  \n",
       "30      14.49  2993.0     258.00                   3   2.826222e+06  \n",
       "6267    17.05  1995.0     184.00                   3   2.826222e+06  \n",
       "6268    17.05  1995.0     184.00                   3   2.826222e+06  \n",
       "6269    19.62  1998.0     190.00                   3   2.826222e+06  \n",
       "6270    19.62  1998.0     190.00                   3   2.826222e+06  \n",
       "6271    19.62  1998.0     190.00                   3   2.826222e+06  \n",
       "6272    15.71  1998.0     189.00                   3   2.826222e+06  \n",
       "6273    16.09  1995.0     184.00                   3   2.826222e+06  \n",
       "6274    16.78  1995.0     190.00                   3   2.826222e+06  \n",
       "6275    16.78  1995.0     190.00                   3   2.826222e+06  \n",
       "6276    16.78  1995.0     190.00                   3   2.826222e+06  \n",
       "6277    16.78  1995.0     190.00                   3   2.826222e+06  \n",
       "6278    11.00  2993.0     235.00                   3   2.826222e+06  \n",
       "6279    11.20  2993.0     241.00                   3   2.826222e+06  \n",
       "6280    13.38  2993.0     265.00                   3   2.826222e+06  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>price_category_bins</th>\n",
       "      <th>brand_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>3200000</td>\n",
       "      <td>13663</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>900000</td>\n",
       "      <td>155000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>2950000</td>\n",
       "      <td>39000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2009</td>\n",
       "      <td>1100000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>1100000</td>\n",
       "      <td>80000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2016</td>\n",
       "      <td>2150000</td>\n",
       "      <td>30000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>1300000</td>\n",
       "      <td>140000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>18.88</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>184.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2010</td>\n",
       "      <td>1100000</td>\n",
       "      <td>102000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>19.62</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>2600000</td>\n",
       "      <td>44000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2013</td>\n",
       "      <td>1300000</td>\n",
       "      <td>140000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>18.88</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>184.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1650000</td>\n",
       "      <td>74000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>18.88</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>184.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2007</td>\n",
       "      <td>480000</td>\n",
       "      <td>110000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2011</td>\n",
       "      <td>1500000</td>\n",
       "      <td>84925</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2016</td>\n",
       "      <td>2950000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>21.76</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>188.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2008</td>\n",
       "      <td>1100000</td>\n",
       "      <td>122000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>18.48</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>177.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2016</td>\n",
       "      <td>2900000</td>\n",
       "      <td>12000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>18.12</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2019</td>\n",
       "      <td>5200000</td>\n",
       "      <td>10000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>18.12</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2016</td>\n",
       "      <td>3200000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>18.12</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>3790000</td>\n",
       "      <td>29500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.48</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>3900000</td>\n",
       "      <td>17100</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.48</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>4000000</td>\n",
       "      <td>10000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.48</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2009</td>\n",
       "      <td>975000</td>\n",
       "      <td>80000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.48</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2010</td>\n",
       "      <td>975000</td>\n",
       "      <td>72200</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>10.80</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>150.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2010</td>\n",
       "      <td>1000000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.73</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>218.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2013</td>\n",
       "      <td>2000000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.20</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>235.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>6000000</td>\n",
       "      <td>28156</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>17.09</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>261.40</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>5500000</td>\n",
       "      <td>22000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>17.09</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>261.40</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>6000000</td>\n",
       "      <td>27000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>17.09</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>261.40</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>5830000</td>\n",
       "      <td>30000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>17.09</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>261.40</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2007</td>\n",
       "      <td>750000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>14.49</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>258.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2010</td>\n",
       "      <td>2000000</td>\n",
       "      <td>90000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>14.49</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>258.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6267</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>2850000</td>\n",
       "      <td>27000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>17.05</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>184.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2011</td>\n",
       "      <td>1000000</td>\n",
       "      <td>50000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>17.05</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>184.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6269</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2011</td>\n",
       "      <td>1075000</td>\n",
       "      <td>42545</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>19.62</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6270</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2018</td>\n",
       "      <td>3350000</td>\n",
       "      <td>18000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>19.62</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6271</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>925000</td>\n",
       "      <td>96000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>19.62</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6272</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2016</td>\n",
       "      <td>2500000</td>\n",
       "      <td>30000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>15.71</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>189.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6273</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>1800000</td>\n",
       "      <td>50000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.09</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>184.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6274</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2019</td>\n",
       "      <td>5800000</td>\n",
       "      <td>7500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.78</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6275</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2019</td>\n",
       "      <td>5400000</td>\n",
       "      <td>7500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.78</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6276</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2019</td>\n",
       "      <td>5500000</td>\n",
       "      <td>8500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.78</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6277</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2019</td>\n",
       "      <td>5800000</td>\n",
       "      <td>21000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.78</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6278</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2008</td>\n",
       "      <td>830000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>235.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6279</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2013</td>\n",
       "      <td>3750000</td>\n",
       "      <td>56000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>11.20</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>241.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6280</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2020</td>\n",
       "      <td>7200000</td>\n",
       "      <td>5000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>13.38</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>265.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:54:25.732024Z",
     "start_time": "2025-03-18T09:54:25.725983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained model\n",
    "joblib.dump(preprocessor, \"preprocessor.pkl\")\n",
    "print(\"Preprocessor saved successfully!\")"
   ],
   "id": "e685ed12d5dcdf4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor saved successfully!\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T04:00:40.565368Z",
     "start_time": "2025-03-18T04:00:40.463244Z"
    }
   },
   "cell_type": "code",
   "source": "sns.displot(data=dff, x='year', kde=True)",
   "id": "d01d7f532f00ce3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12bc5fc20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:13:44.312135Z",
     "start_time": "2025-03-16T18:13:44.196781Z"
    }
   },
   "cell_type": "code",
   "source": "sns.boxplot(data=df, x='selling_price')",
   "id": "8144e240f6c1ac91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='selling_price'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:02:50.838116Z",
     "start_time": "2025-03-18T09:02:50.809697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Customs logistic regression \n",
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self, cv:int=5, lr:float=0.1, \n",
    "                 max_iter:int=5000, weight_init:str='uniform', \n",
    "                 method:str='mini_batch', batch_size:int=64, l2:float=None,):\n",
    "        self.lr = lr\n",
    "        self.cv = cv\n",
    "        self.max_iter = max_iter\n",
    "        self.weight_init = weight_init\n",
    "        self.method = method\n",
    "        self.batch_size = batch_size\n",
    "        self.l2 = 0 if l2 is None else l2\n",
    "        \n",
    "        valid_weight_init = ['uniform', 'normal', 'xavier','ones']\n",
    "        \n",
    "        if weight_init not in valid_weight_init:\n",
    "            raise ValueError(f'weight_init must be one of {valid_weight_init}')\n",
    "        \n",
    "        valid_method = ['mini_batch', 'batch', 'stochastic']\n",
    "        \n",
    "        if method not in valid_method:\n",
    "            raise ValueError(f'method must be one of {valid_method}')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.split = KFold(n_splits=self.cv)\n",
    "        y_class = len(np.unique(y))\n",
    "\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()  # Convert sparse matrix to dense\n",
    "\n",
    "        # Ensure y is a NumPy array\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = y.to_numpy() if hasattr(y, \"to_numpy\") else np.array(y)\n",
    "        \n",
    "        # One-Hot Encode y\n",
    "        self.oh = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        y_encoded = self.oh.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "        self.losses = []\n",
    "        self.valid_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.valid_accuracies = []\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(self.split.split(X)):\n",
    "            X_train, X_val = X[train_index], X[test_index]\n",
    "            y_train, y_val = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "            X_train = self._add_intercept(X_train)\n",
    "            X_val = self._add_intercept(X_val)\n",
    "\n",
    "            self.W = self.weight_initializer(X_train, y_class)\n",
    "            self.velocity = np.zeros_like(self.W)\n",
    "\n",
    "            fold_train_losses = []\n",
    "            fold_train_accuracies = []\n",
    "\n",
    "            if self.method == 'mini_batch':\n",
    "                for i in range(self.max_iter):\n",
    "                    ix = np.random.randint(0, X_train.shape[0])\n",
    "                    X_train_batch = X_train[ix:ix + self.batch_size]\n",
    "                    y_train_batch = y_train[ix:ix + self.batch_size]\n",
    "                    loss = self.train(X_train_batch, y_train_batch)\n",
    "                    fold_train_losses.append(loss)\n",
    "\n",
    "                    _, train_pred = self.predict(X_train_batch)\n",
    "                    train_accuracy = np.mean(np.argmax(y_train_batch, axis=1) == train_pred)\n",
    "                    fold_train_accuracies.append(train_accuracy)\n",
    "\n",
    "                    if i % 500 == 0:\n",
    "                        print(f\"Iteration {i} - Loss: {loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            elif self.method == 'batch':\n",
    "                for i in range(self.max_iter):\n",
    "                    loss = self.train(X_train, y_train)\n",
    "                    fold_train_losses.append(loss)\n",
    "\n",
    "                    _, train_pred = self.predict(X_train)\n",
    "                    train_accuracy = np.mean(np.argmax(y_train, axis=1) == train_pred)\n",
    "                    fold_train_accuracies.append(train_accuracy)\n",
    "\n",
    "                    if i % 500 == 0:\n",
    "                        print(f\"Iteration {i} - Loss: {loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            elif self.method == 'stochastic':\n",
    "                for i in range(self.max_iter):\n",
    "                    idx = np.random.randint(X_train.shape[0])  # Select a random index\n",
    "                    X_sto = X_train[idx, :].reshape(1, -1)  # Get the single sample\n",
    "                    y_sto = y_train[idx].reshape(1, -1)  # Get the corresponding label\n",
    "                    \n",
    "                    loss = self.train(X_sto, y_sto)  # Train the model on this single example\n",
    "                    \n",
    "                    if not np.isnan(loss):  # Ensure the loss is valid\n",
    "                        fold_train_losses.append(loss)\n",
    "                    \n",
    "                    _, train_pred = self.predict(X_sto)  # Get predicted class\n",
    "                    train_accuracy = np.mean(np.argmax(y_sto, axis=1) == train_pred)\n",
    "                    fold_train_accuracies.append(train_accuracy)\n",
    "        \n",
    "                    if i % 500 == 0:\n",
    "                        print(f\"Iteration {i} - Loss: {loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "                    \n",
    "            # Store the average training loss & accuracy for this fold\n",
    "            avg_train_loss = np.mean(fold_train_losses)\n",
    "            avg_train_accuracy = np.mean(fold_train_accuracies)\n",
    "            self.losses.append(avg_train_loss)\n",
    "            self.train_accuracies.append(avg_train_accuracy)\n",
    "\n",
    "            val_pred = self.predict(X_val)[1]  # Get predicted class labels\n",
    "            y_val_labels = self.oh.inverse_transform(y_val)\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss = self.cross_entropy(self.softmax_(X_val @ self.W), y_val)\n",
    "            self.valid_losses.append(val_loss)\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            val_accuracy = np.mean(y_val_labels.flatten() == val_pred)\n",
    "            self.valid_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Fold: {fold}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "                \n",
    "    def train(self, X, y):\n",
    "        y_hat, _  = self.predict(X)\n",
    "\n",
    "        error = y_hat - y\n",
    "        m = max(X.shape[0], 1)  # Ensure division is valid\n",
    "\n",
    "        loss = self.cross_entropy(y_hat, y) if m > 1 else float(np.mean(y_hat))\n",
    "        \n",
    "        grad = X.T @ error  +  2 * self.l2 * self.W \n",
    "        self.velocity = 0.8 * self.velocity - self.lr * grad\n",
    "        self.W += self.velocity\n",
    "        #self.W -= self.lr * grad\n",
    "        \n",
    "        return loss if not np.isnan(loss) else 0.0 \n",
    "                \n",
    "    def predict(self, X, is_test=False):\n",
    "        if is_test:\n",
    "            X = self._add_intercept(X)\n",
    "    \n",
    "        y_hat = X @ self.W\n",
    "        y_hat = self.softmax_(y_hat)\n",
    "        y_real = np.argmax(y_hat, axis=1)\n",
    "        return y_hat, y_real\n",
    "    \n",
    "    def softmax_(self, X):\n",
    "        X_max = np.max(X, axis=1, keepdims=True)  # Find max per row\n",
    "        exp_shifted = np.exp(X - X_max)  # Shift values for numerical stability\n",
    "        return exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)  # Normalize\n",
    "    \n",
    "    def weight_initializer(self, X, num_classes):\n",
    "        if self.weight_init == 'uniform':\n",
    "            return np.random.uniform(low=-self.lr, high=self.lr, size=(X.shape[1], num_classes))\n",
    "        elif self.weight_init == 'normal':\n",
    "            return np.random.randn(X.shape[1], num_classes)\n",
    "        elif self.weight_init == 'xavier':\n",
    "            limit = np.sqrt(6 / (X.shape[1] + num_classes))\n",
    "            return np.random.uniform(low=-limit, high=limit, size=(X.shape[1], num_classes))\n",
    "        else:\n",
    "            return np.ones((X.shape[1], num_classes))\n",
    "        \n",
    "    def cross_entropy(self, y, y_hat):\n",
    "        if y_hat.size == 0 or y.size == 0:\n",
    "            return 0.0  # Return zero loss to avoid NaN issues\n",
    "    \n",
    "        m = max(y.shape[0], 1)  # Prevent division by zero\n",
    "        loss = - np.sum(y * np.log(y_hat + 1e-9))/m ## Prevent log(0)\n",
    "        return  loss + self.l2*np.sum(self.W**2)\n",
    "    \n",
    "    def _add_intercept(self,X):\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    def classification_report(self, pred, y):\n",
    "        # Ensure both pred and y are NumPy arrays\n",
    "        pred = np.array(pred).flatten()\n",
    "        y = np.array(y)\n",
    "        \n",
    "        if len(pred) == 0 or len(y) == 0:  # Prevent empty array issues\n",
    "            return 0.0  # Return zero accuracy to avoid NaN\n",
    "    \n",
    "        # Extract unique labels from both predictions and ground truth\n",
    "        labels = np.unique(np.concatenate((y, pred)))\n",
    "        num_classes = len(labels)  # Update num_classes based on unique labels\n",
    "    \n",
    "        # Initialize confusion matrix\n",
    "        cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "        \n",
    "        for true, pred_label in zip(y, pred):\n",
    "            # Get index positions of true and predicted labels in `labels`\n",
    "            true_idx = np.where(labels == true)[0]\n",
    "            pred_idx = np.where(labels == pred_label)[0]\n",
    "    \n",
    "            if true_idx.size > 0 and pred_idx.size > 0:  # Check if valid indices exist\n",
    "                cm[true_idx[0], pred_idx[0]] += 1\n",
    "    \n",
    "        # Compute metrics\n",
    "        tp = np.diag(cm)\n",
    "        fp = np.sum(cm, axis=0) - tp\n",
    "        fn = np.sum(cm, axis=1) - tp\n",
    "    \n",
    "        precision = np.divide(tp, tp + fp, out=np.zeros_like(tp, dtype=float), where=(tp + fp) > 0)\n",
    "        recall = np.divide(tp, tp + fn, out=np.zeros_like(tp, dtype=float), where=(tp + fn) > 0)\n",
    "        f1_score = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(precision, dtype=float), where=(precision + recall) > 0)\n",
    "        \n",
    "        accuracy_score = np.sum(np.diag(cm)) / np.sum(cm) if np.sum(cm) > 0 else 0.0\n",
    "        \n",
    "        # Weighted metrics\n",
    "        class_counts = np.array([(y == label).sum() for label in labels]) / len(y)\n",
    "        weighted_precision = np.sum(class_counts * precision)\n",
    "        weighted_recall = np.sum(class_counts * recall)\n",
    "        weighted_f1 = np.sum(class_counts * f1_score)\n",
    "    \n",
    "        # Print report\n",
    "        print(\"\\nClassification Report:\\n\")\n",
    "        print(\"{:<10} {:<10} {:<10} {:<10}\".format(\"Class\", \"Precision\", \"Recall\", \"F1-score\"))\n",
    "        print(\"-\" * 40)\n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"{label:<10} {precision[i]:<10.2f} {recall[i]:<10.2f} {f1_score[i]:<10.2f}\")\n",
    "    \n",
    "        return round(accuracy_score, 2), round(weighted_precision, 2), round(weighted_recall, 2), round(weighted_f1, 2)\n",
    "    \n",
    "    \n",
    "    def _coeff_and_biases(self, feature_names):\n",
    "        if not hasattr(self, \"W\"):\n",
    "            raise ValueError(\"Model is not trained yet. Fit the model before retrieving coefficients.\")\n",
    "        \n",
    "        coef = self.W[1:,:]  # Exclude bias term\n",
    "        bias = self.W[0, :]\n",
    "        print(coef.shape, len(feature_names))\n",
    "        # Create a DataFrame for easy interpretation\n",
    "        coef_df = pd.DataFrame(coef, index=feature_names, columns=[f\"Class_{i}\" for i in range(coef.shape[1])] if coef.ndim > 1 else [\"Coefficient\"])\n",
    "        \n",
    "        print(\"\\nTop Important Features:\")\n",
    "        print(coef_df.abs().sum(axis=1).sort_values(ascending=False).head(10))\n",
    "\n",
    "        return coef_df, bias"
   ],
   "id": "f5a27a736dfa7df0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T09:04:06.671288Z",
     "start_time": "2025-03-11T09:04:06.668274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax_(self, X):\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)"
   ],
   "id": "d92cdf35d5f35b46",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T15:48:34.998913Z",
     "start_time": "2025-03-17T15:48:34.890198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## remove engine, owners, seats\n",
    "num_col = df.select_dtypes(include=['float','int64']).columns.tolist()\n",
    "sns.heatmap(df[num_col].corr(), cmap='Greens', annot=True, linewidths=0.5)"
   ],
   "id": "8425033d27460262",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:59:25.075291Z",
     "start_time": "2025-03-16T17:59:25.070869Z"
    }
   },
   "cell_type": "code",
   "source": "df['selling_price'].describe()",
   "id": "e2dec6f2252dcc98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6.663000e+03\n",
       "mean     5.243708e+05\n",
       "std      5.090416e+05\n",
       "min      2.999900e+04\n",
       "25%      2.500000e+05\n",
       "50%      4.200000e+05\n",
       "75%      6.500000e+05\n",
       "max      1.000000e+07\n",
       "Name: selling_price, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T12:01:41.359775Z",
     "start_time": "2025-03-11T12:01:41.237620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.histplot(data=df, x='selling_price', kde=True, color='green')"
   ],
   "id": "719173f42f3a807e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='selling_price', ylabel='Count'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:25:46.738501Z",
     "start_time": "2025-03-17T17:25:46.736332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = np.random.randint(low=1, high=100, size=(10,4))\n",
    "print(a.shape, a.flatten().shape)"
   ],
   "id": "4a40872088efaf94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4) (40,)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:53:23.685806Z",
     "start_time": "2025-03-18T09:53:22.463431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MyLogisticRegression(weight_init='uniform', lr=0.001, batch_size=64, max_iter=7500, method='mini_batch', l2=0.01)\n",
    "model.fit(X_train_trf, ytrain)"
   ],
   "id": "415a822bcc982fae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Loss: 15.5435, Accuracy: 0.3125\n",
      "Iteration 500 - Loss: 13.4214, Accuracy: 0.4844\n",
      "Iteration 1000 - Loss: 13.3199, Accuracy: 0.5156\n",
      "Iteration 1500 - Loss: 12.7763, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 13.3444, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 13.7307, Accuracy: 0.5781\n",
      "Iteration 3000 - Loss: 14.4218, Accuracy: 0.5625\n",
      "Iteration 3500 - Loss: 14.7671, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 14.1614, Accuracy: 0.6406\n",
      "Iteration 4500 - Loss: 13.9069, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 14.2293, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 14.3805, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 14.6237, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 15.0926, Accuracy: 0.6719\n",
      "Iteration 7000 - Loss: 14.0260, Accuracy: 0.7188\n",
      "Fold: 0, Train Loss: 14.0470, Train Accuracy: 0.6298, Val Loss: 14.8722, Val Accuracy: 0.6867\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 12.5881, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 13.4430, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 12.8380, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 14.2904, Accuracy: 0.5312\n",
      "Iteration 2500 - Loss: 12.8700, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 14.0329, Accuracy: 0.6094\n",
      "Iteration 3500 - Loss: 14.2510, Accuracy: 0.6406\n",
      "Iteration 4000 - Loss: 13.5285, Accuracy: 0.7812\n",
      "Iteration 4500 - Loss: 14.1978, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 14.7951, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 14.4080, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 14.5182, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 14.5880, Accuracy: 0.6719\n",
      "Iteration 7000 - Loss: 14.4925, Accuracy: 0.7500\n",
      "Fold: 1, Train Loss: 13.9222, Train Accuracy: 0.6358, Val Loss: 15.2793, Val Accuracy: 0.6285\n",
      "Iteration 0 - Loss: 15.5402, Accuracy: 0.3906\n",
      "Iteration 500 - Loss: 12.8186, Accuracy: 0.6406\n",
      "Iteration 1000 - Loss: 12.8988, Accuracy: 0.5469\n",
      "Iteration 1500 - Loss: 13.6169, Accuracy: 0.4688\n",
      "Iteration 2000 - Loss: 13.5136, Accuracy: 0.5781\n",
      "Iteration 2500 - Loss: 14.3597, Accuracy: 0.5938\n",
      "Iteration 3000 - Loss: 12.6727, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 13.7664, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 13.7242, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 14.5473, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 15.2793, Accuracy: 0.5156\n",
      "Iteration 5500 - Loss: 14.4835, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 14.9779, Accuracy: 0.5938\n",
      "Iteration 6500 - Loss: 14.2337, Accuracy: 0.7344\n",
      "Iteration 7000 - Loss: 15.4398, Accuracy: 0.6562\n",
      "Fold: 2, Train Loss: 13.9763, Train Accuracy: 0.6331, Val Loss: 15.1687, Val Accuracy: 0.6520\n",
      "Iteration 0 - Loss: 15.5400, Accuracy: 0.3906\n",
      "Iteration 500 - Loss: 12.7624, Accuracy: 0.6406\n",
      "Iteration 1000 - Loss: 12.4064, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 12.6366, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 13.3359, Accuracy: 0.5938\n",
      "Iteration 2500 - Loss: 14.0054, Accuracy: 0.5312\n",
      "Iteration 3000 - Loss: 13.5426, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 14.4436, Accuracy: 0.5625\n",
      "Iteration 4000 - Loss: 15.1880, Accuracy: 0.5938\n",
      "Iteration 4500 - Loss: 13.5309, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 14.6596, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 15.5687, Accuracy: 0.5312\n",
      "Iteration 6000 - Loss: 15.3601, Accuracy: 0.5938\n",
      "Iteration 6500 - Loss: 14.9942, Accuracy: 0.6406\n",
      "Iteration 7000 - Loss: 15.1608, Accuracy: 0.6250\n",
      "Fold: 3, Train Loss: 14.0896, Train Accuracy: 0.6281, Val Loss: 14.8013, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.3438\n",
      "Iteration 500 - Loss: 13.2936, Accuracy: 0.5469\n",
      "Iteration 1000 - Loss: 13.4521, Accuracy: 0.5000\n",
      "Iteration 1500 - Loss: 12.5273, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 12.9999, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 12.8695, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 14.4828, Accuracy: 0.5156\n",
      "Iteration 3500 - Loss: 13.0894, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 13.9606, Accuracy: 0.5781\n",
      "Iteration 4500 - Loss: 14.7963, Accuracy: 0.5781\n",
      "Iteration 5000 - Loss: 14.7511, Accuracy: 0.6094\n",
      "Iteration 5500 - Loss: 13.6282, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 15.4840, Accuracy: 0.6719\n",
      "Iteration 6500 - Loss: 15.4459, Accuracy: 0.6406\n",
      "Iteration 7000 - Loss: 15.4656, Accuracy: 0.6250\n",
      "Fold: 4, Train Loss: 13.9287, Train Accuracy: 0.6303, Val Loss: 14.9922, Val Accuracy: 0.6379\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:54:43.928551Z",
     "start_time": "2025-03-18T09:54:43.920986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_ , pred = model.predict(X_test_trf, is_test=True)\n",
    "print(model.classification_report(pred, ytest))\n",
    "print(len(pred))"
   ],
   "id": "76d242e009bf632f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.74       0.79      \n",
      "1          0.54       0.83       0.66      \n",
      "2          0.46       0.24       0.31      \n",
      "3          0.75       0.57       0.64      \n",
      "(np.float64(0.63), np.float64(0.63), np.float64(0.63), np.float64(0.61))\n",
      "1333\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:54:50.515772Z",
     "start_time": "2025-03-18T09:54:50.512163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_, w = model._coeff_and_biases(feature_names)\n",
    "print(w)"
   ],
   "id": "5d77e4da4b79f231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4) 10\n",
      "\n",
      "Top Important Features:\n",
      "year                            25.176805\n",
      "max_power                       14.248193\n",
      "engine                          12.260812\n",
      "transmission_Manual              7.494497\n",
      "seller_type_Dealer               6.473414\n",
      "brand_encoded                    5.632021\n",
      "km_driven                        4.798056\n",
      "transmission_Automatic           4.756510\n",
      "seller_type_Individual           4.426512\n",
      "seller_type_Trustmark_Dealer     1.469455\n",
      "dtype: float64\n",
      "[ 4.71619872  1.40862785 -2.10999629 -4.01531078]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:55:39.681432Z",
     "start_time": "2025-03-18T09:55:39.678438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "joblib.dump(model, \"model.pkl\")\n",
    "print(\"Preprocessor saved successfully!\")"
   ],
   "id": "57ff20a4244ce518",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor saved successfully!\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:16:43.420155Z",
     "start_time": "2025-03-18T09:16:43.417662Z"
    }
   },
   "cell_type": "code",
   "source": "usr_data = {'brand',}",
   "id": "2b8c38c418bd9d94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 1 1 1 3 2 3 2 3 1 2 1 0 1 2 2 1 1]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:19:46.693075Z",
     "start_time": "2025-03-17T16:08:03.692129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlflow.set_experiment(\"MLflow_Classification_Registry\")\n",
    "\n",
    "# Lists of hyperparameters\n",
    "methods = [\"batch\", \"mini_batch\", \"stochastic\"]  \n",
    "weights = [\"xavier\", \"normal\", \"uniform\"]\n",
    "penalty = [None, 0.01]\n",
    "lr = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]  # Added batch sizes\n",
    "\n",
    "# Initialize best model tracking\n",
    "best_model_acc, best_acc, best_params_acc = None, float('-inf'), None\n",
    "best_run_id = None\n",
    "\n",
    "def run_experiment(method, weight, lr, penalty, batch_size):\n",
    "    \"\"\"Runs ML experiments, logs results in MLflow, and tracks the best models.\"\"\"\n",
    "    global best_model_acc, best_acc, best_params_acc, best_run_id\n",
    "\n",
    "    params = {\n",
    "        \"method\": method, \"lr\": lr, \"weight_init\": weight, \"l2\": penalty, \"batch_size\": batch_size\n",
    "    }\n",
    "\n",
    "    print(f\"Running MyLogisticRegression | {method} | {weight} | LR: {lr} | l2: {penalty} | Batch Size: {batch_size}\")\n",
    "\n",
    "    # Initialize model with current hyperparameters\n",
    "    model = MyLogisticRegression(\n",
    "        lr=lr, \n",
    "        max_iter=7000, \n",
    "        weight_init=weight, \n",
    "        method=method,\n",
    "        batch_size=batch_size, \n",
    "        l2=penalty\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"method-{method}-lr-{lr}-weight-{weight}-batch-{batch_size}\", nested=True):\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_trf, ytrain)\n",
    "\n",
    "        # Predictions\n",
    "        _, y_pred = model.predict(X_test_trf, is_test=True)\n",
    "\n",
    "        # Compute metrics\n",
    "        acc, precision, recall, f1_score = model.classification_report(y_pred, ytest)\n",
    "\n",
    "        # Log hyperparameters & metrics in MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics({\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1_score\": f1_score})\n",
    "\n",
    "        # Save Model Signature\n",
    "        signature = mlflow.models.infer_signature(X_train_trf, model.predict(X_train_trf, is_test=True)[1])\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature)\n",
    "\n",
    "        print(f\" Model: MyLogisticRegression | Method: {method} | Accuracy: {acc:.4f} | F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "        # Track the Best Model Based on Accuracy (HIGHER IS BETTER)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_acc = model\n",
    "            best_params_acc = params\n",
    "            best_run_id = mlflow.active_run().info.run_id  # Save the best run ID for registry\n",
    "\n",
    "\n",
    "# Iterate over all hyperparameter combinations\n",
    "combinations_ = product(methods, weights, lr, batch_sizes)\n",
    "\n",
    "# Run all experiments\n",
    "for method, weight, lr, batch_size in combinations_:\n",
    "    run_experiment(method, weight, lr, None, batch_size)\n",
    "\n",
    "# Final Results: Best Model Based on Accuracy\n",
    "print(f\"Best Model by Accuracy: {best_model_acc} with Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "# -----------------------\n",
    "# Register Best Model in MLflow Model Registry\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")  # Make sure MLflow server is running\n",
    "\n",
    "if best_run_id:\n",
    "    model_uri = f\"runs:/{best_run_id}/model\"\n",
    "    model_name = \"st124783-a3-model\"  # Replace with your student ID\n",
    "\n",
    "    # Register the best model\n",
    "    model_version = mlflow.register_model(model_uri, model_name)\n",
    "    print(f\"\\n Registered Model: {model_name} (Version: {model_version})\")\n",
    "\n",
    "    # Move the Model to 'Staging'\n",
    "    client = MlflowClient()\n",
    "    latest_version = client.get_latest_versions(model_name, stages=[\"None\"])[0].version\n",
    "\n",
    "    client.transition_model_version_stage(name=model_name, version=latest_version, stage=\"Staging\")\n",
    "    print(f\"Model Version {latest_version} is now in 'Staging'!\")"
   ],
   "id": "72f96d9b29a1d53d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/17 23:08:03 INFO mlflow.tracking.fluent: Experiment with name 'MLflow_Classification_Registry' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MyLogisticRegression | batch | xavier | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.4983, Accuracy: 0.4641\n",
      "Iteration 500 - Loss: 8.3558, Accuracy: 0.6794\n",
      "Iteration 1000 - Loss: 6.3595, Accuracy: 0.6789\n",
      "Iteration 1500 - Loss: 7.3733, Accuracy: 0.6100\n",
      "Iteration 2000 - Loss: 8.0450, Accuracy: 0.5424\n",
      "Iteration 2500 - Loss: 10.0565, Accuracy: 0.5227\n",
      "Iteration 3000 - Loss: 8.5419, Accuracy: 0.6677\n",
      "Iteration 3500 - Loss: 6.9325, Accuracy: 0.6041\n",
      "Iteration 4000 - Loss: 9.0302, Accuracy: 0.5910\n",
      "Iteration 4500 - Loss: 7.6723, Accuracy: 0.6393\n",
      "Iteration 5000 - Loss: 8.2311, Accuracy: 0.6606\n",
      "Iteration 5500 - Loss: 7.4938, Accuracy: 0.6651\n",
      "Iteration 6000 - Loss: 9.5955, Accuracy: 0.6489\n",
      "Iteration 6500 - Loss: 7.0547, Accuracy: 0.6360\n",
      "Fold: 0, Train Loss: 8.1863, Train Accuracy: 0.6055, Val Loss: 7.1282, Val Accuracy: 0.6595\n",
      "Iteration 0 - Loss: 16.5833, Accuracy: 0.4308\n",
      "Iteration 500 - Loss: 7.4965, Accuracy: 0.6520\n",
      "Iteration 1000 - Loss: 9.0381, Accuracy: 0.5471\n",
      "Iteration 1500 - Loss: 6.2497, Accuracy: 0.6438\n",
      "Iteration 2000 - Loss: 9.7010, Accuracy: 0.6295\n",
      "Iteration 2500 - Loss: 8.5572, Accuracy: 0.6095\n",
      "Iteration 3000 - Loss: 8.6277, Accuracy: 0.6041\n",
      "Iteration 3500 - Loss: 8.6449, Accuracy: 0.5598\n",
      "Iteration 4000 - Loss: 8.9519, Accuracy: 0.6062\n",
      "Iteration 4500 - Loss: 8.4718, Accuracy: 0.5476\n",
      "Iteration 5000 - Loss: 7.0150, Accuracy: 0.6670\n",
      "Iteration 5500 - Loss: 9.4160, Accuracy: 0.5516\n",
      "Iteration 6000 - Loss: 8.9239, Accuracy: 0.5483\n",
      "Iteration 6500 - Loss: 6.9563, Accuracy: 0.6435\n",
      "Fold: 1, Train Loss: 8.0893, Train Accuracy: 0.6102, Val Loss: 7.7931, Val Accuracy: 0.6285\n",
      "Iteration 0 - Loss: 14.7750, Accuracy: 0.3935\n",
      "Iteration 500 - Loss: 8.0770, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 10.1438, Accuracy: 0.6067\n",
      "Iteration 1500 - Loss: 7.5532, Accuracy: 0.6147\n",
      "Iteration 2000 - Loss: 6.7299, Accuracy: 0.5905\n",
      "Iteration 2500 - Loss: 8.4786, Accuracy: 0.7127\n",
      "Iteration 3000 - Loss: 7.4542, Accuracy: 0.6140\n",
      "Iteration 3500 - Loss: 7.3668, Accuracy: 0.5917\n",
      "Iteration 4000 - Loss: 7.8785, Accuracy: 0.5912\n",
      "Iteration 4500 - Loss: 7.1436, Accuracy: 0.6177\n",
      "Iteration 5000 - Loss: 7.6572, Accuracy: 0.5223\n",
      "Iteration 5500 - Loss: 6.8945, Accuracy: 0.6531\n",
      "Iteration 6000 - Loss: 9.5149, Accuracy: 0.6198\n",
      "Iteration 6500 - Loss: 8.2839, Accuracy: 0.6550\n",
      "Fold: 2, Train Loss: 8.0557, Train Accuracy: 0.6118, Val Loss: 7.4937, Val Accuracy: 0.6379\n",
      "Iteration 0 - Loss: 16.7099, Accuracy: 0.4233\n",
      "Iteration 500 - Loss: 7.2399, Accuracy: 0.6405\n",
      "Iteration 1000 - Loss: 6.6880, Accuracy: 0.6095\n",
      "Iteration 1500 - Loss: 7.3765, Accuracy: 0.6602\n",
      "Iteration 2000 - Loss: 8.1798, Accuracy: 0.5359\n",
      "Iteration 2500 - Loss: 8.0342, Accuracy: 0.6196\n",
      "Iteration 3000 - Loss: 9.4437, Accuracy: 0.6377\n",
      "Iteration 3500 - Loss: 9.5908, Accuracy: 0.6229\n",
      "Iteration 4000 - Loss: 7.3388, Accuracy: 0.6280\n",
      "Iteration 4500 - Loss: 8.2192, Accuracy: 0.6337\n",
      "Iteration 5000 - Loss: 8.8824, Accuracy: 0.5692\n",
      "Iteration 5500 - Loss: 8.4753, Accuracy: 0.6234\n",
      "Iteration 6000 - Loss: 9.0143, Accuracy: 0.5689\n",
      "Iteration 6500 - Loss: 9.0540, Accuracy: 0.4967\n",
      "Fold: 3, Train Loss: 8.1867, Train Accuracy: 0.6054, Val Loss: 7.6830, Val Accuracy: 0.6266\n",
      "Iteration 0 - Loss: 15.8489, Accuracy: 0.3806\n",
      "Iteration 500 - Loss: 8.5183, Accuracy: 0.5849\n",
      "Iteration 1000 - Loss: 9.3844, Accuracy: 0.6259\n",
      "Iteration 1500 - Loss: 7.0738, Accuracy: 0.6440\n",
      "Iteration 2000 - Loss: 6.9537, Accuracy: 0.6871\n",
      "Iteration 2500 - Loss: 8.9287, Accuracy: 0.6030\n",
      "Iteration 3000 - Loss: 8.6367, Accuracy: 0.6262\n",
      "Iteration 3500 - Loss: 7.3628, Accuracy: 0.6098\n",
      "Iteration 4000 - Loss: 6.8149, Accuracy: 0.5734\n",
      "Iteration 4500 - Loss: 7.8470, Accuracy: 0.5403\n",
      "Iteration 5000 - Loss: 9.4251, Accuracy: 0.6037\n",
      "Iteration 5500 - Loss: 7.1329, Accuracy: 0.6515\n",
      "Iteration 6000 - Loss: 7.4530, Accuracy: 0.6198\n",
      "Iteration 6500 - Loss: 7.1090, Accuracy: 0.6039\n",
      "Fold: 4, Train Loss: 8.1415, Train Accuracy: 0.6076, Val Loss: 7.6755, Val Accuracy: 0.6313\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.86       0.69       0.77      \n",
      "1          0.54       0.88       0.67      \n",
      "2          0.45       0.02       0.03      \n",
      "3          0.63       0.85       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6200 | F1-Score: 0.5500\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/d49a43162f424ba792210fa71557f1a1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.2319, Accuracy: 0.4024\n",
      "Iteration 500 - Loss: 9.3729, Accuracy: 0.6076\n",
      "Iteration 1000 - Loss: 7.1711, Accuracy: 0.6067\n",
      "Iteration 1500 - Loss: 7.7935, Accuracy: 0.6222\n",
      "Iteration 2000 - Loss: 8.3169, Accuracy: 0.5722\n",
      "Iteration 2500 - Loss: 8.0148, Accuracy: 0.6886\n",
      "Iteration 3000 - Loss: 8.0824, Accuracy: 0.5849\n",
      "Iteration 3500 - Loss: 8.1802, Accuracy: 0.5223\n",
      "Iteration 4000 - Loss: 8.8163, Accuracy: 0.6384\n",
      "Iteration 4500 - Loss: 8.3606, Accuracy: 0.6550\n",
      "Iteration 5000 - Loss: 8.1689, Accuracy: 0.5532\n",
      "Iteration 5500 - Loss: 9.1189, Accuracy: 0.6611\n",
      "Iteration 6000 - Loss: 7.3679, Accuracy: 0.5910\n",
      "Iteration 6500 - Loss: 8.9668, Accuracy: 0.6175\n",
      "Fold: 0, Train Loss: 8.1863, Train Accuracy: 0.6054, Val Loss: 9.5588, Val Accuracy: 0.5385\n",
      "Iteration 0 - Loss: 16.0344, Accuracy: 0.4723\n",
      "Iteration 500 - Loss: 9.2086, Accuracy: 0.6646\n",
      "Iteration 1000 - Loss: 7.6223, Accuracy: 0.6606\n",
      "Iteration 1500 - Loss: 8.2770, Accuracy: 0.6222\n",
      "Iteration 2000 - Loss: 7.4342, Accuracy: 0.5012\n",
      "Iteration 2500 - Loss: 7.9712, Accuracy: 0.6590\n",
      "Iteration 3000 - Loss: 7.7938, Accuracy: 0.5514\n",
      "Iteration 3500 - Loss: 7.6430, Accuracy: 0.5955\n",
      "Iteration 4000 - Loss: 8.4258, Accuracy: 0.6501\n",
      "Iteration 4500 - Loss: 9.3283, Accuracy: 0.5521\n",
      "Iteration 5000 - Loss: 9.1253, Accuracy: 0.5518\n",
      "Iteration 5500 - Loss: 6.7036, Accuracy: 0.5525\n",
      "Iteration 6000 - Loss: 6.9146, Accuracy: 0.6477\n",
      "Iteration 6500 - Loss: 7.9122, Accuracy: 0.6445\n",
      "Fold: 1, Train Loss: 8.0855, Train Accuracy: 0.6104, Val Loss: 7.6187, Val Accuracy: 0.6341\n",
      "Iteration 0 - Loss: 15.8746, Accuracy: 0.4925\n",
      "Iteration 500 - Loss: 6.7237, Accuracy: 0.6506\n",
      "Iteration 1000 - Loss: 9.9990, Accuracy: 0.5980\n",
      "Iteration 1500 - Loss: 8.3380, Accuracy: 0.5997\n",
      "Iteration 2000 - Loss: 8.0636, Accuracy: 0.5626\n",
      "Iteration 2500 - Loss: 7.5820, Accuracy: 0.6881\n",
      "Iteration 3000 - Loss: 8.0198, Accuracy: 0.5741\n",
      "Iteration 3500 - Loss: 7.4667, Accuracy: 0.5260\n",
      "Iteration 4000 - Loss: 9.2164, Accuracy: 0.6386\n",
      "Iteration 4500 - Loss: 7.5117, Accuracy: 0.6011\n",
      "Iteration 5000 - Loss: 8.0754, Accuracy: 0.6536\n",
      "Iteration 5500 - Loss: 6.2195, Accuracy: 0.6266\n",
      "Iteration 6000 - Loss: 9.7825, Accuracy: 0.6041\n",
      "Iteration 6500 - Loss: 8.4189, Accuracy: 0.6067\n",
      "Fold: 2, Train Loss: 8.0422, Train Accuracy: 0.6125, Val Loss: 8.7059, Val Accuracy: 0.5779\n",
      "Iteration 0 - Loss: 16.1641, Accuracy: 0.4184\n",
      "Iteration 500 - Loss: 9.3178, Accuracy: 0.5239\n",
      "Iteration 1000 - Loss: 8.0907, Accuracy: 0.5138\n",
      "Iteration 1500 - Loss: 7.4117, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 7.9064, Accuracy: 0.6135\n",
      "Iteration 2500 - Loss: 7.7195, Accuracy: 0.5516\n",
      "Iteration 3000 - Loss: 7.9851, Accuracy: 0.6032\n",
      "Iteration 3500 - Loss: 6.7868, Accuracy: 0.6705\n",
      "Iteration 4000 - Loss: 9.5519, Accuracy: 0.6510\n",
      "Iteration 4500 - Loss: 7.1623, Accuracy: 0.6222\n",
      "Iteration 5000 - Loss: 10.2322, Accuracy: 0.6053\n",
      "Iteration 5500 - Loss: 6.7633, Accuracy: 0.6363\n",
      "Iteration 6000 - Loss: 8.5238, Accuracy: 0.6032\n",
      "Iteration 6500 - Loss: 8.6938, Accuracy: 0.6100\n",
      "Fold: 3, Train Loss: 8.1913, Train Accuracy: 0.6052, Val Loss: 8.7354, Val Accuracy: 0.5826\n",
      "Iteration 0 - Loss: 15.1186, Accuracy: 0.3940\n",
      "Iteration 500 - Loss: 8.8195, Accuracy: 0.5521\n",
      "Iteration 1000 - Loss: 8.5170, Accuracy: 0.5673\n",
      "Iteration 1500 - Loss: 8.5749, Accuracy: 0.6492\n",
      "Iteration 2000 - Loss: 7.5799, Accuracy: 0.6449\n",
      "Iteration 2500 - Loss: 8.3813, Accuracy: 0.5835\n",
      "Iteration 3000 - Loss: 8.1969, Accuracy: 0.5288\n",
      "Iteration 3500 - Loss: 9.0650, Accuracy: 0.5636\n",
      "Iteration 4000 - Loss: 8.3513, Accuracy: 0.5687\n",
      "Iteration 4500 - Loss: 7.1910, Accuracy: 0.6203\n",
      "Iteration 5000 - Loss: 7.0378, Accuracy: 0.6487\n",
      "Iteration 5500 - Loss: 8.4886, Accuracy: 0.6137\n",
      "Iteration 6000 - Loss: 7.9308, Accuracy: 0.6168\n",
      "Iteration 6500 - Loss: 9.1339, Accuracy: 0.6224\n",
      "Fold: 4, Train Loss: 8.1426, Train Accuracy: 0.6076, Val Loss: 7.3829, Val Accuracy: 0.6407\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.73       0.88       0.79      \n",
      "1          0.59       0.65       0.62      \n",
      "2          0.57       0.16       0.25      \n",
      "3          0.59       0.87       0.71      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6300 | F1-Score: 0.5900\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/b2c44a3eae3d4e35a3cb4f1cdfb414f0\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 16.2660, Accuracy: 0.4798\n",
      "Iteration 500 - Loss: 7.6973, Accuracy: 0.6288\n",
      "Iteration 1000 - Loss: 9.0740, Accuracy: 0.6264\n",
      "Iteration 1500 - Loss: 7.7975, Accuracy: 0.6703\n",
      "Iteration 2000 - Loss: 7.8078, Accuracy: 0.4791\n",
      "Iteration 2500 - Loss: 9.6599, Accuracy: 0.6543\n",
      "Iteration 3000 - Loss: 9.0136, Accuracy: 0.5385\n",
      "Iteration 3500 - Loss: 6.8551, Accuracy: 0.6302\n",
      "Iteration 4000 - Loss: 7.3149, Accuracy: 0.6468\n",
      "Iteration 4500 - Loss: 8.1667, Accuracy: 0.6370\n",
      "Iteration 5000 - Loss: 7.9208, Accuracy: 0.6768\n",
      "Iteration 5500 - Loss: 9.3598, Accuracy: 0.6119\n",
      "Iteration 6000 - Loss: 8.5586, Accuracy: 0.5697\n",
      "Iteration 6500 - Loss: 9.1104, Accuracy: 0.5136\n",
      "Fold: 0, Train Loss: 8.1852, Train Accuracy: 0.6055, Val Loss: 8.8387, Val Accuracy: 0.5722\n",
      "Iteration 0 - Loss: 14.1520, Accuracy: 0.3523\n",
      "Iteration 500 - Loss: 6.8160, Accuracy: 0.5818\n",
      "Iteration 1000 - Loss: 10.2829, Accuracy: 0.6269\n",
      "Iteration 1500 - Loss: 9.9085, Accuracy: 0.6536\n",
      "Iteration 2000 - Loss: 7.7863, Accuracy: 0.6717\n",
      "Iteration 2500 - Loss: 7.5406, Accuracy: 0.6435\n",
      "Iteration 3000 - Loss: 7.8316, Accuracy: 0.5910\n",
      "Iteration 3500 - Loss: 7.7320, Accuracy: 0.6053\n",
      "Iteration 4000 - Loss: 10.7342, Accuracy: 0.6548\n",
      "Iteration 4500 - Loss: 9.3527, Accuracy: 0.5957\n",
      "Iteration 5000 - Loss: 7.5673, Accuracy: 0.6494\n",
      "Iteration 5500 - Loss: 8.5534, Accuracy: 0.5938\n",
      "Iteration 6000 - Loss: 8.7640, Accuracy: 0.6642\n",
      "Iteration 6500 - Loss: 8.1359, Accuracy: 0.6492\n",
      "Fold: 1, Train Loss: 8.0927, Train Accuracy: 0.6100, Val Loss: 8.2725, Val Accuracy: 0.5994\n",
      "Iteration 0 - Loss: 15.8416, Accuracy: 0.4003\n",
      "Iteration 500 - Loss: 7.2023, Accuracy: 0.6044\n",
      "Iteration 1000 - Loss: 7.3180, Accuracy: 0.5192\n",
      "Iteration 1500 - Loss: 8.2192, Accuracy: 0.5762\n",
      "Iteration 2000 - Loss: 8.0490, Accuracy: 0.6939\n",
      "Iteration 2500 - Loss: 9.3209, Accuracy: 0.6534\n",
      "Iteration 3000 - Loss: 9.9483, Accuracy: 0.6421\n",
      "Iteration 3500 - Loss: 7.5398, Accuracy: 0.5765\n",
      "Iteration 4000 - Loss: 8.3166, Accuracy: 0.5774\n",
      "Iteration 4500 - Loss: 7.4135, Accuracy: 0.6255\n",
      "Iteration 5000 - Loss: 8.3323, Accuracy: 0.6053\n",
      "Iteration 5500 - Loss: 7.3470, Accuracy: 0.5877\n",
      "Iteration 6000 - Loss: 9.2339, Accuracy: 0.5915\n",
      "Iteration 6500 - Loss: 9.2368, Accuracy: 0.6069\n",
      "Fold: 2, Train Loss: 8.0502, Train Accuracy: 0.6120, Val Loss: 7.7033, Val Accuracy: 0.6304\n",
      "Iteration 0 - Loss: 16.3871, Accuracy: 0.4751\n",
      "Iteration 500 - Loss: 7.2462, Accuracy: 0.6632\n",
      "Iteration 1000 - Loss: 9.5495, Accuracy: 0.6102\n",
      "Iteration 1500 - Loss: 8.0410, Accuracy: 0.6191\n",
      "Iteration 2000 - Loss: 7.2613, Accuracy: 0.5331\n",
      "Iteration 2500 - Loss: 7.8311, Accuracy: 0.5490\n",
      "Iteration 3000 - Loss: 6.9113, Accuracy: 0.6644\n",
      "Iteration 3500 - Loss: 8.8951, Accuracy: 0.5823\n",
      "Iteration 4000 - Loss: 9.1935, Accuracy: 0.5912\n",
      "Iteration 4500 - Loss: 7.6119, Accuracy: 0.5532\n",
      "Iteration 5000 - Loss: 7.7661, Accuracy: 0.5706\n",
      "Iteration 5500 - Loss: 7.5613, Accuracy: 0.5736\n",
      "Iteration 6000 - Loss: 8.3234, Accuracy: 0.5478\n",
      "Iteration 6500 - Loss: 9.0433, Accuracy: 0.6407\n",
      "Fold: 3, Train Loss: 8.1891, Train Accuracy: 0.6053, Val Loss: 7.0202, Val Accuracy: 0.6576\n",
      "Iteration 0 - Loss: 16.4192, Accuracy: 0.4156\n",
      "Iteration 500 - Loss: 8.8208, Accuracy: 0.6707\n",
      "Iteration 1000 - Loss: 8.6604, Accuracy: 0.5145\n",
      "Iteration 1500 - Loss: 8.9144, Accuracy: 0.5331\n",
      "Iteration 2000 - Loss: 9.2425, Accuracy: 0.5748\n",
      "Iteration 2500 - Loss: 8.9829, Accuracy: 0.6400\n",
      "Iteration 3000 - Loss: 8.2919, Accuracy: 0.6112\n",
      "Iteration 3500 - Loss: 7.5034, Accuracy: 0.6032\n",
      "Iteration 4000 - Loss: 8.1687, Accuracy: 0.5793\n",
      "Iteration 4500 - Loss: 10.2007, Accuracy: 0.5626\n",
      "Iteration 5000 - Loss: 7.3075, Accuracy: 0.5525\n",
      "Iteration 5500 - Loss: 7.2014, Accuracy: 0.6553\n",
      "Iteration 6000 - Loss: 8.1275, Accuracy: 0.6318\n",
      "Iteration 6500 - Loss: 8.0457, Accuracy: 0.5174\n",
      "Fold: 4, Train Loss: 8.1404, Train Accuracy: 0.6077, Val Loss: 9.0568, Val Accuracy: 0.5647\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.53       0.99       0.69      \n",
      "1          1.00       0.00       0.01      \n",
      "2          0.47       0.75       0.58      \n",
      "3          0.74       0.66       0.70      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.5400 | F1-Score: 0.4300\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/a1786fc90b8f4f7c84fa143221fe4158\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 14.4272, Accuracy: 0.4512\n",
      "Iteration 500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Fold: 0, Train Loss: 8.1048, Train Accuracy: 0.6967, Val Loss: 7.9758, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 16.0542, Accuracy: 0.4613\n",
      "Iteration 500 - Loss: 7.9027, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9907, Train Accuracy: 0.7035, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 16.4328, Accuracy: 0.4130\n",
      "Iteration 500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Fold: 2, Train Loss: 7.9567, Train Accuracy: 0.7124, Val Loss: 8.3286, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 16.6628, Accuracy: 0.4350\n",
      "Iteration 500 - Loss: 8.1733, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1728, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1005, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 14.7016, Accuracy: 0.4034\n",
      "Iteration 500 - Loss: 7.9620, Accuracy: 0.7104\n",
      "Iteration 1000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 1500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Fold: 4, Train Loss: 8.0305, Train Accuracy: 0.7082, Val Loss: 8.0708, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.76       0.80      \n",
      "1          0.60       0.77       0.67      \n",
      "2          0.54       0.38       0.44      \n",
      "3          0.74       0.73       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/72a1a9311c8b43d18ad05f5495f91e81\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 16.0640, Accuracy: 0.4606\n",
      "Iteration 500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Fold: 0, Train Loss: 8.1046, Train Accuracy: 0.6966, Val Loss: 7.8284, Val Accuracy: 0.7139\n",
      "Iteration 0 - Loss: 15.4179, Accuracy: 0.4998\n",
      "Iteration 500 - Loss: 8.0753, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.0746, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 8.0745, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Fold: 1, Train Loss: 7.9903, Train Accuracy: 0.7036, Val Loss: 8.2928, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 16.3993, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 7.8989, Accuracy: 0.7160\n",
      "Iteration 1000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 1500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 2000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 2500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 3000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 3500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 4000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 4500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 5000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 5500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 6000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 6500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Fold: 2, Train Loss: 7.9566, Train Accuracy: 0.7125, Val Loss: 8.1494, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 15.2067, Accuracy: 0.4013\n",
      "Iteration 500 - Loss: 8.1735, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1729, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 16.3713, Accuracy: 0.4524\n",
      "Iteration 500 - Loss: 8.0945, Accuracy: 0.7064\n",
      "Iteration 1000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Fold: 4, Train Loss: 8.0297, Train Accuracy: 0.7083, Val Loss: 8.1073, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.83       0.81      \n",
      "1          0.71       0.63       0.67      \n",
      "2          0.54       0.73       0.62      \n",
      "3          0.86       0.58       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/1d7dc3517be246b1ba404c4fa7ae335e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 14.8631, Accuracy: 0.4285\n",
      "Iteration 500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Fold: 0, Train Loss: 8.1049, Train Accuracy: 0.6966, Val Loss: 7.8284, Val Accuracy: 0.7139\n",
      "Iteration 0 - Loss: 14.1556, Accuracy: 0.4156\n",
      "Iteration 500 - Loss: 7.9028, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9912, Train Accuracy: 0.7035, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 13.9354, Accuracy: 0.3766\n",
      "Iteration 500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Fold: 2, Train Loss: 7.9579, Train Accuracy: 0.7124, Val Loss: 8.3286, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.0507, Accuracy: 0.4573\n",
      "Iteration 500 - Loss: 8.6677, Accuracy: 0.6351\n",
      "Iteration 1000 - Loss: 7.8189, Accuracy: 0.6163\n",
      "Iteration 1500 - Loss: 8.0776, Accuracy: 0.6939\n",
      "Iteration 2000 - Loss: 8.6667, Accuracy: 0.6351\n",
      "Iteration 2500 - Loss: 7.8186, Accuracy: 0.6163\n",
      "Iteration 3000 - Loss: 8.0773, Accuracy: 0.6939\n",
      "Iteration 3500 - Loss: 8.6666, Accuracy: 0.6351\n",
      "Iteration 4000 - Loss: 7.8185, Accuracy: 0.6163\n",
      "Iteration 4500 - Loss: 8.0772, Accuracy: 0.6939\n",
      "Iteration 5000 - Loss: 8.6665, Accuracy: 0.6351\n",
      "Iteration 5500 - Loss: 7.8184, Accuracy: 0.6163\n",
      "Iteration 6000 - Loss: 8.0772, Accuracy: 0.6939\n",
      "Iteration 6500 - Loss: 8.6665, Accuracy: 0.6351\n",
      "Fold: 3, Train Loss: 8.1912, Train Accuracy: 0.6484, Val Loss: 7.6701, Val Accuracy: 0.7111\n",
      "Iteration 0 - Loss: 14.5835, Accuracy: 0.5056\n",
      "Iteration 500 - Loss: 8.6032, Accuracy: 0.6377\n",
      "Iteration 1000 - Loss: 7.7634, Accuracy: 0.6222\n",
      "Iteration 1500 - Loss: 7.9815, Accuracy: 0.7000\n",
      "Iteration 2000 - Loss: 8.6028, Accuracy: 0.6379\n",
      "Iteration 2500 - Loss: 7.7633, Accuracy: 0.6222\n",
      "Iteration 3000 - Loss: 7.9815, Accuracy: 0.7000\n",
      "Iteration 3500 - Loss: 8.6028, Accuracy: 0.6379\n",
      "Iteration 4000 - Loss: 7.7633, Accuracy: 0.6222\n",
      "Iteration 4500 - Loss: 7.9815, Accuracy: 0.7000\n",
      "Iteration 5000 - Loss: 8.6028, Accuracy: 0.6379\n",
      "Iteration 5500 - Loss: 7.7633, Accuracy: 0.6222\n",
      "Iteration 6000 - Loss: 7.9815, Accuracy: 0.7000\n",
      "Iteration 6500 - Loss: 8.6028, Accuracy: 0.6379\n",
      "Fold: 4, Train Loss: 8.1182, Train Accuracy: 0.6535, Val Loss: 7.8771, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.81       0.81      \n",
      "1          0.62       0.71       0.66      \n",
      "2          0.55       0.39       0.46      \n",
      "3          0.68       0.76       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/84182eb24e334a68826bc2a69f8c01b2\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.6308, Accuracy: 0.3213\n",
      "Iteration 500 - Loss: 8.1549, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0923, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7141\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1226, Train Accuracy: 0.7135, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 14.6721, Accuracy: 0.4371\n",
      "Iteration 500 - Loss: 8.0435, Accuracy: 0.7212\n",
      "Iteration 1000 - Loss: 7.9796, Accuracy: 0.7207\n",
      "Iteration 1500 - Loss: 7.9720, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9707, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9703, Accuracy: 0.7223\n",
      "Iteration 3000 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9698, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0087, Train Accuracy: 0.7215, Val Loss: 8.2419, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 14.8303, Accuracy: 0.3415\n",
      "Iteration 500 - Loss: 8.0183, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9534, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9842, Train Accuracy: 0.7206, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.6082, Accuracy: 0.2953\n",
      "Iteration 500 - Loss: 8.1531, Accuracy: 0.7167\n",
      "Iteration 1000 - Loss: 8.0896, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0820, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0805, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0799, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0796, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7167\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0789, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1186, Train Accuracy: 0.7162, Val Loss: 7.9512, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 14.5021, Accuracy: 0.3619\n",
      "Iteration 500 - Loss: 8.0934, Accuracy: 0.7151\n",
      "Iteration 1000 - Loss: 8.0270, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0190, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0178, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7146\n",
      "Iteration 3000 - Loss: 8.0173, Accuracy: 0.7146\n",
      "Iteration 3500 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0579, Train Accuracy: 0.7139, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/8d3feb09e74d46c7be02ec744e27430f\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 16.1292, Accuracy: 0.2756\n",
      "Iteration 500 - Loss: 8.1542, Accuracy: 0.7141\n",
      "Iteration 1000 - Loss: 8.0921, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0852, Accuracy: 0.7139\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1221, Train Accuracy: 0.7134, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 13.8691, Accuracy: 0.3961\n",
      "Iteration 500 - Loss: 8.0431, Accuracy: 0.7205\n",
      "Iteration 1000 - Loss: 7.9793, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9716, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9704, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0087, Train Accuracy: 0.7214, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 14.1718, Accuracy: 0.4308\n",
      "Iteration 500 - Loss: 8.0196, Accuracy: 0.7212\n",
      "Iteration 1000 - Loss: 7.9536, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9844, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 16.4374, Accuracy: 0.2263\n",
      "Iteration 500 - Loss: 8.1530, Accuracy: 0.7165\n",
      "Iteration 1000 - Loss: 8.0898, Accuracy: 0.7169\n",
      "Iteration 1500 - Loss: 8.0822, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0807, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0801, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0797, Accuracy: 0.7169\n",
      "Iteration 3500 - Loss: 8.0794, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0792, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0789, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1187, Train Accuracy: 0.7162, Val Loss: 7.9510, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 15.4388, Accuracy: 0.3180\n",
      "Iteration 500 - Loss: 8.0893, Accuracy: 0.7155\n",
      "Iteration 1000 - Loss: 8.0263, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0188, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0176, Accuracy: 0.7144\n",
      "Iteration 2500 - Loss: 8.0173, Accuracy: 0.7141\n",
      "Iteration 3000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 3500 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 4000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 4500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 5500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0552, Train Accuracy: 0.7139, Val Loss: 8.0755, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/0fe04e84b926446992a1513eabc3fe20\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.8387, Accuracy: 0.3014\n",
      "Iteration 500 - Loss: 8.1559, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0925, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7139\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1233, Train Accuracy: 0.7134, Val Loss: 7.8730, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 16.0647, Accuracy: 0.1890\n",
      "Iteration 500 - Loss: 8.0422, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9788, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9713, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 2500 - Loss: 7.9698, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9692, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9692, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0085, Train Accuracy: 0.7214, Val Loss: 8.2421, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 16.2991, Accuracy: 0.1958\n",
      "Iteration 500 - Loss: 8.0178, Accuracy: 0.7202\n",
      "Iteration 1000 - Loss: 7.9533, Accuracy: 0.7214\n",
      "Iteration 1500 - Loss: 7.9458, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9835, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 16.2767, Accuracy: 0.2772\n",
      "Iteration 500 - Loss: 8.1549, Accuracy: 0.7169\n",
      "Iteration 1000 - Loss: 8.0902, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0823, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0807, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0801, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0797, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0794, Accuracy: 0.7167\n",
      "Iteration 4000 - Loss: 8.0792, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0789, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1201, Train Accuracy: 0.7161, Val Loss: 7.9509, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 14.9576, Accuracy: 0.3419\n",
      "Iteration 500 - Loss: 8.0912, Accuracy: 0.7151\n",
      "Iteration 1000 - Loss: 8.0269, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0191, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0179, Accuracy: 0.7144\n",
      "Iteration 2500 - Loss: 8.0175, Accuracy: 0.7146\n",
      "Iteration 3000 - Loss: 8.0174, Accuracy: 0.7146\n",
      "Iteration 3500 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0567, Train Accuracy: 0.7139, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/3491c0dec62c4192ad571ccd1f64d32f\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 13.9103, Accuracy: 0.3771\n",
      "Iteration 500 - Loss: 7.4736, Accuracy: 0.5685\n",
      "Iteration 1000 - Loss: 8.2167, Accuracy: 0.6691\n",
      "Iteration 1500 - Loss: 7.7265, Accuracy: 0.5488\n",
      "Iteration 2000 - Loss: 8.5029, Accuracy: 0.6039\n",
      "Iteration 2500 - Loss: 8.7129, Accuracy: 0.6402\n",
      "Iteration 3000 - Loss: 7.4561, Accuracy: 0.6076\n",
      "Iteration 3500 - Loss: 8.0293, Accuracy: 0.5755\n",
      "Iteration 4000 - Loss: 7.9094, Accuracy: 0.5572\n",
      "Iteration 4500 - Loss: 7.8586, Accuracy: 0.6649\n",
      "Iteration 5000 - Loss: 11.0692, Accuracy: 0.6306\n",
      "Iteration 5500 - Loss: 7.6518, Accuracy: 0.6635\n",
      "Iteration 6000 - Loss: 6.6052, Accuracy: 0.5901\n",
      "Iteration 6500 - Loss: 9.8455, Accuracy: 0.6538\n",
      "Fold: 0, Train Loss: 8.1805, Train Accuracy: 0.6057, Val Loss: 10.4032, Val Accuracy: 0.4972\n",
      "Iteration 0 - Loss: 16.1177, Accuracy: 0.4876\n",
      "Iteration 500 - Loss: 8.4744, Accuracy: 0.6470\n",
      "Iteration 1000 - Loss: 9.1866, Accuracy: 0.5943\n",
      "Iteration 1500 - Loss: 6.7139, Accuracy: 0.6625\n",
      "Iteration 2000 - Loss: 6.9765, Accuracy: 0.6100\n",
      "Iteration 2500 - Loss: 8.6229, Accuracy: 0.6576\n",
      "Iteration 3000 - Loss: 8.4800, Accuracy: 0.6417\n",
      "Iteration 3500 - Loss: 10.0404, Accuracy: 0.6377\n",
      "Iteration 4000 - Loss: 7.8871, Accuracy: 0.6130\n",
      "Iteration 4500 - Loss: 8.0345, Accuracy: 0.6023\n",
      "Iteration 5000 - Loss: 8.2747, Accuracy: 0.6782\n",
      "Iteration 5500 - Loss: 7.0253, Accuracy: 0.6034\n",
      "Iteration 6000 - Loss: 7.1663, Accuracy: 0.6295\n",
      "Iteration 6500 - Loss: 8.3383, Accuracy: 0.6100\n",
      "Fold: 1, Train Loss: 8.0931, Train Accuracy: 0.6100, Val Loss: 8.4464, Val Accuracy: 0.5938\n",
      "Iteration 0 - Loss: 17.6064, Accuracy: 0.3931\n",
      "Iteration 500 - Loss: 7.4004, Accuracy: 0.6098\n",
      "Iteration 1000 - Loss: 7.5219, Accuracy: 0.6684\n",
      "Iteration 1500 - Loss: 6.3351, Accuracy: 0.6597\n",
      "Iteration 2000 - Loss: 7.5747, Accuracy: 0.5877\n",
      "Iteration 2500 - Loss: 7.7093, Accuracy: 0.6642\n",
      "Iteration 3000 - Loss: 7.6315, Accuracy: 0.6086\n",
      "Iteration 3500 - Loss: 8.2956, Accuracy: 0.5701\n",
      "Iteration 4000 - Loss: 7.7169, Accuracy: 0.6283\n",
      "Iteration 4500 - Loss: 9.8199, Accuracy: 0.5368\n",
      "Iteration 5000 - Loss: 7.1905, Accuracy: 0.6189\n",
      "Iteration 5500 - Loss: 7.6421, Accuracy: 0.5643\n",
      "Iteration 6000 - Loss: 7.4844, Accuracy: 0.6224\n",
      "Iteration 6500 - Loss: 7.4597, Accuracy: 0.6313\n",
      "Fold: 2, Train Loss: 8.0477, Train Accuracy: 0.6122, Val Loss: 7.0990, Val Accuracy: 0.6576\n",
      "Iteration 0 - Loss: 17.0515, Accuracy: 0.4489\n",
      "Iteration 500 - Loss: 9.3224, Accuracy: 0.6388\n",
      "Iteration 1000 - Loss: 9.8948, Accuracy: 0.5769\n",
      "Iteration 1500 - Loss: 9.1401, Accuracy: 0.6510\n",
      "Iteration 2000 - Loss: 9.4320, Accuracy: 0.5816\n",
      "Iteration 2500 - Loss: 8.2975, Accuracy: 0.5650\n",
      "Iteration 3000 - Loss: 7.2886, Accuracy: 0.6037\n",
      "Iteration 3500 - Loss: 10.0403, Accuracy: 0.6234\n",
      "Iteration 4000 - Loss: 9.2611, Accuracy: 0.6067\n",
      "Iteration 4500 - Loss: 7.2029, Accuracy: 0.5772\n",
      "Iteration 5000 - Loss: 7.1478, Accuracy: 0.6714\n",
      "Iteration 5500 - Loss: 8.3385, Accuracy: 0.5849\n",
      "Iteration 6000 - Loss: 8.1084, Accuracy: 0.6135\n",
      "Iteration 6500 - Loss: 10.6418, Accuracy: 0.6447\n",
      "Fold: 3, Train Loss: 8.1891, Train Accuracy: 0.6053, Val Loss: 8.2974, Val Accuracy: 0.6032\n",
      "Iteration 0 - Loss: 15.8704, Accuracy: 0.3771\n",
      "Iteration 500 - Loss: 6.4985, Accuracy: 0.5610\n",
      "Iteration 1000 - Loss: 6.9006, Accuracy: 0.6365\n",
      "Iteration 1500 - Loss: 7.6791, Accuracy: 0.5779\n",
      "Iteration 2000 - Loss: 8.1286, Accuracy: 0.6119\n",
      "Iteration 2500 - Loss: 7.6045, Accuracy: 0.5568\n",
      "Iteration 3000 - Loss: 8.9111, Accuracy: 0.5432\n",
      "Iteration 3500 - Loss: 10.2830, Accuracy: 0.6140\n",
      "Iteration 4000 - Loss: 6.9396, Accuracy: 0.6822\n",
      "Iteration 4500 - Loss: 8.9429, Accuracy: 0.6468\n",
      "Iteration 5000 - Loss: 8.8015, Accuracy: 0.6248\n",
      "Iteration 5500 - Loss: 8.7632, Accuracy: 0.6241\n",
      "Iteration 6000 - Loss: 8.1084, Accuracy: 0.6212\n",
      "Iteration 6500 - Loss: 7.6769, Accuracy: 0.5387\n",
      "Fold: 4, Train Loss: 8.1466, Train Accuracy: 0.6074, Val Loss: 7.4668, Val Accuracy: 0.6388\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.69       0.92       0.79      \n",
      "1          0.61       0.49       0.54      \n",
      "2          0.47       0.36       0.41      \n",
      "3          0.62       0.78       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6200 | F1-Score: 0.6000\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/e3601584012c4907ab17177945755b76\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 17.8297, Accuracy: 0.4167\n",
      "Iteration 500 - Loss: 7.1121, Accuracy: 0.5969\n",
      "Iteration 1000 - Loss: 8.5647, Accuracy: 0.6327\n",
      "Iteration 1500 - Loss: 7.1752, Accuracy: 0.6904\n",
      "Iteration 2000 - Loss: 8.8162, Accuracy: 0.5331\n",
      "Iteration 2500 - Loss: 8.6089, Accuracy: 0.6292\n",
      "Iteration 3000 - Loss: 6.6982, Accuracy: 0.5896\n",
      "Iteration 3500 - Loss: 6.5730, Accuracy: 0.5589\n",
      "Iteration 4000 - Loss: 6.9483, Accuracy: 0.6384\n",
      "Iteration 4500 - Loss: 8.3216, Accuracy: 0.5617\n",
      "Iteration 5000 - Loss: 7.8892, Accuracy: 0.5497\n",
      "Iteration 5500 - Loss: 9.0709, Accuracy: 0.5990\n",
      "Iteration 6000 - Loss: 8.4505, Accuracy: 0.5987\n",
      "Iteration 6500 - Loss: 7.5757, Accuracy: 0.6323\n",
      "Fold: 0, Train Loss: 8.1826, Train Accuracy: 0.6056, Val Loss: 8.9129, Val Accuracy: 0.5675\n",
      "Iteration 0 - Loss: 17.9309, Accuracy: 0.4702\n",
      "Iteration 500 - Loss: 6.3746, Accuracy: 0.6203\n",
      "Iteration 1000 - Loss: 9.2545, Accuracy: 0.6222\n",
      "Iteration 1500 - Loss: 7.8443, Accuracy: 0.6128\n",
      "Iteration 2000 - Loss: 8.1341, Accuracy: 0.5786\n",
      "Iteration 2500 - Loss: 7.1857, Accuracy: 0.5966\n",
      "Iteration 3000 - Loss: 7.5591, Accuracy: 0.6367\n",
      "Iteration 3500 - Loss: 7.5337, Accuracy: 0.6689\n",
      "Iteration 4000 - Loss: 8.8841, Accuracy: 0.6217\n",
      "Iteration 4500 - Loss: 8.7177, Accuracy: 0.6107\n",
      "Iteration 5000 - Loss: 7.3586, Accuracy: 0.6660\n",
      "Iteration 5500 - Loss: 7.3203, Accuracy: 0.6252\n",
      "Iteration 6000 - Loss: 7.7166, Accuracy: 0.6480\n",
      "Iteration 6500 - Loss: 7.9214, Accuracy: 0.6144\n",
      "Fold: 1, Train Loss: 8.0918, Train Accuracy: 0.6101, Val Loss: 7.3515, Val Accuracy: 0.6463\n",
      "Iteration 0 - Loss: 18.3109, Accuracy: 0.4599\n",
      "Iteration 500 - Loss: 7.9722, Accuracy: 0.5865\n",
      "Iteration 1000 - Loss: 7.6627, Accuracy: 0.5636\n",
      "Iteration 1500 - Loss: 6.9616, Accuracy: 0.6541\n",
      "Iteration 2000 - Loss: 7.2708, Accuracy: 0.6564\n",
      "Iteration 2500 - Loss: 7.0990, Accuracy: 0.5971\n",
      "Iteration 3000 - Loss: 7.8329, Accuracy: 0.5673\n",
      "Iteration 3500 - Loss: 7.5651, Accuracy: 0.6257\n",
      "Iteration 4000 - Loss: 8.9667, Accuracy: 0.5983\n",
      "Iteration 4500 - Loss: 7.1994, Accuracy: 0.5919\n",
      "Iteration 5000 - Loss: 6.9844, Accuracy: 0.6724\n",
      "Iteration 5500 - Loss: 7.5888, Accuracy: 0.5708\n",
      "Iteration 6000 - Loss: 9.2415, Accuracy: 0.6576\n",
      "Iteration 6500 - Loss: 9.2006, Accuracy: 0.6541\n",
      "Fold: 2, Train Loss: 8.0467, Train Accuracy: 0.6122, Val Loss: 7.3850, Val Accuracy: 0.6426\n",
      "Iteration 0 - Loss: 17.5729, Accuracy: 0.3745\n",
      "Iteration 500 - Loss: 6.3376, Accuracy: 0.5565\n",
      "Iteration 1000 - Loss: 9.1795, Accuracy: 0.6316\n",
      "Iteration 1500 - Loss: 9.5691, Accuracy: 0.6496\n",
      "Iteration 2000 - Loss: 8.0981, Accuracy: 0.6208\n",
      "Iteration 2500 - Loss: 9.4354, Accuracy: 0.5905\n",
      "Iteration 3000 - Loss: 9.8877, Accuracy: 0.6159\n",
      "Iteration 3500 - Loss: 7.5716, Accuracy: 0.6325\n",
      "Iteration 4000 - Loss: 6.5393, Accuracy: 0.6564\n",
      "Iteration 4500 - Loss: 8.0037, Accuracy: 0.6407\n",
      "Iteration 5000 - Loss: 7.7637, Accuracy: 0.5385\n",
      "Iteration 5500 - Loss: 9.8580, Accuracy: 0.6330\n",
      "Iteration 6000 - Loss: 6.5565, Accuracy: 0.6937\n",
      "Iteration 6500 - Loss: 9.5111, Accuracy: 0.6696\n",
      "Fold: 3, Train Loss: 8.1932, Train Accuracy: 0.6051, Val Loss: 6.7520, Val Accuracy: 0.6717\n",
      "Iteration 0 - Loss: 15.2398, Accuracy: 0.3959\n",
      "Iteration 500 - Loss: 9.6608, Accuracy: 0.5443\n",
      "Iteration 1000 - Loss: 9.3366, Accuracy: 0.6632\n",
      "Iteration 1500 - Loss: 9.5632, Accuracy: 0.6470\n",
      "Iteration 2000 - Loss: 8.5304, Accuracy: 0.6693\n",
      "Iteration 2500 - Loss: 7.8880, Accuracy: 0.5335\n",
      "Iteration 3000 - Loss: 8.4421, Accuracy: 0.5729\n",
      "Iteration 3500 - Loss: 7.9868, Accuracy: 0.6916\n",
      "Iteration 4000 - Loss: 8.0623, Accuracy: 0.6069\n",
      "Iteration 4500 - Loss: 9.3113, Accuracy: 0.6262\n",
      "Iteration 5000 - Loss: 8.0493, Accuracy: 0.5830\n",
      "Iteration 5500 - Loss: 9.5347, Accuracy: 0.5211\n",
      "Iteration 6000 - Loss: 7.9459, Accuracy: 0.6212\n",
      "Iteration 6500 - Loss: 7.8495, Accuracy: 0.6152\n",
      "Fold: 4, Train Loss: 8.1454, Train Accuracy: 0.6074, Val Loss: 7.6823, Val Accuracy: 0.6341\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.97       0.49       0.65      \n",
      "1          0.54       0.84       0.66      \n",
      "2          0.50       0.47       0.49      \n",
      "3          0.84       0.54       0.66      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6200 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/2db2bc3a6b08497f9b51d17103a38185\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 16.7325, Accuracy: 0.3898\n",
      "Iteration 500 - Loss: 7.5324, Accuracy: 0.5399\n",
      "Iteration 1000 - Loss: 8.8781, Accuracy: 0.5856\n",
      "Iteration 1500 - Loss: 9.0810, Accuracy: 0.5610\n",
      "Iteration 2000 - Loss: 8.2480, Accuracy: 0.5288\n",
      "Iteration 2500 - Loss: 7.0746, Accuracy: 0.5558\n",
      "Iteration 3000 - Loss: 7.5875, Accuracy: 0.5176\n",
      "Iteration 3500 - Loss: 8.1608, Accuracy: 0.6524\n",
      "Iteration 4000 - Loss: 7.1010, Accuracy: 0.5804\n",
      "Iteration 4500 - Loss: 7.8710, Accuracy: 0.5708\n",
      "Iteration 5000 - Loss: 8.1867, Accuracy: 0.6517\n",
      "Iteration 5500 - Loss: 10.3348, Accuracy: 0.6227\n",
      "Iteration 6000 - Loss: 9.6265, Accuracy: 0.5675\n",
      "Iteration 6500 - Loss: 8.6374, Accuracy: 0.6435\n",
      "Fold: 0, Train Loss: 8.1854, Train Accuracy: 0.6055, Val Loss: 6.7201, Val Accuracy: 0.6829\n",
      "Iteration 0 - Loss: 14.4606, Accuracy: 0.4773\n",
      "Iteration 500 - Loss: 9.4960, Accuracy: 0.6395\n",
      "Iteration 1000 - Loss: 7.5742, Accuracy: 0.5607\n",
      "Iteration 1500 - Loss: 8.6168, Accuracy: 0.6311\n",
      "Iteration 2000 - Loss: 6.6616, Accuracy: 0.6283\n",
      "Iteration 2500 - Loss: 9.7804, Accuracy: 0.6449\n",
      "Iteration 3000 - Loss: 6.6152, Accuracy: 0.6351\n",
      "Iteration 3500 - Loss: 7.8365, Accuracy: 0.6367\n",
      "Iteration 4000 - Loss: 7.7380, Accuracy: 0.6710\n",
      "Iteration 4500 - Loss: 6.8369, Accuracy: 0.5368\n",
      "Iteration 5000 - Loss: 7.1604, Accuracy: 0.6372\n",
      "Iteration 5500 - Loss: 7.7035, Accuracy: 0.6018\n",
      "Iteration 6000 - Loss: 9.0514, Accuracy: 0.6020\n",
      "Iteration 6500 - Loss: 8.2486, Accuracy: 0.5962\n",
      "Fold: 1, Train Loss: 8.0976, Train Accuracy: 0.6097, Val Loss: 9.1503, Val Accuracy: 0.5544\n",
      "Iteration 0 - Loss: 17.6412, Accuracy: 0.4386\n",
      "Iteration 500 - Loss: 7.9819, Accuracy: 0.6154\n",
      "Iteration 1000 - Loss: 9.0543, Accuracy: 0.6470\n",
      "Iteration 1500 - Loss: 6.8895, Accuracy: 0.5879\n",
      "Iteration 2000 - Loss: 9.7679, Accuracy: 0.6473\n",
      "Iteration 2500 - Loss: 7.5577, Accuracy: 0.6595\n",
      "Iteration 3000 - Loss: 10.0853, Accuracy: 0.6337\n",
      "Iteration 3500 - Loss: 7.5596, Accuracy: 0.5687\n",
      "Iteration 4000 - Loss: 10.3229, Accuracy: 0.6815\n",
      "Iteration 4500 - Loss: 9.4223, Accuracy: 0.5847\n",
      "Iteration 5000 - Loss: 6.5493, Accuracy: 0.6112\n",
      "Iteration 5500 - Loss: 7.1956, Accuracy: 0.6975\n",
      "Iteration 6000 - Loss: 8.6083, Accuracy: 0.6316\n",
      "Iteration 6500 - Loss: 6.9325, Accuracy: 0.6283\n",
      "Fold: 2, Train Loss: 8.0461, Train Accuracy: 0.6123, Val Loss: 9.4192, Val Accuracy: 0.5478\n",
      "Iteration 0 - Loss: 15.6755, Accuracy: 0.4003\n",
      "Iteration 500 - Loss: 7.5742, Accuracy: 0.5671\n",
      "Iteration 1000 - Loss: 6.7521, Accuracy: 0.6555\n",
      "Iteration 1500 - Loss: 8.2809, Accuracy: 0.6116\n",
      "Iteration 2000 - Loss: 9.1908, Accuracy: 0.6008\n",
      "Iteration 2500 - Loss: 7.9285, Accuracy: 0.6121\n",
      "Iteration 3000 - Loss: 8.0219, Accuracy: 0.5985\n",
      "Iteration 3500 - Loss: 8.3533, Accuracy: 0.5802\n",
      "Iteration 4000 - Loss: 9.6798, Accuracy: 0.5443\n",
      "Iteration 4500 - Loss: 7.9580, Accuracy: 0.6609\n",
      "Iteration 5000 - Loss: 10.8838, Accuracy: 0.5823\n",
      "Iteration 5500 - Loss: 7.0699, Accuracy: 0.6198\n",
      "Iteration 6000 - Loss: 8.7663, Accuracy: 0.6076\n",
      "Iteration 6500 - Loss: 9.5018, Accuracy: 0.6417\n",
      "Fold: 3, Train Loss: 8.1928, Train Accuracy: 0.6051, Val Loss: 9.3999, Val Accuracy: 0.5441\n",
      "Iteration 0 - Loss: 17.2361, Accuracy: 0.3572\n",
      "Iteration 500 - Loss: 8.3356, Accuracy: 0.5631\n",
      "Iteration 1000 - Loss: 8.7232, Accuracy: 0.6492\n",
      "Iteration 1500 - Loss: 8.7252, Accuracy: 0.6508\n",
      "Iteration 2000 - Loss: 8.2281, Accuracy: 0.5314\n",
      "Iteration 2500 - Loss: 7.7390, Accuracy: 0.5530\n",
      "Iteration 3000 - Loss: 7.5228, Accuracy: 0.4838\n",
      "Iteration 3500 - Loss: 9.5838, Accuracy: 0.6102\n",
      "Iteration 4000 - Loss: 9.3152, Accuracy: 0.5183\n",
      "Iteration 4500 - Loss: 9.2146, Accuracy: 0.6309\n",
      "Iteration 5000 - Loss: 7.0885, Accuracy: 0.6198\n",
      "Iteration 5500 - Loss: 7.5566, Accuracy: 0.5584\n",
      "Iteration 6000 - Loss: 8.2010, Accuracy: 0.6166\n",
      "Iteration 6500 - Loss: 10.9157, Accuracy: 0.6487\n",
      "Fold: 4, Train Loss: 8.1368, Train Accuracy: 0.6079, Val Loss: 7.4966, Val Accuracy: 0.6407\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.63       0.95       0.76      \n",
      "1          0.66       0.44       0.53      \n",
      "2          0.54       0.42       0.47      \n",
      "3          0.63       0.80       0.70      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6200 | F1-Score: 0.6000\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/b26587ff497c4540b639e6b131e91f58\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 14.9316, Accuracy: 0.3433\n",
      "Iteration 500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Fold: 0, Train Loss: 8.1035, Train Accuracy: 0.6966, Val Loss: 7.8284, Val Accuracy: 0.7139\n",
      "Iteration 0 - Loss: 16.4114, Accuracy: 0.3335\n",
      "Iteration 500 - Loss: 7.9030, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9889, Train Accuracy: 0.7036, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 11.8786, Accuracy: 0.3841\n",
      "Iteration 500 - Loss: 7.8990, Accuracy: 0.7160\n",
      "Iteration 1000 - Loss: 7.8989, Accuracy: 0.7160\n",
      "Iteration 1500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 2000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 2500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 3000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 3500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 4000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 4500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 5000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 5500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 6000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 6500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Fold: 2, Train Loss: 7.9579, Train Accuracy: 0.7123, Val Loss: 8.1494, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 14.0098, Accuracy: 0.3654\n",
      "Iteration 500 - Loss: 8.1732, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1723, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1723, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1007, Train Accuracy: 0.6968, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 17.6463, Accuracy: 0.4611\n",
      "Iteration 500 - Loss: 8.0944, Accuracy: 0.7064\n",
      "Iteration 1000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Fold: 4, Train Loss: 8.0287, Train Accuracy: 0.7082, Val Loss: 8.1073, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.83       0.81      \n",
      "1          0.71       0.63       0.67      \n",
      "2          0.54       0.73       0.62      \n",
      "3          0.86       0.58       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/c12567669a7c497b80cb0e749d82ed6e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 16.4122, Accuracy: 0.4683\n",
      "Iteration 500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Fold: 0, Train Loss: 8.1036, Train Accuracy: 0.6967, Val Loss: 7.9758, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 16.2806, Accuracy: 0.5113\n",
      "Iteration 500 - Loss: 7.9029, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9891, Train Accuracy: 0.7037, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 16.2893, Accuracy: 0.3192\n",
      "Iteration 500 - Loss: 8.0119, Accuracy: 0.7090\n",
      "Iteration 1000 - Loss: 8.0116, Accuracy: 0.7094\n",
      "Iteration 1500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Fold: 2, Train Loss: 7.9566, Train Accuracy: 0.7123, Val Loss: 8.3286, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.8452, Accuracy: 0.4681\n",
      "Iteration 500 - Loss: 8.0262, Accuracy: 0.7012\n",
      "Iteration 1000 - Loss: 8.0258, Accuracy: 0.7012\n",
      "Iteration 1500 - Loss: 8.0256, Accuracy: 0.7012\n",
      "Iteration 2000 - Loss: 8.0255, Accuracy: 0.7012\n",
      "Iteration 2500 - Loss: 8.0255, Accuracy: 0.7012\n",
      "Iteration 3000 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 3500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 4000 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 4500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 5000 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Iteration 5500 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Iteration 6000 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Iteration 6500 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6963, Val Loss: 7.8488, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 16.0782, Accuracy: 0.3745\n",
      "Iteration 500 - Loss: 8.0945, Accuracy: 0.7064\n",
      "Iteration 1000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Fold: 4, Train Loss: 8.0295, Train Accuracy: 0.7082, Val Loss: 8.1073, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.83       0.81      \n",
      "1          0.71       0.63       0.67      \n",
      "2          0.54       0.73       0.62      \n",
      "3          0.86       0.58       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/338453d3ac9840b59bdcffa3c2d3a300\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5396, Accuracy: 0.4770\n",
      "Iteration 500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Fold: 0, Train Loss: 8.1038, Train Accuracy: 0.6967, Val Loss: 7.8284, Val Accuracy: 0.7139\n",
      "Iteration 0 - Loss: 16.8843, Accuracy: 0.3621\n",
      "Iteration 500 - Loss: 8.0752, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.0746, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 8.0745, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Fold: 1, Train Loss: 7.9893, Train Accuracy: 0.7037, Val Loss: 8.2928, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 16.3244, Accuracy: 0.4576\n",
      "Iteration 500 - Loss: 8.0116, Accuracy: 0.7094\n",
      "Iteration 1000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Fold: 2, Train Loss: 7.9564, Train Accuracy: 0.7124, Val Loss: 8.3286, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 14.9709, Accuracy: 0.3752\n",
      "Iteration 500 - Loss: 8.0266, Accuracy: 0.7015\n",
      "Iteration 1000 - Loss: 8.0259, Accuracy: 0.7015\n",
      "Iteration 1500 - Loss: 8.0257, Accuracy: 0.7012\n",
      "Iteration 2000 - Loss: 8.0256, Accuracy: 0.7012\n",
      "Iteration 2500 - Loss: 8.0255, Accuracy: 0.7012\n",
      "Iteration 3000 - Loss: 8.0255, Accuracy: 0.7012\n",
      "Iteration 3500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 4000 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 4500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 5000 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 5500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 6000 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Iteration 6500 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Fold: 3, Train Loss: 8.1003, Train Accuracy: 0.6969, Val Loss: 7.8498, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 14.6904, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 8.0946, Accuracy: 0.7064\n",
      "Iteration 1000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Fold: 4, Train Loss: 8.0299, Train Accuracy: 0.7083, Val Loss: 8.1073, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.83       0.81      \n",
      "1          0.71       0.63       0.67      \n",
      "2          0.54       0.73       0.62      \n",
      "3          0.86       0.58       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/49fb09869358422b9bf3caba630425c6\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 18.6307, Accuracy: 0.0708\n",
      "Iteration 500 - Loss: 8.1520, Accuracy: 0.7134\n",
      "Iteration 1000 - Loss: 8.0922, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0852, Accuracy: 0.7139\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7141\n",
      "Iteration 2500 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1208, Train Accuracy: 0.7133, Val Loss: 7.8728, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 15.3871, Accuracy: 0.3016\n",
      "Iteration 500 - Loss: 8.0433, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9800, Accuracy: 0.7207\n",
      "Iteration 1500 - Loss: 7.9724, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9711, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9706, Accuracy: 0.7223\n",
      "Iteration 3000 - Loss: 7.9703, Accuracy: 0.7223\n",
      "Iteration 3500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9700, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9698, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0091, Train Accuracy: 0.7213, Val Loss: 8.2417, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 14.0793, Accuracy: 0.3499\n",
      "Iteration 500 - Loss: 8.0181, Accuracy: 0.7209\n",
      "Iteration 1000 - Loss: 7.9533, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9457, Accuracy: 0.7209\n",
      "Iteration 2000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9827, Train Accuracy: 0.7205, Val Loss: 8.2310, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.8129, Accuracy: 0.2371\n",
      "Iteration 500 - Loss: 8.1588, Accuracy: 0.7165\n",
      "Iteration 1000 - Loss: 8.0911, Accuracy: 0.7174\n",
      "Iteration 1500 - Loss: 8.0826, Accuracy: 0.7169\n",
      "Iteration 2000 - Loss: 8.0808, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0801, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0797, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7167\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7167\n",
      "Iteration 4500 - Loss: 8.0789, Accuracy: 0.7167\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7167\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0785, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1208, Train Accuracy: 0.7158, Val Loss: 7.9511, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 13.8590, Accuracy: 0.3408\n",
      "Iteration 500 - Loss: 8.0915, Accuracy: 0.7155\n",
      "Iteration 1000 - Loss: 8.0262, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0187, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0176, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0173, Accuracy: 0.7146\n",
      "Iteration 3500 - Loss: 8.0173, Accuracy: 0.7146\n",
      "Iteration 4000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 5000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Fold: 4, Train Loss: 8.0581, Train Accuracy: 0.7137, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/b71bc1bcf10e49dbaec311715c18274a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 14.9818, Accuracy: 0.2866\n",
      "Iteration 500 - Loss: 8.1566, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0925, Accuracy: 0.7136\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7136\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7136\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1238, Train Accuracy: 0.7133, Val Loss: 7.8728, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 16.0460, Accuracy: 0.2047\n",
      "Iteration 500 - Loss: 8.0405, Accuracy: 0.7200\n",
      "Iteration 1000 - Loss: 7.9781, Accuracy: 0.7205\n",
      "Iteration 1500 - Loss: 7.9710, Accuracy: 0.7221\n",
      "Iteration 2000 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 2500 - Loss: 7.9698, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0098, Train Accuracy: 0.7210, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 17.4287, Accuracy: 0.1686\n",
      "Iteration 500 - Loss: 8.0162, Accuracy: 0.7212\n",
      "Iteration 1000 - Loss: 7.9516, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9447, Accuracy: 0.7209\n",
      "Iteration 2000 - Loss: 7.9441, Accuracy: 0.7209\n",
      "Iteration 2500 - Loss: 7.9442, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9442, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9847, Train Accuracy: 0.7203, Val Loss: 8.2310, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 13.7173, Accuracy: 0.3872\n",
      "Iteration 500 - Loss: 8.1523, Accuracy: 0.7167\n",
      "Iteration 1000 - Loss: 8.0909, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0831, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0814, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0806, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0801, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0797, Accuracy: 0.7167\n",
      "Iteration 4000 - Loss: 8.0794, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0792, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0789, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1152, Train Accuracy: 0.7161, Val Loss: 7.9501, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 14.7365, Accuracy: 0.3290\n",
      "Iteration 500 - Loss: 8.0811, Accuracy: 0.7148\n",
      "Iteration 1000 - Loss: 8.0231, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Iteration 2000 - Loss: 8.0163, Accuracy: 0.7148\n",
      "Iteration 2500 - Loss: 8.0163, Accuracy: 0.7148\n",
      "Iteration 3000 - Loss: 8.0164, Accuracy: 0.7148\n",
      "Iteration 3500 - Loss: 8.0165, Accuracy: 0.7148\n",
      "Iteration 4000 - Loss: 8.0166, Accuracy: 0.7146\n",
      "Iteration 4500 - Loss: 8.0166, Accuracy: 0.7146\n",
      "Iteration 5000 - Loss: 8.0167, Accuracy: 0.7146\n",
      "Iteration 5500 - Loss: 8.0167, Accuracy: 0.7146\n",
      "Iteration 6000 - Loss: 8.0167, Accuracy: 0.7146\n",
      "Iteration 6500 - Loss: 8.0167, Accuracy: 0.7146\n",
      "Fold: 4, Train Loss: 8.0515, Train Accuracy: 0.7140, Val Loss: 8.0758, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/406e2491005443cbb6a89205d5252904\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.3138, Accuracy: 0.3323\n",
      "Iteration 500 - Loss: 8.1557, Accuracy: 0.7132\n",
      "Iteration 1000 - Loss: 8.0928, Accuracy: 0.7136\n",
      "Iteration 1500 - Loss: 8.0855, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0844, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1221, Train Accuracy: 0.7132, Val Loss: 7.8729, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 13.8625, Accuracy: 0.3440\n",
      "Iteration 500 - Loss: 8.0429, Accuracy: 0.7200\n",
      "Iteration 1000 - Loss: 7.9784, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9708, Accuracy: 0.7221\n",
      "Iteration 2000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 2500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9692, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9692, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0082, Train Accuracy: 0.7212, Val Loss: 8.2424, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 16.5926, Accuracy: 0.2261\n",
      "Iteration 500 - Loss: 8.0195, Accuracy: 0.7212\n",
      "Iteration 1000 - Loss: 7.9534, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9458, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9447, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9837, Train Accuracy: 0.7205, Val Loss: 8.2311, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 14.8037, Accuracy: 0.3152\n",
      "Iteration 500 - Loss: 8.1520, Accuracy: 0.7167\n",
      "Iteration 1000 - Loss: 8.0887, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0813, Accuracy: 0.7169\n",
      "Iteration 2000 - Loss: 8.0799, Accuracy: 0.7169\n",
      "Iteration 2500 - Loss: 8.0795, Accuracy: 0.7169\n",
      "Iteration 3000 - Loss: 8.0792, Accuracy: 0.7169\n",
      "Iteration 3500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0786, Accuracy: 0.7172\n",
      "Iteration 5500 - Loss: 8.0785, Accuracy: 0.7172\n",
      "Iteration 6000 - Loss: 8.0784, Accuracy: 0.7172\n",
      "Iteration 6500 - Loss: 8.0784, Accuracy: 0.7172\n",
      "Fold: 3, Train Loss: 8.1196, Train Accuracy: 0.7161, Val Loss: 7.9524, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 14.2205, Accuracy: 0.3860\n",
      "Iteration 500 - Loss: 8.0890, Accuracy: 0.7153\n",
      "Iteration 1000 - Loss: 8.0264, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0191, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0180, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0178, Accuracy: 0.7146\n",
      "Iteration 3000 - Loss: 8.0176, Accuracy: 0.7146\n",
      "Iteration 3500 - Loss: 8.0175, Accuracy: 0.7146\n",
      "Iteration 4000 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0173, Accuracy: 0.7146\n",
      "Iteration 5000 - Loss: 8.0172, Accuracy: 0.7146\n",
      "Iteration 5500 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Fold: 4, Train Loss: 8.0542, Train Accuracy: 0.7141, Val Loss: 8.0752, Val Accuracy: 0.7139\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/c1f26965ef5341aca5c3ad01bfd760f4\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5690, Accuracy: 0.4998\n",
      "Iteration 500 - Loss: 8.2672, Accuracy: 0.6360\n",
      "Iteration 1000 - Loss: 7.1318, Accuracy: 0.5633\n",
      "Iteration 1500 - Loss: 7.8229, Accuracy: 0.6667\n",
      "Iteration 2000 - Loss: 7.6225, Accuracy: 0.6435\n",
      "Iteration 2500 - Loss: 7.5931, Accuracy: 0.4665\n",
      "Iteration 3000 - Loss: 7.8435, Accuracy: 0.6060\n",
      "Iteration 3500 - Loss: 7.6291, Accuracy: 0.5962\n",
      "Iteration 4000 - Loss: 8.3781, Accuracy: 0.6379\n",
      "Iteration 4500 - Loss: 8.1927, Accuracy: 0.5851\n",
      "Iteration 5000 - Loss: 7.9868, Accuracy: 0.6614\n",
      "Iteration 5500 - Loss: 8.5416, Accuracy: 0.6538\n",
      "Iteration 6000 - Loss: 8.1869, Accuracy: 0.6008\n",
      "Iteration 6500 - Loss: 10.4090, Accuracy: 0.6128\n",
      "Fold: 0, Train Loss: 8.1869, Train Accuracy: 0.6054, Val Loss: 8.7326, Val Accuracy: 0.5797\n",
      "Iteration 0 - Loss: 15.5251, Accuracy: 0.5038\n",
      "Iteration 500 - Loss: 7.5670, Accuracy: 0.6503\n",
      "Iteration 1000 - Loss: 9.4123, Accuracy: 0.6384\n",
      "Iteration 1500 - Loss: 10.1644, Accuracy: 0.6367\n",
      "Iteration 2000 - Loss: 6.3164, Accuracy: 0.6395\n",
      "Iteration 2500 - Loss: 9.9492, Accuracy: 0.6698\n",
      "Iteration 3000 - Loss: 8.7348, Accuracy: 0.5251\n",
      "Iteration 3500 - Loss: 8.5848, Accuracy: 0.6506\n",
      "Iteration 4000 - Loss: 9.4843, Accuracy: 0.6543\n",
      "Iteration 4500 - Loss: 6.2733, Accuracy: 0.6438\n",
      "Iteration 5000 - Loss: 7.5791, Accuracy: 0.5436\n",
      "Iteration 5500 - Loss: 7.9136, Accuracy: 0.5741\n",
      "Iteration 6000 - Loss: 7.4644, Accuracy: 0.6646\n",
      "Iteration 6500 - Loss: 8.2711, Accuracy: 0.6098\n",
      "Fold: 1, Train Loss: 8.0958, Train Accuracy: 0.6098, Val Loss: 9.0460, Val Accuracy: 0.5619\n",
      "Iteration 0 - Loss: 15.5147, Accuracy: 0.5108\n",
      "Iteration 500 - Loss: 8.4445, Accuracy: 0.6123\n",
      "Iteration 1000 - Loss: 6.4799, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.9220, Accuracy: 0.6503\n",
      "Iteration 2000 - Loss: 8.1223, Accuracy: 0.5288\n",
      "Iteration 2500 - Loss: 8.5500, Accuracy: 0.6599\n",
      "Iteration 3000 - Loss: 7.6294, Accuracy: 0.6114\n",
      "Iteration 3500 - Loss: 7.7760, Accuracy: 0.6023\n",
      "Iteration 4000 - Loss: 7.6897, Accuracy: 0.6407\n",
      "Iteration 4500 - Loss: 8.1864, Accuracy: 0.6341\n",
      "Iteration 5000 - Loss: 7.7927, Accuracy: 0.6123\n",
      "Iteration 5500 - Loss: 8.5220, Accuracy: 0.5500\n",
      "Iteration 6000 - Loss: 8.1932, Accuracy: 0.6257\n",
      "Iteration 6500 - Loss: 9.0870, Accuracy: 0.5650\n",
      "Fold: 2, Train Loss: 8.0451, Train Accuracy: 0.6123, Val Loss: 8.6965, Val Accuracy: 0.5816\n",
      "Iteration 0 - Loss: 15.5350, Accuracy: 0.5009\n",
      "Iteration 500 - Loss: 8.3296, Accuracy: 0.6902\n",
      "Iteration 1000 - Loss: 8.3748, Accuracy: 0.5434\n",
      "Iteration 1500 - Loss: 8.8254, Accuracy: 0.5952\n",
      "Iteration 2000 - Loss: 8.4075, Accuracy: 0.6639\n",
      "Iteration 2500 - Loss: 8.0660, Accuracy: 0.5579\n",
      "Iteration 3000 - Loss: 6.8774, Accuracy: 0.5804\n",
      "Iteration 3500 - Loss: 7.9578, Accuracy: 0.6147\n",
      "Iteration 4000 - Loss: 6.3721, Accuracy: 0.5844\n",
      "Iteration 4500 - Loss: 6.3400, Accuracy: 0.6477\n",
      "Iteration 5000 - Loss: 6.7934, Accuracy: 0.6384\n",
      "Iteration 5500 - Loss: 8.8424, Accuracy: 0.6123\n",
      "Iteration 6000 - Loss: 8.5839, Accuracy: 0.6388\n",
      "Iteration 6500 - Loss: 7.5149, Accuracy: 0.5861\n",
      "Fold: 3, Train Loss: 8.1905, Train Accuracy: 0.6053, Val Loss: 6.6200, Val Accuracy: 0.6820\n",
      "Iteration 0 - Loss: 15.5494, Accuracy: 0.5012\n",
      "Iteration 500 - Loss: 8.6684, Accuracy: 0.6424\n",
      "Iteration 1000 - Loss: 7.7154, Accuracy: 0.5607\n",
      "Iteration 1500 - Loss: 7.6474, Accuracy: 0.5645\n",
      "Iteration 2000 - Loss: 7.2598, Accuracy: 0.5983\n",
      "Iteration 2500 - Loss: 9.3288, Accuracy: 0.5575\n",
      "Iteration 3000 - Loss: 8.1078, Accuracy: 0.5915\n",
      "Iteration 3500 - Loss: 6.3340, Accuracy: 0.6796\n",
      "Iteration 4000 - Loss: 7.8539, Accuracy: 0.5689\n",
      "Iteration 4500 - Loss: 6.9437, Accuracy: 0.5434\n",
      "Iteration 5000 - Loss: 7.7101, Accuracy: 0.6098\n",
      "Iteration 5500 - Loss: 7.9671, Accuracy: 0.5692\n",
      "Iteration 6000 - Loss: 7.0538, Accuracy: 0.5753\n",
      "Iteration 6500 - Loss: 7.3923, Accuracy: 0.6656\n",
      "Fold: 4, Train Loss: 8.1397, Train Accuracy: 0.6077, Val Loss: 7.1885, Val Accuracy: 0.6557\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.73       0.85       0.78      \n",
      "1          0.64       0.54       0.59      \n",
      "2          0.48       0.42       0.45      \n",
      "3          0.60       0.76       0.67      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6300 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/c7948461e7bc46c5912802845db996bc\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5503, Accuracy: 0.5021\n",
      "Iteration 500 - Loss: 8.2772, Accuracy: 0.6116\n",
      "Iteration 1000 - Loss: 7.3692, Accuracy: 0.6241\n",
      "Iteration 1500 - Loss: 7.7666, Accuracy: 0.6105\n",
      "Iteration 2000 - Loss: 9.2469, Accuracy: 0.6339\n",
      "Iteration 2500 - Loss: 7.7631, Accuracy: 0.6520\n",
      "Iteration 3000 - Loss: 8.7153, Accuracy: 0.5879\n",
      "Iteration 3500 - Loss: 7.6537, Accuracy: 0.6006\n",
      "Iteration 4000 - Loss: 9.1431, Accuracy: 0.5788\n",
      "Iteration 4500 - Loss: 9.6048, Accuracy: 0.6187\n",
      "Iteration 5000 - Loss: 6.8403, Accuracy: 0.5328\n",
      "Iteration 5500 - Loss: 9.2222, Accuracy: 0.5856\n",
      "Iteration 6000 - Loss: 8.6368, Accuracy: 0.6161\n",
      "Iteration 6500 - Loss: 9.5417, Accuracy: 0.5621\n",
      "Fold: 0, Train Loss: 8.1868, Train Accuracy: 0.6054, Val Loss: 7.5114, Val Accuracy: 0.6379\n",
      "Iteration 0 - Loss: 15.5581, Accuracy: 0.5096\n",
      "Iteration 500 - Loss: 7.7943, Accuracy: 0.6245\n",
      "Iteration 1000 - Loss: 10.1900, Accuracy: 0.6567\n",
      "Iteration 1500 - Loss: 6.5506, Accuracy: 0.6313\n",
      "Iteration 2000 - Loss: 9.0395, Accuracy: 0.5903\n",
      "Iteration 2500 - Loss: 9.5518, Accuracy: 0.5389\n",
      "Iteration 3000 - Loss: 7.2610, Accuracy: 0.6768\n",
      "Iteration 3500 - Loss: 10.2914, Accuracy: 0.5441\n",
      "Iteration 4000 - Loss: 9.0048, Accuracy: 0.5561\n",
      "Iteration 4500 - Loss: 8.2113, Accuracy: 0.6320\n",
      "Iteration 5000 - Loss: 7.7962, Accuracy: 0.6388\n",
      "Iteration 5500 - Loss: 8.2807, Accuracy: 0.6553\n",
      "Iteration 6000 - Loss: 7.8167, Accuracy: 0.5307\n",
      "Iteration 6500 - Loss: 8.2282, Accuracy: 0.6426\n",
      "Fold: 1, Train Loss: 8.0943, Train Accuracy: 0.6099, Val Loss: 7.5106, Val Accuracy: 0.6379\n",
      "Iteration 0 - Loss: 15.5646, Accuracy: 0.5089\n",
      "Iteration 500 - Loss: 9.5958, Accuracy: 0.6752\n",
      "Iteration 1000 - Loss: 9.4617, Accuracy: 0.5331\n",
      "Iteration 1500 - Loss: 7.2189, Accuracy: 0.5966\n",
      "Iteration 2000 - Loss: 7.5547, Accuracy: 0.6909\n",
      "Iteration 2500 - Loss: 9.6171, Accuracy: 0.6062\n",
      "Iteration 3000 - Loss: 8.3513, Accuracy: 0.6215\n",
      "Iteration 3500 - Loss: 8.2606, Accuracy: 0.5300\n",
      "Iteration 4000 - Loss: 7.6838, Accuracy: 0.6313\n",
      "Iteration 4500 - Loss: 7.1107, Accuracy: 0.5929\n",
      "Iteration 5000 - Loss: 9.2961, Accuracy: 0.5652\n",
      "Iteration 5500 - Loss: 10.1037, Accuracy: 0.6395\n",
      "Iteration 6000 - Loss: 7.2904, Accuracy: 0.6273\n",
      "Iteration 6500 - Loss: 7.6290, Accuracy: 0.6487\n",
      "Fold: 2, Train Loss: 8.0459, Train Accuracy: 0.6123, Val Loss: 7.5850, Val Accuracy: 0.6360\n",
      "Iteration 0 - Loss: 15.5322, Accuracy: 0.4981\n",
      "Iteration 500 - Loss: 9.6921, Accuracy: 0.6224\n",
      "Iteration 1000 - Loss: 7.7755, Accuracy: 0.5933\n",
      "Iteration 1500 - Loss: 9.9208, Accuracy: 0.6320\n",
      "Iteration 2000 - Loss: 7.6031, Accuracy: 0.6836\n",
      "Iteration 2500 - Loss: 6.7401, Accuracy: 0.6184\n",
      "Iteration 3000 - Loss: 6.5166, Accuracy: 0.6606\n",
      "Iteration 3500 - Loss: 7.4780, Accuracy: 0.5797\n",
      "Iteration 4000 - Loss: 7.2850, Accuracy: 0.6302\n",
      "Iteration 4500 - Loss: 8.3013, Accuracy: 0.5553\n",
      "Iteration 5000 - Loss: 7.9772, Accuracy: 0.5544\n",
      "Iteration 5500 - Loss: 8.5758, Accuracy: 0.6147\n",
      "Iteration 6000 - Loss: 9.8068, Accuracy: 0.5912\n",
      "Iteration 6500 - Loss: 7.4989, Accuracy: 0.5368\n",
      "Fold: 3, Train Loss: 8.1937, Train Accuracy: 0.6051, Val Loss: 8.5590, Val Accuracy: 0.5901\n",
      "Iteration 0 - Loss: 15.5340, Accuracy: 0.5023\n",
      "Iteration 500 - Loss: 7.0746, Accuracy: 0.5701\n",
      "Iteration 1000 - Loss: 7.5098, Accuracy: 0.5687\n",
      "Iteration 1500 - Loss: 8.5868, Accuracy: 0.5908\n",
      "Iteration 2000 - Loss: 9.8102, Accuracy: 0.6419\n",
      "Iteration 2500 - Loss: 8.4892, Accuracy: 0.6590\n",
      "Iteration 3000 - Loss: 7.8266, Accuracy: 0.6238\n",
      "Iteration 3500 - Loss: 6.4506, Accuracy: 0.5732\n",
      "Iteration 4000 - Loss: 8.8260, Accuracy: 0.6353\n",
      "Iteration 4500 - Loss: 6.8789, Accuracy: 0.6393\n",
      "Iteration 5000 - Loss: 8.9888, Accuracy: 0.5957\n",
      "Iteration 5500 - Loss: 8.9709, Accuracy: 0.5779\n",
      "Iteration 6000 - Loss: 6.8505, Accuracy: 0.5500\n",
      "Iteration 6500 - Loss: 9.4861, Accuracy: 0.5868\n",
      "Fold: 4, Train Loss: 8.1391, Train Accuracy: 0.6077, Val Loss: 7.5297, Val Accuracy: 0.6351\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.73       0.85       0.78      \n",
      "1          0.72       0.44       0.54      \n",
      "2          0.48       0.86       0.61      \n",
      "3          0.90       0.40       0.56      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6400 | F1-Score: 0.6300\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/ec292aab84b542f0abb02a4fd73ea369\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5118, Accuracy: 0.5007\n",
      "Iteration 500 - Loss: 8.1447, Accuracy: 0.6215\n",
      "Iteration 1000 - Loss: 7.1878, Accuracy: 0.6428\n",
      "Iteration 1500 - Loss: 7.1882, Accuracy: 0.6353\n",
      "Iteration 2000 - Loss: 9.3922, Accuracy: 0.5926\n",
      "Iteration 2500 - Loss: 8.1097, Accuracy: 0.6726\n",
      "Iteration 3000 - Loss: 9.3013, Accuracy: 0.6907\n",
      "Iteration 3500 - Loss: 6.8028, Accuracy: 0.6266\n",
      "Iteration 4000 - Loss: 7.0237, Accuracy: 0.6468\n",
      "Iteration 4500 - Loss: 9.1569, Accuracy: 0.6419\n",
      "Iteration 5000 - Loss: 9.7936, Accuracy: 0.6466\n",
      "Iteration 5500 - Loss: 8.4419, Accuracy: 0.6503\n",
      "Iteration 6000 - Loss: 8.4848, Accuracy: 0.5298\n",
      "Iteration 6500 - Loss: 7.5519, Accuracy: 0.5708\n",
      "Fold: 0, Train Loss: 8.1816, Train Accuracy: 0.6056, Val Loss: 9.6429, Val Accuracy: 0.5310\n",
      "Iteration 0 - Loss: 15.5117, Accuracy: 0.5101\n",
      "Iteration 500 - Loss: 8.7171, Accuracy: 0.6407\n",
      "Iteration 1000 - Loss: 7.9057, Accuracy: 0.6712\n",
      "Iteration 1500 - Loss: 6.9779, Accuracy: 0.6116\n",
      "Iteration 2000 - Loss: 9.3821, Accuracy: 0.6609\n",
      "Iteration 2500 - Loss: 7.7422, Accuracy: 0.5227\n",
      "Iteration 3000 - Loss: 7.4429, Accuracy: 0.6278\n",
      "Iteration 3500 - Loss: 7.4329, Accuracy: 0.5962\n",
      "Iteration 4000 - Loss: 7.3763, Accuracy: 0.5652\n",
      "Iteration 4500 - Loss: 9.3855, Accuracy: 0.6468\n",
      "Iteration 5000 - Loss: 8.4141, Accuracy: 0.6004\n",
      "Iteration 5500 - Loss: 7.4585, Accuracy: 0.5631\n",
      "Iteration 6000 - Loss: 8.8803, Accuracy: 0.6280\n",
      "Iteration 6500 - Loss: 7.5084, Accuracy: 0.6440\n",
      "Fold: 1, Train Loss: 8.0905, Train Accuracy: 0.6101, Val Loss: 8.6574, Val Accuracy: 0.5826\n",
      "Iteration 0 - Loss: 15.5859, Accuracy: 0.5103\n",
      "Iteration 500 - Loss: 7.0807, Accuracy: 0.6163\n",
      "Iteration 1000 - Loss: 8.5261, Accuracy: 0.6330\n",
      "Iteration 1500 - Loss: 8.4081, Accuracy: 0.6869\n",
      "Iteration 2000 - Loss: 8.8909, Accuracy: 0.6672\n",
      "Iteration 2500 - Loss: 7.6884, Accuracy: 0.6278\n",
      "Iteration 3000 - Loss: 7.0954, Accuracy: 0.6262\n",
      "Iteration 3500 - Loss: 9.6350, Accuracy: 0.6438\n",
      "Iteration 4000 - Loss: 7.0172, Accuracy: 0.6346\n",
      "Iteration 4500 - Loss: 9.6987, Accuracy: 0.6541\n",
      "Iteration 5000 - Loss: 7.3892, Accuracy: 0.6133\n",
      "Iteration 5500 - Loss: 8.6471, Accuracy: 0.6402\n",
      "Iteration 6000 - Loss: 7.6126, Accuracy: 0.6428\n",
      "Iteration 6500 - Loss: 7.0542, Accuracy: 0.5596\n",
      "Fold: 2, Train Loss: 8.0497, Train Accuracy: 0.6121, Val Loss: 7.7473, Val Accuracy: 0.6304\n",
      "Iteration 0 - Loss: 15.5187, Accuracy: 0.4932\n",
      "Iteration 500 - Loss: 8.0872, Accuracy: 0.5141\n",
      "Iteration 1000 - Loss: 8.0187, Accuracy: 0.5579\n",
      "Iteration 1500 - Loss: 7.8815, Accuracy: 0.5643\n",
      "Iteration 2000 - Loss: 8.1670, Accuracy: 0.5842\n",
      "Iteration 2500 - Loss: 8.9537, Accuracy: 0.5591\n",
      "Iteration 3000 - Loss: 8.8089, Accuracy: 0.6180\n",
      "Iteration 3500 - Loss: 7.4003, Accuracy: 0.6081\n",
      "Iteration 4000 - Loss: 8.5964, Accuracy: 0.6529\n",
      "Iteration 4500 - Loss: 8.3987, Accuracy: 0.6316\n",
      "Iteration 5000 - Loss: 8.4715, Accuracy: 0.5800\n",
      "Iteration 5500 - Loss: 8.2224, Accuracy: 0.6508\n",
      "Iteration 6000 - Loss: 10.2429, Accuracy: 0.5640\n",
      "Iteration 6500 - Loss: 8.1588, Accuracy: 0.5328\n",
      "Fold: 3, Train Loss: 8.1917, Train Accuracy: 0.6052, Val Loss: 6.8734, Val Accuracy: 0.6651\n",
      "Iteration 0 - Loss: 15.5264, Accuracy: 0.5091\n",
      "Iteration 500 - Loss: 7.6290, Accuracy: 0.5706\n",
      "Iteration 1000 - Loss: 6.8841, Accuracy: 0.5826\n",
      "Iteration 1500 - Loss: 8.2006, Accuracy: 0.6499\n",
      "Iteration 2000 - Loss: 7.9680, Accuracy: 0.5978\n",
      "Iteration 2500 - Loss: 6.9725, Accuracy: 0.5910\n",
      "Iteration 3000 - Loss: 7.9254, Accuracy: 0.6393\n",
      "Iteration 3500 - Loss: 6.2388, Accuracy: 0.6499\n",
      "Iteration 4000 - Loss: 8.4980, Accuracy: 0.6707\n",
      "Iteration 4500 - Loss: 7.1849, Accuracy: 0.6499\n",
      "Iteration 5000 - Loss: 7.0720, Accuracy: 0.6273\n",
      "Iteration 5500 - Loss: 6.7554, Accuracy: 0.5678\n",
      "Iteration 6000 - Loss: 9.5136, Accuracy: 0.5976\n",
      "Iteration 6500 - Loss: 6.8754, Accuracy: 0.6585\n",
      "Fold: 4, Train Loss: 8.1401, Train Accuracy: 0.6077, Val Loss: 8.2215, Val Accuracy: 0.6023\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.73       0.86       0.79      \n",
      "1          0.63       0.24       0.35      \n",
      "2          0.44       0.79       0.56      \n",
      "3          0.75       0.66       0.70      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.5900 | F1-Score: 0.5700\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/7a342f01fd3145909ca4db11d6518b77\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5405, Accuracy: 0.5016\n",
      "Iteration 500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Fold: 0, Train Loss: 8.1052, Train Accuracy: 0.6964, Val Loss: 7.9758, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 15.5429, Accuracy: 0.5122\n",
      "Iteration 500 - Loss: 7.9028, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9906, Train Accuracy: 0.7036, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.5064, Accuracy: 0.6459\n",
      "Iteration 1000 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 1500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 2000 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 2500 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 3000 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 3500 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 4000 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 4500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 5000 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 5500 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 6000 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 6500 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Fold: 2, Train Loss: 8.0413, Train Accuracy: 0.6581, Val Loss: 7.9481, Val Accuracy: 0.6829\n",
      "Iteration 0 - Loss: 15.5433, Accuracy: 0.4967\n",
      "Iteration 500 - Loss: 8.1734, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1729, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5052\n",
      "Iteration 500 - Loss: 7.9621, Accuracy: 0.7104\n",
      "Iteration 1000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 1500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Fold: 4, Train Loss: 8.0305, Train Accuracy: 0.7081, Val Loss: 8.0708, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.76       0.80      \n",
      "1          0.60       0.77       0.67      \n",
      "2          0.54       0.38       0.44      \n",
      "3          0.74       0.73       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/3101f8fde7744b75b70c1871afb0bc50\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5420, Accuracy: 0.5009\n",
      "Iteration 500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Fold: 0, Train Loss: 8.1056, Train Accuracy: 0.6961, Val Loss: 7.9758, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 15.5400, Accuracy: 0.5120\n",
      "Iteration 500 - Loss: 7.9028, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9910, Train Accuracy: 0.7034, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 15.5407, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 1000 - Loss: 8.5063, Accuracy: 0.6459\n",
      "Iteration 1500 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 2000 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 2500 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 3000 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 3500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 4000 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 4500 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 5000 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 5500 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 6000 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 6500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Fold: 2, Train Loss: 8.0411, Train Accuracy: 0.6582, Val Loss: 8.8413, Val Accuracy: 0.6079\n",
      "Iteration 0 - Loss: 15.5421, Accuracy: 0.4953\n",
      "Iteration 500 - Loss: 8.1734, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1729, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 15.5416, Accuracy: 0.5056\n",
      "Iteration 500 - Loss: 7.9621, Accuracy: 0.7104\n",
      "Iteration 1000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 1500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Fold: 4, Train Loss: 8.0305, Train Accuracy: 0.7081, Val Loss: 8.0708, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.76       0.80      \n",
      "1          0.60       0.77       0.67      \n",
      "2          0.54       0.38       0.44      \n",
      "3          0.74       0.73       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/fe4366d797e241ff91e3c031576d4b3b\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5407, Accuracy: 0.5012\n",
      "Iteration 500 - Loss: 8.6981, Accuracy: 0.6372\n",
      "Iteration 1000 - Loss: 7.8005, Accuracy: 0.6130\n",
      "Iteration 1500 - Loss: 8.0592, Accuracy: 0.6961\n",
      "Iteration 2000 - Loss: 8.6980, Accuracy: 0.6372\n",
      "Iteration 2500 - Loss: 7.8005, Accuracy: 0.6130\n",
      "Iteration 3000 - Loss: 8.0592, Accuracy: 0.6961\n",
      "Iteration 3500 - Loss: 8.6980, Accuracy: 0.6372\n",
      "Iteration 4000 - Loss: 7.8005, Accuracy: 0.6130\n",
      "Iteration 4500 - Loss: 8.0592, Accuracy: 0.6961\n",
      "Iteration 5000 - Loss: 8.6980, Accuracy: 0.6372\n",
      "Iteration 5500 - Loss: 7.8005, Accuracy: 0.6130\n",
      "Iteration 6000 - Loss: 8.0592, Accuracy: 0.6961\n",
      "Iteration 6500 - Loss: 8.6980, Accuracy: 0.6372\n",
      "Fold: 0, Train Loss: 8.1886, Train Accuracy: 0.6488, Val Loss: 7.6711, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.5419, Accuracy: 0.5120\n",
      "Iteration 500 - Loss: 7.9028, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9906, Train Accuracy: 0.7036, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 15.5407, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.3822, Accuracy: 0.6424\n",
      "Iteration 1000 - Loss: 7.7120, Accuracy: 0.6152\n",
      "Iteration 1500 - Loss: 7.9676, Accuracy: 0.7115\n",
      "Iteration 2000 - Loss: 8.5634, Accuracy: 0.6412\n",
      "Iteration 2500 - Loss: 7.6420, Accuracy: 0.6363\n",
      "Iteration 3000 - Loss: 7.9400, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 8.4268, Accuracy: 0.6461\n",
      "Iteration 4000 - Loss: 7.7437, Accuracy: 0.6367\n",
      "Iteration 4500 - Loss: 7.8998, Accuracy: 0.6947\n",
      "Iteration 5000 - Loss: 8.3824, Accuracy: 0.6428\n",
      "Iteration 5500 - Loss: 7.7120, Accuracy: 0.6152\n",
      "Iteration 6000 - Loss: 7.9676, Accuracy: 0.7115\n",
      "Iteration 6500 - Loss: 8.5634, Accuracy: 0.6412\n",
      "Fold: 2, Train Loss: 8.0338, Train Accuracy: 0.6587, Val Loss: 7.8591, Val Accuracy: 0.6942\n",
      "Iteration 0 - Loss: 15.5398, Accuracy: 0.4958\n",
      "Iteration 500 - Loss: 8.1734, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1729, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 15.5441, Accuracy: 0.5054\n",
      "Iteration 500 - Loss: 7.9621, Accuracy: 0.7104\n",
      "Iteration 1000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 1500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Fold: 4, Train Loss: 8.0305, Train Accuracy: 0.7081, Val Loss: 8.0708, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.76       0.80      \n",
      "1          0.60       0.77       0.67      \n",
      "2          0.54       0.38       0.44      \n",
      "3          0.74       0.73       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/29304057806c44409f4698c9fe780285\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5014\n",
      "Iteration 500 - Loss: 8.1550, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0923, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1230, Train Accuracy: 0.7136, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.5120\n",
      "Iteration 500 - Loss: 8.0422, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9791, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9716, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9704, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0083, Train Accuracy: 0.7216, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.5120\n",
      "Iteration 500 - Loss: 8.0194, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9536, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9846, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.4953\n",
      "Iteration 500 - Loss: 8.1535, Accuracy: 0.7169\n",
      "Iteration 1000 - Loss: 8.0896, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0819, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0805, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0799, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0796, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1192, Train Accuracy: 0.7163, Val Loss: 7.9512, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.5056\n",
      "Iteration 500 - Loss: 8.0914, Accuracy: 0.7153\n",
      "Iteration 1000 - Loss: 8.0267, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0189, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0177, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 4500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0566, Train Accuracy: 0.7140, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/1f9ac20a581d4bf7b026525e77f237d1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5014\n",
      "Iteration 500 - Loss: 8.1550, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0923, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1230, Train Accuracy: 0.7136, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5122\n",
      "Iteration 500 - Loss: 8.0422, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9791, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9716, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9704, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0083, Train Accuracy: 0.7216, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.0194, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9536, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9846, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.4955\n",
      "Iteration 500 - Loss: 8.1535, Accuracy: 0.7169\n",
      "Iteration 1000 - Loss: 8.0896, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0819, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0805, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0799, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0796, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1192, Train Accuracy: 0.7163, Val Loss: 7.9512, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.5056\n",
      "Iteration 500 - Loss: 8.0914, Accuracy: 0.7153\n",
      "Iteration 1000 - Loss: 8.0267, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0189, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0177, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 4500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0566, Train Accuracy: 0.7140, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/d1437f74c90e466383b195eb8d1cbcad\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5428, Accuracy: 0.5012\n",
      "Iteration 500 - Loss: 8.1550, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0923, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1230, Train Accuracy: 0.7136, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.0422, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9791, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9716, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9704, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0083, Train Accuracy: 0.7216, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5426, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.0194, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9536, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9846, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5426, Accuracy: 0.4953\n",
      "Iteration 500 - Loss: 8.1535, Accuracy: 0.7169\n",
      "Iteration 1000 - Loss: 8.0896, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0819, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0805, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0799, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0796, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1192, Train Accuracy: 0.7163, Val Loss: 7.9512, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5054\n",
      "Iteration 500 - Loss: 8.0914, Accuracy: 0.7153\n",
      "Iteration 1000 - Loss: 8.0267, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0189, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0177, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 4500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0566, Train Accuracy: 0.7140, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/5f3483c2db2e41bfb994b2608a9f3339\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.6000, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 6.0833, Accuracy: 0.8750\n",
      "Iteration 1000 - Loss: 9.9602, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 10.0697, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 9.3078, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 7.1958, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.3696, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 7.5318, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 7.1664, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 6.7986, Accuracy: 0.8438\n",
      "Iteration 5000 - Loss: 8.1780, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 8.1168, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 8.4316, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 9.2428, Accuracy: 0.6562\n",
      "Fold: 0, Train Loss: 8.1459, Train Accuracy: 0.7035, Val Loss: 7.8779, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 15.2652, Accuracy: 0.3125\n",
      "Iteration 500 - Loss: 8.8270, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 5.0506, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 7.4322, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 6.6870, Accuracy: 0.8125\n",
      "Iteration 2500 - Loss: 8.0853, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 9.5114, Accuracy: 0.5625\n",
      "Iteration 3500 - Loss: 8.9361, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 7.8235, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3310, Accuracy: 0.5938\n",
      "Iteration 5000 - Loss: 6.8331, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 7.5512, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 7.4603, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.0043, Accuracy: 0.6875\n",
      "Fold: 1, Train Loss: 8.0504, Train Accuracy: 0.7107, Val Loss: 8.1096, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 16.1741, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 9.3679, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 7.8483, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 7.7944, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.1568, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 7.5931, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.8468, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.5786, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 8.2344, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.0948, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 9.4822, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 7.2956, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 7.9574, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 8.1849, Accuracy: 0.7188\n",
      "Fold: 2, Train Loss: 8.0169, Train Accuracy: 0.7136, Val Loss: 8.2873, Val Accuracy: 0.6942\n",
      "Iteration 0 - Loss: 16.8534, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 8.7117, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 6.6978, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 7.1218, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 8.6186, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 10.0522, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 9.0413, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.3738, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 6.1363, Accuracy: 0.8750\n",
      "Iteration 4500 - Loss: 8.8610, Accuracy: 0.6250\n",
      "Iteration 5000 - Loss: 9.1371, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.5012, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 6.7357, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.0261, Accuracy: 0.7500\n",
      "Fold: 3, Train Loss: 8.1682, Train Accuracy: 0.7057, Val Loss: 7.7529, Val Accuracy: 0.6876\n",
      "Iteration 0 - Loss: 13.8114, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 7.5409, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.4497, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 6.7108, Accuracy: 0.7812\n",
      "Iteration 2000 - Loss: 8.5908, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.2274, Accuracy: 0.5625\n",
      "Iteration 3000 - Loss: 7.4210, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 5.4358, Accuracy: 0.8750\n",
      "Iteration 4000 - Loss: 6.9756, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.0004, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.7816, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 6.6715, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.4929, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 7.0651, Accuracy: 0.7500\n",
      "Fold: 4, Train Loss: 8.0827, Train Accuracy: 0.7087, Val Loss: 8.0171, Val Accuracy: 0.6829\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.86       0.72       0.79      \n",
      "1          0.59       0.79       0.67      \n",
      "2          0.54       0.38       0.45      \n",
      "3          0.74       0.70       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6600 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/24247c1332824905b3c68c7fa0ef6106\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 14.8920, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 8.2152, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 7.1862, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 8.0606, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.9623, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 8.5947, Accuracy: 0.5938\n",
      "Iteration 3000 - Loss: 7.8289, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.9042, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.2112, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 8.5643, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.7274, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 6.3692, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.3874, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 6.6549, Accuracy: 0.7812\n",
      "Fold: 0, Train Loss: 8.1131, Train Accuracy: 0.7031, Val Loss: 7.9876, Val Accuracy: 0.7148\n",
      "Iteration 0 - Loss: 17.3783, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 7.0905, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.2485, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 7.2930, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 8.1906, Accuracy: 0.7656\n",
      "Iteration 2500 - Loss: 8.4824, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.8539, Accuracy: 0.6406\n",
      "Iteration 3500 - Loss: 8.5475, Accuracy: 0.6094\n",
      "Iteration 4000 - Loss: 9.1676, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.5496, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 6.8893, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 8.1675, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.8710, Accuracy: 0.7969\n",
      "Iteration 6500 - Loss: 7.9694, Accuracy: 0.6562\n",
      "Fold: 1, Train Loss: 8.0104, Train Accuracy: 0.7101, Val Loss: 8.1578, Val Accuracy: 0.7073\n",
      "Iteration 0 - Loss: 14.5629, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 7.9273, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.1697, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.8860, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 7.4765, Accuracy: 0.7344\n",
      "Iteration 2500 - Loss: 7.1846, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.0751, Accuracy: 0.8281\n",
      "Iteration 3500 - Loss: 7.0447, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.1493, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 6.7738, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 7.2777, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 7.6655, Accuracy: 0.8281\n",
      "Iteration 6000 - Loss: 8.7892, Accuracy: 0.6094\n",
      "Iteration 6500 - Loss: 7.8632, Accuracy: 0.6719\n",
      "Fold: 2, Train Loss: 7.9592, Train Accuracy: 0.7126, Val Loss: 8.3791, Val Accuracy: 0.6576\n",
      "Iteration 0 - Loss: 16.5516, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 8.0695, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.8893, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 7.7612, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 6.5365, Accuracy: 0.8125\n",
      "Iteration 2500 - Loss: 6.8059, Accuracy: 0.7857\n",
      "Iteration 3000 - Loss: 8.0868, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.7368, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.0021, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 9.1124, Accuracy: 0.6250\n",
      "Iteration 5000 - Loss: 7.3478, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 8.8776, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.0314, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.4850, Accuracy: 0.7969\n",
      "Fold: 3, Train Loss: 8.0842, Train Accuracy: 0.7084, Val Loss: 7.9175, Val Accuracy: 0.6848\n",
      "Iteration 0 - Loss: 16.9269, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 8.2340, Accuracy: 0.6406\n",
      "Iteration 1000 - Loss: 8.5890, Accuracy: 0.6094\n",
      "Iteration 1500 - Loss: 6.8634, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 7.7858, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.3473, Accuracy: 0.6719\n",
      "Iteration 3000 - Loss: 8.3042, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.1064, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 9.1633, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.1913, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 7.0764, Accuracy: 0.8281\n",
      "Iteration 5500 - Loss: 7.9321, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.6878, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.1057, Accuracy: 0.6719\n",
      "Fold: 4, Train Loss: 8.0688, Train Accuracy: 0.7078, Val Loss: 8.4215, Val Accuracy: 0.6604\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.70       0.88       0.78      \n",
      "1          0.70       0.48       0.57      \n",
      "2          0.50       0.75       0.60      \n",
      "3          0.87       0.46       0.60      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6400 | F1-Score: 0.6400\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/ec40e07d532943adbd01dd4425808516\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 13.4581, Accuracy: 0.5078\n",
      "Iteration 500 - Loss: 8.6881, Accuracy: 0.7109\n",
      "Iteration 1000 - Loss: 8.4124, Accuracy: 0.6172\n",
      "Iteration 1500 - Loss: 8.2720, Accuracy: 0.7578\n",
      "Iteration 2000 - Loss: 7.6792, Accuracy: 0.7734\n",
      "Iteration 2500 - Loss: 7.4314, Accuracy: 0.7656\n",
      "Iteration 3000 - Loss: 6.5873, Accuracy: 0.8047\n",
      "Iteration 3500 - Loss: 7.8849, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 8.6310, Accuracy: 0.6406\n",
      "Iteration 4500 - Loss: 8.4796, Accuracy: 0.6406\n",
      "Iteration 5000 - Loss: 8.9451, Accuracy: 0.6484\n",
      "Iteration 5500 - Loss: 7.7514, Accuracy: 0.7422\n",
      "Iteration 6000 - Loss: 8.0156, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.3944, Accuracy: 0.6641\n",
      "Fold: 0, Train Loss: 8.1074, Train Accuracy: 0.7001, Val Loss: 7.9016, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.1025, Accuracy: 0.4609\n",
      "Iteration 500 - Loss: 7.1484, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 7.7844, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 8.0455, Accuracy: 0.7818\n",
      "Iteration 2000 - Loss: 8.2810, Accuracy: 0.6797\n",
      "Iteration 2500 - Loss: 8.1812, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 8.4307, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.8522, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.2511, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 7.0989, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 7.4721, Accuracy: 0.7109\n",
      "Iteration 5500 - Loss: 8.3787, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.7430, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.4252, Accuracy: 0.7578\n",
      "Fold: 1, Train Loss: 7.9971, Train Accuracy: 0.7068, Val Loss: 8.2494, Val Accuracy: 0.6754\n",
      "Iteration 0 - Loss: 15.4581, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 7.5794, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.5090, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 7.4324, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.5353, Accuracy: 0.6641\n",
      "Iteration 2500 - Loss: 7.6599, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 7.7451, Accuracy: 0.7422\n",
      "Iteration 3500 - Loss: 7.8242, Accuracy: 0.7578\n",
      "Iteration 4000 - Loss: 7.5611, Accuracy: 0.7266\n",
      "Iteration 4500 - Loss: 8.6399, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 8.2745, Accuracy: 0.7473\n",
      "Iteration 5500 - Loss: 7.8149, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 7.7507, Accuracy: 0.7266\n",
      "Iteration 6500 - Loss: 7.8283, Accuracy: 0.7031\n",
      "Fold: 2, Train Loss: 7.9661, Train Accuracy: 0.7099, Val Loss: 8.0274, Val Accuracy: 0.6829\n",
      "Iteration 0 - Loss: 16.5271, Accuracy: 0.3672\n",
      "Iteration 500 - Loss: 8.4816, Accuracy: 0.6765\n",
      "Iteration 1000 - Loss: 7.7891, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 8.2683, Accuracy: 0.7109\n",
      "Iteration 2000 - Loss: 7.9783, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 7.9265, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 7.1443, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 7.9280, Accuracy: 0.6641\n",
      "Iteration 4000 - Loss: 7.1627, Accuracy: 0.7109\n",
      "Iteration 4500 - Loss: 8.2149, Accuracy: 0.6953\n",
      "Iteration 5000 - Loss: 8.2479, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.4842, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 8.9081, Accuracy: 0.7109\n",
      "Iteration 6500 - Loss: 8.4505, Accuracy: 0.6719\n",
      "Fold: 3, Train Loss: 8.0868, Train Accuracy: 0.7047, Val Loss: 7.8256, Val Accuracy: 0.7092\n",
      "Iteration 0 - Loss: 15.3574, Accuracy: 0.4545\n",
      "Iteration 500 - Loss: 8.0813, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.5843, Accuracy: 0.6641\n",
      "Iteration 1500 - Loss: 8.5808, Accuracy: 0.7109\n",
      "Iteration 2000 - Loss: 7.5551, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 7.9137, Accuracy: 0.7578\n",
      "Iteration 3000 - Loss: 8.0588, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 8.0883, Accuracy: 0.6719\n",
      "Iteration 4000 - Loss: 7.6430, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 7.8822, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 7.7226, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 7.9348, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 7.8903, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 8.6776, Accuracy: 0.6328\n",
      "Fold: 4, Train Loss: 8.0480, Train Accuracy: 0.7059, Val Loss: 7.8741, Val Accuracy: 0.6914\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.86       0.76       0.81      \n",
      "1          0.60       0.82       0.69      \n",
      "2          0.52       0.43       0.47      \n",
      "3          0.84       0.54       0.66      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/02a66f67534241b2a4389734f48fff85\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 16.0322, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 9.2486, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 10.9722, Accuracy: 0.5312\n",
      "Iteration 1500 - Loss: 8.6501, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.5841, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.9799, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.5128, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 7.9301, Accuracy: 0.8750\n",
      "Iteration 4000 - Loss: 6.3687, Accuracy: 0.8438\n",
      "Iteration 4500 - Loss: 9.4007, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 6.8880, Accuracy: 0.8438\n",
      "Iteration 5500 - Loss: 8.8893, Accuracy: 0.5938\n",
      "Iteration 6000 - Loss: 8.5946, Accuracy: 0.5938\n",
      "Iteration 6500 - Loss: 8.4620, Accuracy: 0.6562\n",
      "Fold: 0, Train Loss: 8.5781, Train Accuracy: 0.7082, Val Loss: 7.9379, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 15.0109, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 8.5356, Accuracy: 0.8438\n",
      "Iteration 1000 - Loss: 9.3333, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 9.7912, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 7.0013, Accuracy: 0.8438\n",
      "Iteration 2500 - Loss: 8.1908, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.2324, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.3474, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 8.1992, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 9.2193, Accuracy: 0.5000\n",
      "Iteration 5000 - Loss: 8.4066, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 7.4369, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.2684, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 7.8696, Accuracy: 0.7500\n",
      "Fold: 1, Train Loss: 8.4669, Train Accuracy: 0.7138, Val Loss: 8.3207, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 14.7251, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 9.2208, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 9.5933, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 9.3963, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 8.4236, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.1101, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.9588, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 7.0383, Accuracy: 0.8125\n",
      "Iteration 4000 - Loss: 7.4306, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 7.7747, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.1420, Accuracy: 0.6250\n",
      "Iteration 5500 - Loss: 8.8407, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 7.3334, Accuracy: 0.8125\n",
      "Iteration 6500 - Loss: 6.3324, Accuracy: 0.8438\n",
      "Fold: 2, Train Loss: 8.4686, Train Accuracy: 0.7144, Val Loss: 8.2927, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 14.9630, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 8.9075, Accuracy: 0.7812\n",
      "Iteration 1000 - Loss: 8.9851, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 9.2528, Accuracy: 0.8438\n",
      "Iteration 2000 - Loss: 8.7076, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.0701, Accuracy: 0.8750\n",
      "Iteration 3000 - Loss: 6.6169, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 9.6993, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 8.2880, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 9.4728, Accuracy: 0.5312\n",
      "Iteration 5000 - Loss: 6.6505, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 7.4232, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.4616, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.1498, Accuracy: 0.6875\n",
      "Fold: 3, Train Loss: 8.6247, Train Accuracy: 0.7084, Val Loss: 7.9929, Val Accuracy: 0.7111\n",
      "Iteration 0 - Loss: 16.8037, Accuracy: 0.1250\n",
      "Iteration 500 - Loss: 10.6698, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 7.7061, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 9.5496, Accuracy: 0.5312\n",
      "Iteration 2000 - Loss: 8.9670, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.9848, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.0553, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 7.9920, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 6.8221, Accuracy: 0.8438\n",
      "Iteration 4500 - Loss: 7.0713, Accuracy: 0.8125\n",
      "Iteration 5000 - Loss: 6.9868, Accuracy: 0.8438\n",
      "Iteration 5500 - Loss: 9.4522, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 6.3601, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 8.3027, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 8.5468, Train Accuracy: 0.7090, Val Loss: 8.1396, Val Accuracy: 0.7176\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.75       0.80      \n",
      "1          0.63       0.72       0.68      \n",
      "2          0.55       0.57       0.56      \n",
      "3          0.82       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/2e745c2c712c49008454f482018401e1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.8248, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 9.5490, Accuracy: 0.5938\n",
      "Iteration 1000 - Loss: 7.4933, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 8.6337, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 9.2516, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.8998, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.8366, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 9.8811, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 7.1287, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 8.1796, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.1154, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 7.6514, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.3492, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.9028, Accuracy: 0.6562\n",
      "Fold: 0, Train Loss: 8.3667, Train Accuracy: 0.7101, Val Loss: 7.9014, Val Accuracy: 0.7195\n",
      "Iteration 0 - Loss: 16.4011, Accuracy: 0.1094\n",
      "Iteration 500 - Loss: 8.4555, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.7908, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 8.3359, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 7.4130, Accuracy: 0.7344\n",
      "Iteration 2500 - Loss: 7.9251, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.3083, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 7.4192, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 9.0522, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 8.0283, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.7993, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 7.0469, Accuracy: 0.8438\n",
      "Iteration 6000 - Loss: 7.4445, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 7.0621, Accuracy: 0.7812\n",
      "Fold: 1, Train Loss: 8.2460, Train Accuracy: 0.7166, Val Loss: 8.2125, Val Accuracy: 0.6914\n",
      "Iteration 0 - Loss: 14.4335, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 8.4313, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.7600, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 7.3978, Accuracy: 0.7812\n",
      "Iteration 2000 - Loss: 8.2296, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 7.7901, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 8.4358, Accuracy: 0.6719\n",
      "Iteration 3500 - Loss: 8.0340, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 6.7481, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 9.0562, Accuracy: 0.6406\n",
      "Iteration 5000 - Loss: 7.4907, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 7.8461, Accuracy: 0.7969\n",
      "Iteration 6000 - Loss: 8.4123, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 7.8756, Accuracy: 0.7812\n",
      "Fold: 2, Train Loss: 8.2339, Train Accuracy: 0.7170, Val Loss: 8.2667, Val Accuracy: 0.6998\n",
      "Iteration 0 - Loss: 15.8599, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 9.1843, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.7346, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 8.6373, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.7102, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.6295, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.6874, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 7.1620, Accuracy: 0.8281\n",
      "Iteration 4000 - Loss: 7.8656, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 7.9667, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 7.1508, Accuracy: 0.8125\n",
      "Iteration 5500 - Loss: 8.5609, Accuracy: 0.6406\n",
      "Iteration 6000 - Loss: 7.4613, Accuracy: 0.7969\n",
      "Iteration 6500 - Loss: 7.7689, Accuracy: 0.7500\n",
      "Fold: 3, Train Loss: 8.3624, Train Accuracy: 0.7131, Val Loss: 7.9439, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 15.4037, Accuracy: 0.2969\n",
      "Iteration 500 - Loss: 9.2721, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 7.3520, Accuracy: 0.8906\n",
      "Iteration 1500 - Loss: 8.4415, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 8.8189, Accuracy: 0.6406\n",
      "Iteration 2500 - Loss: 8.3701, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.8733, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.5047, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.2774, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 7.3794, Accuracy: 0.7656\n",
      "Iteration 5000 - Loss: 8.3336, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 6.8092, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.5407, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 8.1345, Accuracy: 0.6250\n",
      "Fold: 4, Train Loss: 8.3089, Train Accuracy: 0.7113, Val Loss: 8.0525, Val Accuracy: 0.7120\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.82       0.82      \n",
      "1          0.64       0.70       0.67      \n",
      "2          0.54       0.54       0.54      \n",
      "3          0.81       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/d67ef0dc22d1448b8176b76590102957\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.7822, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 8.4825, Accuracy: 0.6797\n",
      "Iteration 1000 - Loss: 9.0063, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 8.8510, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.6771, Accuracy: 0.6641\n",
      "Iteration 2500 - Loss: 8.2602, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 8.1297, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.1791, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 8.2258, Accuracy: 0.6953\n",
      "Iteration 4500 - Loss: 7.9635, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.1914, Accuracy: 0.7429\n",
      "Iteration 5500 - Loss: 8.3352, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 8.2673, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 8.5399, Accuracy: 0.7266\n",
      "Fold: 0, Train Loss: 8.2226, Train Accuracy: 0.7128, Val Loss: 7.9901, Val Accuracy: 0.7251\n",
      "Iteration 0 - Loss: 15.5390, Accuracy: 0.2344\n",
      "Iteration 500 - Loss: 8.9608, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 8.9670, Accuracy: 0.6953\n",
      "Iteration 1500 - Loss: 7.7744, Accuracy: 0.7891\n",
      "Iteration 2000 - Loss: 8.1373, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 7.6065, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.7329, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.5611, Accuracy: 0.6953\n",
      "Iteration 4000 - Loss: 8.5314, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 8.5870, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 7.4877, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 7.2591, Accuracy: 0.7578\n",
      "Iteration 6000 - Loss: 8.4503, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 7.3539, Accuracy: 0.7422\n",
      "Fold: 1, Train Loss: 8.1091, Train Accuracy: 0.7196, Val Loss: 8.2517, Val Accuracy: 0.7167\n",
      "Iteration 0 - Loss: 14.2947, Accuracy: 0.4062\n",
      "Iteration 500 - Loss: 8.6848, Accuracy: 0.7109\n",
      "Iteration 1000 - Loss: 7.5431, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 8.1196, Accuracy: 0.7422\n",
      "Iteration 2000 - Loss: 7.9052, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 7.8393, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.0567, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.0912, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 7.2412, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.5580, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 8.2982, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 7.4997, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.1837, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.1730, Accuracy: 0.6923\n",
      "Fold: 2, Train Loss: 8.0743, Train Accuracy: 0.7200, Val Loss: 8.3345, Val Accuracy: 0.6942\n",
      "Iteration 0 - Loss: 15.9561, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 8.8998, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 8.0409, Accuracy: 0.7422\n",
      "Iteration 1500 - Loss: 8.8137, Accuracy: 0.6953\n",
      "Iteration 2000 - Loss: 8.6358, Accuracy: 0.6719\n",
      "Iteration 2500 - Loss: 7.8952, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 8.5135, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 8.4084, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.8417, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 8.3732, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 8.8349, Accuracy: 0.6328\n",
      "Iteration 5500 - Loss: 8.4574, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.1012, Accuracy: 0.7266\n",
      "Iteration 6500 - Loss: 8.5602, Accuracy: 0.6953\n",
      "Fold: 3, Train Loss: 8.2149, Train Accuracy: 0.7154, Val Loss: 7.8769, Val Accuracy: 0.7186\n",
      "Iteration 0 - Loss: 16.2650, Accuracy: 0.2656\n",
      "Iteration 500 - Loss: 8.8814, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.2843, Accuracy: 0.7578\n",
      "Iteration 1500 - Loss: 8.6078, Accuracy: 0.6406\n",
      "Iteration 2000 - Loss: 8.4534, Accuracy: 0.6953\n",
      "Iteration 2500 - Loss: 7.5090, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 7.5254, Accuracy: 0.7422\n",
      "Iteration 3500 - Loss: 7.7502, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 7.8603, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 7.2026, Accuracy: 0.8047\n",
      "Iteration 5000 - Loss: 8.4120, Accuracy: 0.7266\n",
      "Iteration 5500 - Loss: 7.3826, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 8.4055, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 7.3743, Accuracy: 0.7656\n",
      "Fold: 4, Train Loss: 8.1561, Train Accuracy: 0.7141, Val Loss: 8.0847, Val Accuracy: 0.7158\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.80       0.81      \n",
      "1          0.66       0.68       0.67      \n",
      "2          0.55       0.60       0.57      \n",
      "3          0.82       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/3bb50accfc97414d9673d88108911a9d\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 16.2907, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 12.4455, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 11.8484, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 10.6781, Accuracy: 0.8125\n",
      "Iteration 2000 - Loss: 10.0166, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.6996, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 10.3356, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 9.2653, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 8.6885, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.5803, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 9.0380, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 9.8530, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.3263, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.6549, Accuracy: 0.8125\n",
      "Fold: 0, Train Loss: 10.2970, Train Accuracy: 0.6631, Val Loss: 9.1293, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 15.4537, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 11.4214, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 10.5695, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 10.7510, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 9.3591, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 11.2006, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.2388, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 9.2147, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 10.2454, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.5349, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 11.0300, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 9.2117, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.3183, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 10.4042, Accuracy: 0.5312\n",
      "Fold: 1, Train Loss: 10.1149, Train Accuracy: 0.6740, Val Loss: 9.5436, Val Accuracy: 0.6754\n",
      "Iteration 0 - Loss: 14.5090, Accuracy: 0.3438\n",
      "Iteration 500 - Loss: 11.5193, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 10.8496, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 10.3230, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 10.4623, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 9.1454, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 9.8455, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 10.4364, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 11.3621, Accuracy: 0.5312\n",
      "Iteration 4500 - Loss: 9.6037, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.7701, Accuracy: 0.8125\n",
      "Iteration 5500 - Loss: 9.4664, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.5280, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 9.0313, Accuracy: 0.7500\n",
      "Fold: 2, Train Loss: 10.1077, Train Accuracy: 0.6719, Val Loss: 9.4474, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 16.2806, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 12.5389, Accuracy: 0.4688\n",
      "Iteration 1000 - Loss: 11.5952, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 10.3631, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 10.2080, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 11.5540, Accuracy: 0.5625\n",
      "Iteration 3000 - Loss: 10.2921, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 9.7252, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 9.3447, Accuracy: 0.7812\n",
      "Iteration 4500 - Loss: 10.0381, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.3078, Accuracy: 0.8438\n",
      "Iteration 5500 - Loss: 10.3030, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 9.8832, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.5775, Accuracy: 0.8000\n",
      "Fold: 3, Train Loss: 10.4472, Train Accuracy: 0.6577, Val Loss: 9.1228, Val Accuracy: 0.7111\n",
      "Iteration 0 - Loss: 15.2570, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 12.4434, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 11.2494, Accuracy: 0.4688\n",
      "Iteration 1500 - Loss: 11.2166, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 10.5079, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 9.1957, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 9.0529, Accuracy: 0.9062\n",
      "Iteration 3500 - Loss: 8.4829, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 8.4810, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 8.0332, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.2708, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 10.2239, Accuracy: 0.5625\n",
      "Iteration 6000 - Loss: 10.0075, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 9.1611, Accuracy: 0.7188\n",
      "Fold: 4, Train Loss: 10.1895, Train Accuracy: 0.6745, Val Loss: 9.2645, Val Accuracy: 0.7008\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.63       0.72       0.67      \n",
      "2          0.50       0.49       0.49      \n",
      "3          0.75       0.57       0.65      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6600 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/4be62dc29ce849619b980c2a90a21aa7\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 14.4026, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.5745, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 10.9319, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 9.7272, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 9.6419, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 9.3569, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 9.5113, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 10.2415, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.8548, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 9.0051, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.7946, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 9.5713, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.1467, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.2839, Accuracy: 0.7812\n",
      "Fold: 0, Train Loss: 9.6435, Train Accuracy: 0.6842, Val Loss: 8.6508, Val Accuracy: 0.7336\n",
      "Iteration 0 - Loss: 15.1421, Accuracy: 0.3594\n",
      "Iteration 500 - Loss: 10.5122, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 10.4386, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 10.3022, Accuracy: 0.6406\n",
      "Iteration 2000 - Loss: 9.7308, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.7303, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.5930, Accuracy: 0.6406\n",
      "Iteration 3500 - Loss: 9.3081, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 9.8588, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 7.9694, Accuracy: 0.8594\n",
      "Iteration 5000 - Loss: 7.7956, Accuracy: 0.7969\n",
      "Iteration 5500 - Loss: 8.7220, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 9.2041, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 8.1202, Accuracy: 0.8281\n",
      "Fold: 1, Train Loss: 9.5429, Train Accuracy: 0.6922, Val Loss: 9.0396, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 16.4294, Accuracy: 0.1719\n",
      "Iteration 500 - Loss: 10.2223, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 10.8715, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 9.7816, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 9.9587, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 9.5922, Accuracy: 0.6786\n",
      "Iteration 3000 - Loss: 9.4552, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 10.0257, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 9.1198, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 10.2151, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.2938, Accuracy: 0.6094\n",
      "Iteration 5500 - Loss: 8.0290, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.3833, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 9.0874, Accuracy: 0.7031\n",
      "Fold: 2, Train Loss: 9.5969, Train Accuracy: 0.6856, Val Loss: 9.0016, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 16.2894, Accuracy: 0.1562\n",
      "Iteration 500 - Loss: 11.8229, Accuracy: 0.6406\n",
      "Iteration 1000 - Loss: 10.3180, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.1686, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 9.6710, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 9.6717, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.8479, Accuracy: 0.7656\n",
      "Iteration 3500 - Loss: 8.4151, Accuracy: 0.7969\n",
      "Iteration 4000 - Loss: 9.8944, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.7282, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.9888, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 8.9430, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 9.1241, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 9.3949, Accuracy: 0.7031\n",
      "Fold: 3, Train Loss: 9.7028, Train Accuracy: 0.6857, Val Loss: 8.6456, Val Accuracy: 0.7158\n",
      "Iteration 0 - Loss: 16.0311, Accuracy: 0.2344\n",
      "Iteration 500 - Loss: 10.6452, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 10.5090, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.3124, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.1077, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 9.9522, Accuracy: 0.6250\n",
      "Iteration 3000 - Loss: 8.6000, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 9.2593, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 9.1447, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.1084, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.8029, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 9.4682, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 8.5517, Accuracy: 0.7969\n",
      "Iteration 6500 - Loss: 8.9994, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 9.5654, Train Accuracy: 0.6918, Val Loss: 8.7824, Val Accuracy: 0.7045\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.63       0.73       0.68      \n",
      "2          0.53       0.53       0.53      \n",
      "3          0.79       0.60       0.68      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/5adabb22fc4e46f8bea268d9e3f65a5e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 16.6674, Accuracy: 0.1094\n",
      "Iteration 500 - Loss: 10.1788, Accuracy: 0.6797\n",
      "Iteration 1000 - Loss: 9.3205, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 9.2692, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 9.6477, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 9.5994, Accuracy: 0.6172\n",
      "Iteration 3000 - Loss: 8.8594, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.9145, Accuracy: 0.7266\n",
      "Iteration 4000 - Loss: 7.8767, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.2307, Accuracy: 0.7578\n",
      "Iteration 5000 - Loss: 8.4913, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 9.2429, Accuracy: 0.6406\n",
      "Iteration 6000 - Loss: 8.7720, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 8.5862, Accuracy: 0.6797\n",
      "Fold: 0, Train Loss: 9.1258, Train Accuracy: 0.6988, Val Loss: 8.2692, Val Accuracy: 0.7355\n",
      "Iteration 0 - Loss: 15.3018, Accuracy: 0.2969\n",
      "Iteration 500 - Loss: 10.1944, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 9.3323, Accuracy: 0.7344\n",
      "Iteration 1500 - Loss: 8.9208, Accuracy: 0.7266\n",
      "Iteration 2000 - Loss: 9.0305, Accuracy: 0.6953\n",
      "Iteration 2500 - Loss: 8.6505, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.2528, Accuracy: 0.7422\n",
      "Iteration 3500 - Loss: 7.5773, Accuracy: 0.8047\n",
      "Iteration 4000 - Loss: 8.2466, Accuracy: 0.7734\n",
      "Iteration 4500 - Loss: 9.4801, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 9.0126, Accuracy: 0.7266\n",
      "Iteration 5500 - Loss: 7.5134, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.5907, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.6898, Accuracy: 0.8281\n",
      "Fold: 1, Train Loss: 8.9830, Train Accuracy: 0.7080, Val Loss: 8.6101, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.9365, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 11.0723, Accuracy: 0.5703\n",
      "Iteration 1000 - Loss: 10.2602, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 9.5373, Accuracy: 0.6933\n",
      "Iteration 2000 - Loss: 9.7917, Accuracy: 0.6328\n",
      "Iteration 2500 - Loss: 9.1370, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.8589, Accuracy: 0.7414\n",
      "Iteration 3500 - Loss: 8.4999, Accuracy: 0.6953\n",
      "Iteration 4000 - Loss: 8.2442, Accuracy: 0.7734\n",
      "Iteration 4500 - Loss: 8.1004, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 8.7128, Accuracy: 0.7109\n",
      "Iteration 5500 - Loss: 7.7045, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 7.9175, Accuracy: 0.7891\n",
      "Iteration 6500 - Loss: 7.8697, Accuracy: 0.7500\n",
      "Fold: 2, Train Loss: 9.0320, Train Accuracy: 0.7039, Val Loss: 8.5968, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 15.7576, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 10.2820, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 10.1482, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 9.9344, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.8644, Accuracy: 0.6797\n",
      "Iteration 2500 - Loss: 8.8164, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.0624, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 9.7319, Accuracy: 0.5781\n",
      "Iteration 4000 - Loss: 8.7831, Accuracy: 0.7155\n",
      "Iteration 4500 - Loss: 8.8804, Accuracy: 0.6797\n",
      "Iteration 5000 - Loss: 8.5502, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 9.1689, Accuracy: 0.6406\n",
      "Iteration 6000 - Loss: 8.0101, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 7.7093, Accuracy: 0.7969\n",
      "Fold: 3, Train Loss: 9.1343, Train Accuracy: 0.7004, Val Loss: 8.2253, Val Accuracy: 0.7223\n",
      "Iteration 0 - Loss: 15.2127, Accuracy: 0.3359\n",
      "Iteration 500 - Loss: 10.2217, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 9.3980, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 9.0814, Accuracy: 0.7266\n",
      "Iteration 2000 - Loss: 8.8250, Accuracy: 0.7266\n",
      "Iteration 2500 - Loss: 8.6360, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 8.9009, Accuracy: 0.7377\n",
      "Iteration 3500 - Loss: 9.2335, Accuracy: 0.6719\n",
      "Iteration 4000 - Loss: 9.1337, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 7.8525, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.4670, Accuracy: 0.7266\n",
      "Iteration 5500 - Loss: 7.9809, Accuracy: 0.7578\n",
      "Iteration 6000 - Loss: 8.7073, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.2168, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 9.0448, Train Accuracy: 0.7042, Val Loss: 8.3984, Val Accuracy: 0.7073\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.63       0.73       0.68      \n",
      "2          0.55       0.52       0.53      \n",
      "3          0.81       0.64       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/3f4cb1ad856347c288eeba3760d366da\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 12.3062, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 7.3287, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 6.9323, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 8.8157, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 8.8492, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 8.5257, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.4475, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.6399, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.0612, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.1633, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.7033, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 7.7870, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 7.7467, Accuracy: 0.8125\n",
      "Iteration 6500 - Loss: 8.5782, Accuracy: 0.5938\n",
      "Fold: 0, Train Loss: 8.1508, Train Accuracy: 0.7039, Val Loss: 7.9389, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.1478, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 8.2813, Accuracy: 0.5625\n",
      "Iteration 1000 - Loss: 7.4642, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 9.4926, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 7.7833, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.0578, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 5.5823, Accuracy: 0.8750\n",
      "Iteration 3500 - Loss: 9.4350, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 6.9438, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 7.5434, Accuracy: 0.8125\n",
      "Iteration 5000 - Loss: 9.0021, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 6.3188, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 6.9089, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 7.7353, Accuracy: 0.6875\n",
      "Fold: 1, Train Loss: 8.0449, Train Accuracy: 0.7100, Val Loss: 8.3646, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 17.6013, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 8.2447, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.1802, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 8.1398, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.3370, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 9.2796, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 7.6736, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 9.4324, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 7.9737, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 9.5010, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.6172, Accuracy: 0.6250\n",
      "Iteration 5500 - Loss: 7.7400, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 7.5529, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 6.8425, Accuracy: 0.7812\n",
      "Fold: 2, Train Loss: 8.0134, Train Accuracy: 0.7141, Val Loss: 8.2839, Val Accuracy: 0.6735\n",
      "Iteration 0 - Loss: 14.8307, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 7.6491, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.5129, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 8.2746, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 7.7784, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 7.2896, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 7.5982, Accuracy: 0.8438\n",
      "Iteration 3500 - Loss: 7.8410, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 6.1961, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 7.7546, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.4512, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 5.6953, Accuracy: 0.8438\n",
      "Iteration 6000 - Loss: 8.2960, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 7.6969, Accuracy: 0.7188\n",
      "Fold: 3, Train Loss: 8.1115, Train Accuracy: 0.7096, Val Loss: 7.7903, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.2186, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 7.0702, Accuracy: 0.8125\n",
      "Iteration 1000 - Loss: 7.1221, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 10.0473, Accuracy: 0.5625\n",
      "Iteration 2000 - Loss: 7.7301, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 7.6575, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.1560, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.3668, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 9.4041, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 9.5779, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 10.1222, Accuracy: 0.5312\n",
      "Iteration 5500 - Loss: 7.0505, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.8154, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 10.3717, Accuracy: 0.5000\n",
      "Fold: 4, Train Loss: 8.1248, Train Accuracy: 0.7064, Val Loss: 8.0563, Val Accuracy: 0.6801\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.87       0.70       0.78      \n",
      "1          0.63       0.74       0.68      \n",
      "2          0.53       0.59       0.56      \n",
      "3          0.81       0.59       0.68      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/783f9f3f88044effb087e661f1274a48\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 16.5932, Accuracy: 0.3438\n",
      "Iteration 500 - Loss: 7.8613, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.3622, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 8.9835, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 7.6229, Accuracy: 0.7969\n",
      "Iteration 2500 - Loss: 7.4526, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.3748, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 7.8507, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.7154, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3406, Accuracy: 0.6406\n",
      "Iteration 5000 - Loss: 8.4205, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 6.9144, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 7.2422, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 6.8583, Accuracy: 0.7500\n",
      "Fold: 0, Train Loss: 8.1319, Train Accuracy: 0.7034, Val Loss: 7.7599, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 13.4292, Accuracy: 0.4219\n",
      "Iteration 500 - Loss: 7.0174, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.0376, Accuracy: 0.7344\n",
      "Iteration 1500 - Loss: 8.6852, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.4137, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 8.9138, Accuracy: 0.6406\n",
      "Iteration 3000 - Loss: 6.8798, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 7.8513, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 8.5125, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 8.5442, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.9610, Accuracy: 0.5625\n",
      "Iteration 5500 - Loss: 7.5466, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 7.5972, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 8.5233, Accuracy: 0.6719\n",
      "Fold: 1, Train Loss: 8.0075, Train Accuracy: 0.7096, Val Loss: 8.3009, Val Accuracy: 0.6839\n",
      "Iteration 0 - Loss: 14.6990, Accuracy: 0.3125\n",
      "Iteration 500 - Loss: 6.8562, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.1497, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 7.7459, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.5441, Accuracy: 0.6406\n",
      "Iteration 2500 - Loss: 7.9696, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.1169, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.7070, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.2996, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 7.3936, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 6.9370, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 6.8686, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.8920, Accuracy: 0.5938\n",
      "Iteration 6500 - Loss: 8.0123, Accuracy: 0.7500\n",
      "Fold: 2, Train Loss: 7.9724, Train Accuracy: 0.7121, Val Loss: 8.1379, Val Accuracy: 0.6923\n",
      "Iteration 0 - Loss: 14.5651, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 7.9062, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 7.9397, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.4122, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.5513, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 9.0671, Accuracy: 0.6250\n",
      "Iteration 3000 - Loss: 8.6125, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.6962, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 7.7794, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 7.4362, Accuracy: 0.7656\n",
      "Iteration 5000 - Loss: 7.8678, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 7.5096, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 7.2042, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 7.9954, Accuracy: 0.7500\n",
      "Fold: 3, Train Loss: 8.1065, Train Accuracy: 0.7071, Val Loss: 8.1141, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 17.3315, Accuracy: 0.1562\n",
      "Iteration 500 - Loss: 8.9380, Accuracy: 0.6406\n",
      "Iteration 1000 - Loss: 7.8300, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.3813, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.6288, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.5716, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 9.5154, Accuracy: 0.5781\n",
      "Iteration 3500 - Loss: 7.2644, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 7.0180, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 8.8824, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 7.9483, Accuracy: 0.6406\n",
      "Iteration 5500 - Loss: 7.2257, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 6.6861, Accuracy: 0.7969\n",
      "Iteration 6500 - Loss: 9.0065, Accuracy: 0.6562\n",
      "Fold: 4, Train Loss: 8.0636, Train Accuracy: 0.7081, Val Loss: 8.5319, Val Accuracy: 0.6651\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.67       0.75      \n",
      "1          0.65       0.64       0.64      \n",
      "2          0.53       0.72       0.61      \n",
      "3          0.81       0.67       0.73      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/a4c04f8d9796491aa3069b225910aa0e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 16.3477, Accuracy: 0.2656\n",
      "Iteration 500 - Loss: 7.9601, Accuracy: 0.6797\n",
      "Iteration 1000 - Loss: 8.0325, Accuracy: 0.7422\n",
      "Iteration 1500 - Loss: 8.2138, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 8.5736, Accuracy: 0.6016\n",
      "Iteration 2500 - Loss: 7.7643, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 8.1811, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 8.3205, Accuracy: 0.6719\n",
      "Iteration 4000 - Loss: 8.8509, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 7.4192, Accuracy: 0.7109\n",
      "Iteration 5000 - Loss: 9.7698, Accuracy: 0.7109\n",
      "Iteration 5500 - Loss: 7.9042, Accuracy: 0.7422\n",
      "Iteration 6000 - Loss: 7.3029, Accuracy: 0.7656\n",
      "Iteration 6500 - Loss: 8.9878, Accuracy: 0.6641\n",
      "Fold: 0, Train Loss: 8.1165, Train Accuracy: 0.7007, Val Loss: 7.9408, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 16.9521, Accuracy: 0.2109\n",
      "Iteration 500 - Loss: 7.4552, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.6373, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 8.6018, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 8.4332, Accuracy: 0.7266\n",
      "Iteration 2500 - Loss: 8.0903, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.6803, Accuracy: 0.6797\n",
      "Iteration 3500 - Loss: 7.7789, Accuracy: 0.6328\n",
      "Iteration 4000 - Loss: 8.4796, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 7.3810, Accuracy: 0.7266\n",
      "Iteration 5000 - Loss: 6.8661, Accuracy: 0.7422\n",
      "Iteration 5500 - Loss: 8.1729, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 9.0722, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 7.7360, Accuracy: 0.7188\n",
      "Fold: 1, Train Loss: 7.9946, Train Accuracy: 0.7077, Val Loss: 8.4126, Val Accuracy: 0.6473\n",
      "Iteration 0 - Loss: 13.6935, Accuracy: 0.5000\n",
      "Iteration 500 - Loss: 7.6897, Accuracy: 0.7422\n",
      "Iteration 1000 - Loss: 7.7863, Accuracy: 0.6797\n",
      "Iteration 1500 - Loss: 8.1254, Accuracy: 0.6797\n",
      "Iteration 2000 - Loss: 8.3267, Accuracy: 0.6328\n",
      "Iteration 2500 - Loss: 6.8955, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.8158, Accuracy: 0.7266\n",
      "Iteration 3500 - Loss: 8.4261, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.6219, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 7.8774, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 7.8257, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 7.4975, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 7.5397, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 7.8569, Accuracy: 0.6829\n",
      "Fold: 2, Train Loss: 7.9595, Train Accuracy: 0.7093, Val Loss: 8.2934, Val Accuracy: 0.6435\n",
      "Iteration 0 - Loss: 16.7482, Accuracy: 0.2422\n",
      "Iteration 500 - Loss: 8.7138, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.1238, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.9142, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 8.2582, Accuracy: 0.7109\n",
      "Iteration 2500 - Loss: 8.5821, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.9874, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.5137, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 8.8229, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 7.3624, Accuracy: 0.7109\n",
      "Iteration 5000 - Loss: 8.0164, Accuracy: 0.6953\n",
      "Iteration 5500 - Loss: 7.5968, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 7.8675, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 9.0933, Accuracy: 0.6406\n",
      "Fold: 3, Train Loss: 8.1008, Train Accuracy: 0.7026, Val Loss: 8.1035, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 15.1400, Accuracy: 0.5000\n",
      "Iteration 500 - Loss: 8.5452, Accuracy: 0.6484\n",
      "Iteration 1000 - Loss: 8.8064, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 8.0897, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.5552, Accuracy: 0.7344\n",
      "Iteration 2500 - Loss: 7.5254, Accuracy: 0.7578\n",
      "Iteration 3000 - Loss: 7.7378, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 7.9099, Accuracy: 0.7422\n",
      "Iteration 4000 - Loss: 7.8413, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 7.7680, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 8.1480, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 7.8630, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 8.4643, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.9863, Accuracy: 0.7109\n",
      "Fold: 4, Train Loss: 8.0468, Train Accuracy: 0.7058, Val Loss: 8.0984, Val Accuracy: 0.6735\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.87       0.75       0.80      \n",
      "1          0.60       0.82       0.70      \n",
      "2          0.52       0.48       0.50      \n",
      "3          0.89       0.49       0.63      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/e989bb41e1274afaa57602a8cf680b6a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.6594, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 10.3979, Accuracy: 0.7812\n",
      "Iteration 1000 - Loss: 10.4303, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 8.1117, Accuracy: 0.8125\n",
      "Iteration 2000 - Loss: 8.4598, Accuracy: 0.5938\n",
      "Iteration 2500 - Loss: 6.5772, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 9.3190, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 9.4930, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 9.1247, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 8.9806, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.1323, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 8.3215, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 9.6012, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.9418, Accuracy: 0.7812\n",
      "Fold: 0, Train Loss: 8.5996, Train Accuracy: 0.7064, Val Loss: 7.8739, Val Accuracy: 0.7308\n",
      "Iteration 0 - Loss: 14.4917, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 8.8325, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.9684, Accuracy: 0.8125\n",
      "Iteration 1500 - Loss: 8.4112, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.4827, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.7659, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 9.4813, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 7.1241, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 6.9612, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 9.6673, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.0002, Accuracy: 0.5625\n",
      "Iteration 5500 - Loss: 8.0623, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.9160, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 7.2474, Accuracy: 0.7188\n",
      "Fold: 1, Train Loss: 8.4950, Train Accuracy: 0.7117, Val Loss: 8.3104, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 14.9406, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 9.1277, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.7205, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.3517, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 9.0019, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.4938, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 7.9739, Accuracy: 0.8125\n",
      "Iteration 3500 - Loss: 8.1513, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 6.6882, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 9.3832, Accuracy: 0.5938\n",
      "Iteration 5000 - Loss: 9.0914, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 8.3870, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.3460, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.2605, Accuracy: 0.6562\n",
      "Fold: 2, Train Loss: 8.4810, Train Accuracy: 0.7104, Val Loss: 8.2937, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 15.1643, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 10.0648, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 10.1457, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 8.3919, Accuracy: 0.7812\n",
      "Iteration 2000 - Loss: 9.8603, Accuracy: 0.5312\n",
      "Iteration 2500 - Loss: 10.3072, Accuracy: 0.5312\n",
      "Iteration 3000 - Loss: 9.4409, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 8.3500, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 8.6458, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 10.1861, Accuracy: 0.5625\n",
      "Iteration 5000 - Loss: 8.9174, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 7.6948, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 6.8216, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.2115, Accuracy: 0.7188\n",
      "Fold: 3, Train Loss: 8.6301, Train Accuracy: 0.7051, Val Loss: 7.9624, Val Accuracy: 0.7101\n",
      "Iteration 0 - Loss: 12.4408, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 9.6947, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 9.4036, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 9.2557, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.6589, Accuracy: 0.8125\n",
      "Iteration 2500 - Loss: 9.1150, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.2365, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.5477, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 6.9854, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.4368, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.7102, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.0899, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.8373, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 8.3114, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 8.5181, Train Accuracy: 0.7083, Val Loss: 8.1672, Val Accuracy: 0.7008\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.62       0.72       0.67      \n",
      "2          0.53       0.49       0.51      \n",
      "3          0.78       0.66       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/4747d30a8bcb42cabed9c5a0041de1c3\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 13.7224, Accuracy: 0.3906\n",
      "Iteration 500 - Loss: 9.6353, Accuracy: 0.6719\n",
      "Iteration 1000 - Loss: 9.9889, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 7.4708, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 7.5909, Accuracy: 0.7656\n",
      "Iteration 2500 - Loss: 8.3056, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 7.3968, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 9.0315, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.9600, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 7.8702, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.3577, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 7.8716, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.1833, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.0102, Accuracy: 0.6719\n",
      "Fold: 0, Train Loss: 8.3252, Train Accuracy: 0.7114, Val Loss: 7.9088, Val Accuracy: 0.7308\n",
      "Iteration 0 - Loss: 14.8952, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 8.4393, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 8.7023, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 8.5001, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 7.9873, Accuracy: 0.8281\n",
      "Iteration 2500 - Loss: 8.1146, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.2676, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 7.5438, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 8.2902, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 7.9111, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 7.9389, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 7.7686, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 8.1004, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 9.0290, Accuracy: 0.6406\n",
      "Fold: 1, Train Loss: 8.2148, Train Accuracy: 0.7176, Val Loss: 8.2708, Val Accuracy: 0.6942\n",
      "Iteration 0 - Loss: 17.4724, Accuracy: 0.0938\n",
      "Iteration 500 - Loss: 8.8019, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.4667, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.6422, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 7.6946, Accuracy: 0.8281\n",
      "Iteration 2500 - Loss: 7.1858, Accuracy: 0.8182\n",
      "Iteration 3000 - Loss: 7.0835, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 7.6509, Accuracy: 0.8125\n",
      "Iteration 4000 - Loss: 8.5205, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 8.1031, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.8963, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 6.8818, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 8.4020, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.6714, Accuracy: 0.7812\n",
      "Fold: 2, Train Loss: 8.1996, Train Accuracy: 0.7165, Val Loss: 8.2164, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 13.2801, Accuracy: 0.4062\n",
      "Iteration 500 - Loss: 8.4674, Accuracy: 0.7656\n",
      "Iteration 1000 - Loss: 8.1183, Accuracy: 0.7969\n",
      "Iteration 1500 - Loss: 8.8147, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.2983, Accuracy: 0.7656\n",
      "Iteration 2500 - Loss: 8.0333, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.0469, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 7.8348, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 6.7700, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 7.1049, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.1206, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 7.9571, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 7.9527, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.1098, Accuracy: 0.7188\n",
      "Fold: 3, Train Loss: 8.3621, Train Accuracy: 0.7117, Val Loss: 7.8921, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 15.9620, Accuracy: 0.2031\n",
      "Iteration 500 - Loss: 9.2676, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 8.9000, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 8.3211, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 8.4428, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 7.8979, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.9443, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.5649, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.5130, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 7.6040, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 7.6609, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.3087, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.4178, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.5761, Accuracy: 0.6406\n",
      "Fold: 4, Train Loss: 8.3026, Train Accuracy: 0.7112, Val Loss: 8.1109, Val Accuracy: 0.7214\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.80       0.81      \n",
      "1          0.68       0.67       0.67      \n",
      "2          0.56       0.66       0.61      \n",
      "3          0.83       0.64       0.73      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/852996d2a2c447d88cfa885110ea6213\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 14.3922, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 8.9592, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 7.7617, Accuracy: 0.7344\n",
      "Iteration 1500 - Loss: 7.9189, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 7.3316, Accuracy: 0.7578\n",
      "Iteration 2500 - Loss: 8.3730, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.3664, Accuracy: 0.6797\n",
      "Iteration 3500 - Loss: 8.0785, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.0611, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 8.5585, Accuracy: 0.6953\n",
      "Iteration 5000 - Loss: 8.5177, Accuracy: 0.6797\n",
      "Iteration 5500 - Loss: 8.1535, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 6.7684, Accuracy: 0.8182\n",
      "Iteration 6500 - Loss: 7.4883, Accuracy: 0.7344\n",
      "Fold: 0, Train Loss: 8.2156, Train Accuracy: 0.7127, Val Loss: 7.8832, Val Accuracy: 0.7270\n",
      "Iteration 0 - Loss: 14.6711, Accuracy: 0.2891\n",
      "Iteration 500 - Loss: 8.0691, Accuracy: 0.7891\n",
      "Iteration 1000 - Loss: 8.3798, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.7065, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 8.4144, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.6125, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.5595, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 7.1667, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 7.7828, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 7.3996, Accuracy: 0.7578\n",
      "Iteration 5000 - Loss: 8.3329, Accuracy: 0.7422\n",
      "Iteration 5500 - Loss: 8.4066, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 7.5123, Accuracy: 0.7578\n",
      "Iteration 6500 - Loss: 7.5126, Accuracy: 0.7031\n",
      "Fold: 1, Train Loss: 8.1027, Train Accuracy: 0.7195, Val Loss: 8.2281, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 14.9850, Accuracy: 0.2969\n",
      "Iteration 500 - Loss: 8.4306, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.7172, Accuracy: 0.6797\n",
      "Iteration 1500 - Loss: 7.3209, Accuracy: 0.7891\n",
      "Iteration 2000 - Loss: 7.5238, Accuracy: 0.7422\n",
      "Iteration 2500 - Loss: 6.8643, Accuracy: 0.7891\n",
      "Iteration 3000 - Loss: 8.1322, Accuracy: 0.7266\n",
      "Iteration 3500 - Loss: 7.5749, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 8.5066, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 7.9882, Accuracy: 0.6797\n",
      "Iteration 5000 - Loss: 8.0354, Accuracy: 0.6953\n",
      "Iteration 5500 - Loss: 8.2447, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.5495, Accuracy: 0.6719\n",
      "Iteration 6500 - Loss: 7.2072, Accuracy: 0.7734\n",
      "Fold: 2, Train Loss: 8.0813, Train Accuracy: 0.7197, Val Loss: 8.1902, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 13.0734, Accuracy: 0.3594\n",
      "Iteration 500 - Loss: 7.6688, Accuracy: 0.7969\n",
      "Iteration 1000 - Loss: 8.4244, Accuracy: 0.6797\n",
      "Iteration 1500 - Loss: 7.8215, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.4341, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.6657, Accuracy: 0.6797\n",
      "Iteration 3000 - Loss: 8.2447, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 7.8665, Accuracy: 0.7266\n",
      "Iteration 4000 - Loss: 8.8314, Accuracy: 0.6641\n",
      "Iteration 4500 - Loss: 7.8184, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.3995, Accuracy: 0.7266\n",
      "Iteration 5500 - Loss: 7.9458, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.2846, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.5891, Accuracy: 0.6875\n",
      "Fold: 3, Train Loss: 8.2028, Train Accuracy: 0.7146, Val Loss: 8.0191, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 12.1792, Accuracy: 0.4531\n",
      "Iteration 500 - Loss: 8.1348, Accuracy: 0.7109\n",
      "Iteration 1000 - Loss: 8.1551, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 7.6832, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 8.2111, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 7.0299, Accuracy: 0.7891\n",
      "Iteration 3000 - Loss: 8.7304, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 7.9757, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 7.3477, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 7.9332, Accuracy: 0.7109\n",
      "Iteration 5000 - Loss: 8.5076, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 7.2989, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 7.7557, Accuracy: 0.7266\n",
      "Iteration 6500 - Loss: 7.6578, Accuracy: 0.7422\n",
      "Fold: 4, Train Loss: 8.1167, Train Accuracy: 0.7154, Val Loss: 8.0525, Val Accuracy: 0.7064\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.62       0.72       0.67      \n",
      "2          0.55       0.48       0.51      \n",
      "3          0.76       0.70       0.73      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/d348fcebef7943628dee0a782562ea9e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 13.6224, Accuracy: 0.3438\n",
      "Iteration 500 - Loss: 12.0177, Accuracy: 0.5000\n",
      "Iteration 1000 - Loss: 10.2230, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 9.4798, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.6540, Accuracy: 0.5938\n",
      "Iteration 2500 - Loss: 11.4066, Accuracy: 0.5000\n",
      "Iteration 3000 - Loss: 9.5651, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 9.9894, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 9.2875, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 9.2913, Accuracy: 0.8125\n",
      "Iteration 5000 - Loss: 9.0314, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.0348, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.9566, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.9411, Accuracy: 0.7812\n",
      "Fold: 0, Train Loss: 10.0486, Train Accuracy: 0.6488, Val Loss: 9.1061, Val Accuracy: 0.7251\n",
      "Iteration 0 - Loss: 17.5801, Accuracy: 0.1562\n",
      "Iteration 500 - Loss: 11.8496, Accuracy: 0.5000\n",
      "Iteration 1000 - Loss: 11.5518, Accuracy: 0.5000\n",
      "Iteration 1500 - Loss: 10.4300, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 11.3423, Accuracy: 0.5312\n",
      "Iteration 2500 - Loss: 10.8786, Accuracy: 0.5938\n",
      "Iteration 3000 - Loss: 10.3794, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 10.0104, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 7.5822, Accuracy: 0.8750\n",
      "Iteration 4500 - Loss: 9.1400, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 9.2898, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 9.0614, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.1225, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.8984, Accuracy: 0.7188\n",
      "Fold: 1, Train Loss: 10.1195, Train Accuracy: 0.6580, Val Loss: 9.5822, Val Accuracy: 0.6764\n",
      "Iteration 0 - Loss: 16.0163, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 10.8610, Accuracy: 0.7812\n",
      "Iteration 1000 - Loss: 11.6614, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 8.9960, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 10.1510, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 9.6729, Accuracy: 0.7059\n",
      "Iteration 3000 - Loss: 9.4542, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 10.2851, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 10.2811, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 9.5551, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.8849, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 9.8292, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 7.5078, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 10.6067, Accuracy: 0.6250\n",
      "Fold: 2, Train Loss: 9.9818, Train Accuracy: 0.6585, Val Loss: 9.3768, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 17.3329, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 13.0356, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 10.4883, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.5363, Accuracy: 0.5312\n",
      "Iteration 2000 - Loss: 10.6877, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 9.7707, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 10.5879, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 9.4118, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 9.8718, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 10.4819, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.9290, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 9.1412, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 9.4433, Accuracy: 0.5938\n",
      "Iteration 6500 - Loss: 8.7417, Accuracy: 0.7500\n",
      "Fold: 3, Train Loss: 10.6590, Train Accuracy: 0.6231, Val Loss: 9.1201, Val Accuracy: 0.7176\n",
      "Iteration 0 - Loss: 16.6226, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 12.5255, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 11.5103, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 9.1495, Accuracy: 0.8125\n",
      "Iteration 2000 - Loss: 9.3039, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.7476, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 11.0509, Accuracy: 0.4688\n",
      "Iteration 3500 - Loss: 10.3151, Accuracy: 0.5625\n",
      "Iteration 4000 - Loss: 8.7626, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 9.0087, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 9.6179, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 9.7915, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 7.7998, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 7.7684, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 10.0109, Train Accuracy: 0.6568, Val Loss: 9.1747, Val Accuracy: 0.7054\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.82       0.82      \n",
      "1          0.64       0.72       0.68      \n",
      "2          0.52       0.53       0.52      \n",
      "3          0.79       0.57       0.66      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/7076f5c9e8a8482db507b1b4e9787601\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 13.1577, Accuracy: 0.3594\n",
      "Iteration 500 - Loss: 10.8955, Accuracy: 0.6094\n",
      "Iteration 1000 - Loss: 9.0179, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 9.2532, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 8.5715, Accuracy: 0.8750\n",
      "Iteration 2500 - Loss: 8.3677, Accuracy: 0.7656\n",
      "Iteration 3000 - Loss: 10.0028, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 8.7605, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 9.8406, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 9.4664, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 9.5308, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.7670, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.5802, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.8166, Accuracy: 0.7031\n",
      "Fold: 0, Train Loss: 9.3268, Train Accuracy: 0.6910, Val Loss: 8.5399, Val Accuracy: 0.7345\n",
      "Iteration 0 - Loss: 16.2558, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 11.8462, Accuracy: 0.5469\n",
      "Iteration 1000 - Loss: 10.0457, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.4973, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 10.3232, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 9.1000, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 9.7206, Accuracy: 0.6406\n",
      "Iteration 3500 - Loss: 9.2575, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 9.7104, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 8.6213, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.4235, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 9.4025, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 8.5673, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.9273, Accuracy: 0.6719\n",
      "Fold: 1, Train Loss: 9.5145, Train Accuracy: 0.6789, Val Loss: 9.0182, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 15.8901, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 10.8420, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 9.0096, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 10.2754, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 9.2096, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 9.7678, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 8.9792, Accuracy: 0.7656\n",
      "Iteration 3500 - Loss: 8.9030, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 8.5292, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 9.4067, Accuracy: 0.6094\n",
      "Iteration 5000 - Loss: 8.9147, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 9.1252, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 7.6641, Accuracy: 0.8125\n",
      "Iteration 6500 - Loss: 8.9638, Accuracy: 0.7031\n",
      "Fold: 2, Train Loss: 9.5645, Train Accuracy: 0.6755, Val Loss: 8.9517, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.7865, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 11.6234, Accuracy: 0.5938\n",
      "Iteration 1000 - Loss: 10.0956, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 9.2820, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.6324, Accuracy: 0.6719\n",
      "Iteration 2500 - Loss: 9.6458, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 10.4884, Accuracy: 0.6094\n",
      "Iteration 3500 - Loss: 8.0522, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 9.9657, Accuracy: 0.6094\n",
      "Iteration 4500 - Loss: 7.9611, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 9.2595, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 9.3615, Accuracy: 0.5938\n",
      "Iteration 6000 - Loss: 9.3122, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 9.1784, Accuracy: 0.7031\n",
      "Fold: 3, Train Loss: 9.6341, Train Accuracy: 0.6823, Val Loss: 8.5516, Val Accuracy: 0.7167\n",
      "Iteration 0 - Loss: 14.5689, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 11.6928, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 9.6368, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.9534, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 9.9188, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.6772, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.2498, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.7531, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 9.1953, Accuracy: 0.6406\n",
      "Iteration 4500 - Loss: 8.5627, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.6772, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 8.8666, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.6573, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.4873, Accuracy: 0.7188\n",
      "Fold: 4, Train Loss: 9.6190, Train Accuracy: 0.6771, Val Loss: 8.8247, Val Accuracy: 0.7120\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.79       0.81      \n",
      "1          0.63       0.73       0.68      \n",
      "2          0.52       0.51       0.51      \n",
      "3          0.78       0.59       0.67      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/f0c609e0745a4944a14ac4f7e4352954\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 13.3802, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 11.0320, Accuracy: 0.6328\n",
      "Iteration 1000 - Loss: 10.2953, Accuracy: 0.6953\n",
      "Iteration 1500 - Loss: 9.3151, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 9.9004, Accuracy: 0.6172\n",
      "Iteration 2500 - Loss: 8.2919, Accuracy: 0.7891\n",
      "Iteration 3000 - Loss: 8.2259, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.6682, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.9373, Accuracy: 0.7891\n",
      "Iteration 4500 - Loss: 9.2052, Accuracy: 0.6596\n",
      "Iteration 5000 - Loss: 8.6699, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 8.7040, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 9.0182, Accuracy: 0.6797\n",
      "Iteration 6500 - Loss: 8.5584, Accuracy: 0.6875\n",
      "Fold: 0, Train Loss: 9.1116, Train Accuracy: 0.6949, Val Loss: 8.2741, Val Accuracy: 0.7251\n",
      "Iteration 0 - Loss: 14.5740, Accuracy: 0.3203\n",
      "Iteration 500 - Loss: 9.7313, Accuracy: 0.6719\n",
      "Iteration 1000 - Loss: 9.7289, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 9.1126, Accuracy: 0.7422\n",
      "Iteration 2000 - Loss: 9.4459, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 9.2282, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.6716, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.4485, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.6943, Accuracy: 0.7266\n",
      "Iteration 4500 - Loss: 7.9330, Accuracy: 0.7578\n",
      "Iteration 5000 - Loss: 8.8226, Accuracy: 0.7422\n",
      "Iteration 5500 - Loss: 9.0775, Accuracy: 0.6797\n",
      "Iteration 6000 - Loss: 8.0148, Accuracy: 0.7656\n",
      "Iteration 6500 - Loss: 7.8015, Accuracy: 0.7422\n",
      "Fold: 1, Train Loss: 8.9319, Train Accuracy: 0.7053, Val Loss: 8.6173, Val Accuracy: 0.7017\n",
      "Iteration 0 - Loss: 17.7112, Accuracy: 0.1484\n",
      "Iteration 500 - Loss: 10.7239, Accuracy: 0.5859\n",
      "Iteration 1000 - Loss: 9.6003, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 8.7578, Accuracy: 0.7734\n",
      "Iteration 2000 - Loss: 9.5209, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 9.0311, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 8.8368, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.9276, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.9577, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 8.7357, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 9.0624, Accuracy: 0.6953\n",
      "Iteration 5500 - Loss: 7.9849, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.9503, Accuracy: 0.7109\n",
      "Iteration 6500 - Loss: 7.7923, Accuracy: 0.7656\n",
      "Fold: 2, Train Loss: 9.0035, Train Accuracy: 0.6983, Val Loss: 8.5755, Val Accuracy: 0.7036\n",
      "Iteration 0 - Loss: 13.1032, Accuracy: 0.3828\n",
      "Iteration 500 - Loss: 10.5495, Accuracy: 0.6172\n",
      "Iteration 1000 - Loss: 9.6517, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.1128, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 9.7169, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.7489, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.3156, Accuracy: 0.6797\n",
      "Iteration 3500 - Loss: 8.8172, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 8.6640, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 9.1733, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.8425, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 8.4802, Accuracy: 0.7266\n",
      "Iteration 6000 - Loss: 8.2299, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 7.6814, Accuracy: 0.8125\n",
      "Fold: 3, Train Loss: 9.0745, Train Accuracy: 0.6982, Val Loss: 8.2716, Val Accuracy: 0.7261\n",
      "Iteration 0 - Loss: 15.5766, Accuracy: 0.2422\n",
      "Iteration 500 - Loss: 10.7520, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 9.1333, Accuracy: 0.7578\n",
      "Iteration 1500 - Loss: 8.8931, Accuracy: 0.7422\n",
      "Iteration 2000 - Loss: 8.6000, Accuracy: 0.7422\n",
      "Iteration 2500 - Loss: 8.4506, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 9.2545, Accuracy: 0.6797\n",
      "Iteration 3500 - Loss: 9.3349, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 8.5908, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 9.4042, Accuracy: 0.6406\n",
      "Iteration 5000 - Loss: 8.1447, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.8972, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 8.5513, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.4102, Accuracy: 0.7422\n",
      "Fold: 4, Train Loss: 9.0555, Train Accuracy: 0.7008, Val Loss: 8.4033, Val Accuracy: 0.7111\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.81       0.81      \n",
      "1          0.64       0.70       0.67      \n",
      "2          0.54       0.55       0.55      \n",
      "3          0.82       0.62       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/a299e956455243d682a90f6f069b37a7\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5762, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 8.6257, Accuracy: 0.5938\n",
      "Iteration 1000 - Loss: 7.9844, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.9803, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.5284, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.4362, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.4864, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.2482, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.3933, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 7.2939, Accuracy: 0.8125\n",
      "Iteration 5000 - Loss: 6.2748, Accuracy: 0.8750\n",
      "Iteration 5500 - Loss: 7.8590, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.2112, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 10.4067, Accuracy: 0.5312\n",
      "Fold: 0, Train Loss: 8.1330, Train Accuracy: 0.7048, Val Loss: 7.7524, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 15.5197, Accuracy: 0.6250\n",
      "Iteration 500 - Loss: 6.7334, Accuracy: 0.7812\n",
      "Iteration 1000 - Loss: 7.5008, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 9.1729, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 6.1439, Accuracy: 0.8125\n",
      "Iteration 2500 - Loss: 8.6613, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 8.9708, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 8.3006, Accuracy: 0.8438\n",
      "Iteration 4000 - Loss: 7.8927, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.0706, Accuracy: 0.8438\n",
      "Iteration 5000 - Loss: 8.5035, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.4131, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 9.1669, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 8.1501, Accuracy: 0.6250\n",
      "Fold: 1, Train Loss: 8.0505, Train Accuracy: 0.7103, Val Loss: 8.2509, Val Accuracy: 0.6679\n",
      "Iteration 0 - Loss: 15.5515, Accuracy: 0.6875\n",
      "Iteration 500 - Loss: 8.8469, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 6.4393, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 8.4440, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 8.5041, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.8427, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 7.3800, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 6.5865, Accuracy: 0.8438\n",
      "Iteration 4000 - Loss: 7.3555, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.1225, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 7.1822, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 9.0727, Accuracy: 0.5312\n",
      "Iteration 6000 - Loss: 9.4214, Accuracy: 0.5625\n",
      "Iteration 6500 - Loss: 7.9049, Accuracy: 0.6875\n",
      "Fold: 2, Train Loss: 8.0380, Train Accuracy: 0.7116, Val Loss: 8.2178, Val Accuracy: 0.6773\n",
      "Iteration 0 - Loss: 15.5596, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 8.4053, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.6335, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 8.4021, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.8115, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.1560, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 6.4746, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 8.0037, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.9926, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.6924, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 7.2217, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 9.6372, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 7.5185, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 6.5354, Accuracy: 0.7812\n",
      "Fold: 3, Train Loss: 8.1496, Train Accuracy: 0.7075, Val Loss: 8.0054, Val Accuracy: 0.6876\n",
      "Iteration 0 - Loss: 15.5502, Accuracy: 0.5625\n",
      "Iteration 500 - Loss: 8.0641, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.8841, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 7.3258, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.3150, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 7.5141, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 6.9451, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.1038, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 9.5511, Accuracy: 0.5000\n",
      "Iteration 4500 - Loss: 7.2645, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 9.9903, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 6.9593, Accuracy: 0.8438\n",
      "Iteration 6000 - Loss: 6.3574, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.8818, Accuracy: 0.7188\n",
      "Fold: 4, Train Loss: 8.0817, Train Accuracy: 0.7090, Val Loss: 8.0701, Val Accuracy: 0.7054\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.75       0.80      \n",
      "1          0.64       0.73       0.68      \n",
      "2          0.56       0.45       0.50      \n",
      "3          0.66       0.78       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/a9a65f7363d24c8aa87e2f1be14eae00\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5540, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 7.1805, Accuracy: 0.8125\n",
      "Iteration 1000 - Loss: 8.2554, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.9961, Accuracy: 0.7969\n",
      "Iteration 2000 - Loss: 9.1317, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.4636, Accuracy: 0.6406\n",
      "Iteration 3000 - Loss: 8.6804, Accuracy: 0.5957\n",
      "Iteration 3500 - Loss: 8.6496, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 8.3652, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 6.8109, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.5489, Accuracy: 0.6800\n",
      "Iteration 5500 - Loss: 7.6108, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.2994, Accuracy: 0.6094\n",
      "Iteration 6500 - Loss: 7.9204, Accuracy: 0.7344\n",
      "Fold: 0, Train Loss: 8.1332, Train Accuracy: 0.7031, Val Loss: 8.1443, Val Accuracy: 0.6895\n",
      "Iteration 0 - Loss: 15.5339, Accuracy: 0.5469\n",
      "Iteration 500 - Loss: 6.9008, Accuracy: 0.7656\n",
      "Iteration 1000 - Loss: 8.4799, Accuracy: 0.6406\n",
      "Iteration 1500 - Loss: 8.8666, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.1881, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.0432, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.2359, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.7517, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.8954, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.8393, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.9454, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 6.7471, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.4527, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.3541, Accuracy: 0.6406\n",
      "Fold: 1, Train Loss: 7.9979, Train Accuracy: 0.7112, Val Loss: 8.2282, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 15.5103, Accuracy: 0.4444\n",
      "Iteration 500 - Loss: 7.5587, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.0818, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 8.1473, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 7.1023, Accuracy: 0.7344\n",
      "Iteration 2500 - Loss: 7.6280, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.9235, Accuracy: 0.6719\n",
      "Iteration 3500 - Loss: 6.6703, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 8.6091, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.9446, Accuracy: 0.6250\n",
      "Iteration 5000 - Loss: 7.9690, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 7.0347, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 7.8833, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 7.2227, Accuracy: 0.7656\n",
      "Fold: 2, Train Loss: 7.9829, Train Accuracy: 0.7117, Val Loss: 8.1088, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 15.5599, Accuracy: 0.5625\n",
      "Iteration 500 - Loss: 8.9022, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.8816, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.5897, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.3923, Accuracy: 0.7391\n",
      "Iteration 2500 - Loss: 8.0686, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.9995, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 6.8095, Accuracy: 0.7969\n",
      "Iteration 4000 - Loss: 6.8700, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 8.6470, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 7.3975, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.1916, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.2759, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.4109, Accuracy: 0.6094\n",
      "Fold: 3, Train Loss: 8.1238, Train Accuracy: 0.7065, Val Loss: 8.3427, Val Accuracy: 0.6679\n",
      "Iteration 0 - Loss: 15.5383, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 8.9776, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.3261, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 8.0571, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 7.3941, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.4043, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 8.2646, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.4740, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.2833, Accuracy: 0.7812\n",
      "Iteration 4500 - Loss: 7.5235, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.3713, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 7.6293, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.5530, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 9.0766, Accuracy: 0.7031\n",
      "Fold: 4, Train Loss: 8.0708, Train Accuracy: 0.7080, Val Loss: 7.8740, Val Accuracy: 0.7139\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.63       0.72       0.67      \n",
      "2          0.54       0.46       0.50      \n",
      "3          0.74       0.70       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/daac9b8e00b047a4b29af3a39a64fb23\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.4932, Accuracy: 0.4453\n",
      "Iteration 500 - Loss: 7.7631, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.2857, Accuracy: 0.7121\n",
      "Iteration 1500 - Loss: 7.7797, Accuracy: 0.7109\n",
      "Iteration 2000 - Loss: 8.8537, Accuracy: 0.6641\n",
      "Iteration 2500 - Loss: 7.4606, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 7.5345, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.5505, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 8.2459, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3840, Accuracy: 0.6953\n",
      "Iteration 5000 - Loss: 8.3443, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 8.7160, Accuracy: 0.6406\n",
      "Iteration 6000 - Loss: 8.2716, Accuracy: 0.6484\n",
      "Iteration 6500 - Loss: 8.2814, Accuracy: 0.6641\n",
      "Fold: 0, Train Loss: 8.1023, Train Accuracy: 0.7011, Val Loss: 7.9205, Val Accuracy: 0.7092\n",
      "Iteration 0 - Loss: 15.4923, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 7.9371, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 7.2039, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 8.3968, Accuracy: 0.6698\n",
      "Iteration 2000 - Loss: 7.2362, Accuracy: 0.8047\n",
      "Iteration 2500 - Loss: 7.5583, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 7.1184, Accuracy: 0.7734\n",
      "Iteration 3500 - Loss: 8.4494, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 7.5529, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 8.7617, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 7.8088, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.9330, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 8.0087, Accuracy: 0.6328\n",
      "Iteration 6500 - Loss: 8.0812, Accuracy: 0.6875\n",
      "Fold: 1, Train Loss: 7.9996, Train Accuracy: 0.7075, Val Loss: 8.2170, Val Accuracy: 0.7036\n",
      "Iteration 0 - Loss: 15.5454, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 8.0012, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.3774, Accuracy: 0.6406\n",
      "Iteration 1500 - Loss: 7.4253, Accuracy: 0.7578\n",
      "Iteration 2000 - Loss: 7.3108, Accuracy: 0.7422\n",
      "Iteration 2500 - Loss: 7.9998, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.1446, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 8.4199, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 8.0174, Accuracy: 0.6953\n",
      "Iteration 4500 - Loss: 6.9111, Accuracy: 0.7734\n",
      "Iteration 5000 - Loss: 8.2437, Accuracy: 0.6953\n",
      "Iteration 5500 - Loss: 7.6683, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 8.3159, Accuracy: 0.6641\n",
      "Iteration 6500 - Loss: 8.0616, Accuracy: 0.7422\n",
      "Fold: 2, Train Loss: 7.9685, Train Accuracy: 0.7095, Val Loss: 8.1575, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 15.5378, Accuracy: 0.4844\n",
      "Iteration 500 - Loss: 7.8784, Accuracy: 0.7266\n",
      "Iteration 1000 - Loss: 8.7766, Accuracy: 0.6641\n",
      "Iteration 1500 - Loss: 8.3670, Accuracy: 0.6953\n",
      "Iteration 2000 - Loss: 8.8836, Accuracy: 0.6484\n",
      "Iteration 2500 - Loss: 7.8014, Accuracy: 0.6641\n",
      "Iteration 3000 - Loss: 8.0916, Accuracy: 0.6641\n",
      "Iteration 3500 - Loss: 7.5673, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 6.3836, Accuracy: 0.7891\n",
      "Iteration 4500 - Loss: 7.2658, Accuracy: 0.7266\n",
      "Iteration 5000 - Loss: 7.9411, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.8876, Accuracy: 0.6797\n",
      "Iteration 6000 - Loss: 7.9147, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 8.9552, Accuracy: 0.6484\n",
      "Fold: 3, Train Loss: 8.0858, Train Accuracy: 0.7047, Val Loss: 8.3947, Val Accuracy: 0.6567\n",
      "Iteration 0 - Loss: 15.5669, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 7.9306, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 8.4711, Accuracy: 0.6484\n",
      "Iteration 1500 - Loss: 7.9487, Accuracy: 0.7734\n",
      "Iteration 2000 - Loss: 7.5004, Accuracy: 0.7266\n",
      "Iteration 2500 - Loss: 7.5300, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.4118, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 7.9052, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 7.4496, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 8.5390, Accuracy: 0.6797\n",
      "Iteration 5000 - Loss: 8.4461, Accuracy: 0.6484\n",
      "Iteration 5500 - Loss: 8.4181, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.8130, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.2288, Accuracy: 0.7422\n",
      "Fold: 4, Train Loss: 8.0485, Train Accuracy: 0.7057, Val Loss: 7.9737, Val Accuracy: 0.6942\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.80       0.82      \n",
      "1          0.64       0.75       0.69      \n",
      "2          0.55       0.45       0.49      \n",
      "3          0.74       0.70       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/fd5832910bcf4e52b9a9428866d0842b\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5419, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 9.9399, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 7.5928, Accuracy: 0.6667\n",
      "Iteration 1500 - Loss: 8.1773, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.3460, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.5459, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 8.5275, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 6.8295, Accuracy: 0.8750\n",
      "Iteration 4000 - Loss: 8.3033, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.5629, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.2491, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 6.7782, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.6565, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 9.1244, Accuracy: 0.5938\n",
      "Fold: 0, Train Loss: 8.6187, Train Accuracy: 0.7073, Val Loss: 7.9092, Val Accuracy: 0.7345\n",
      "Iteration 0 - Loss: 15.5393, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 9.0265, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.4924, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 8.3820, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.1082, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 7.6721, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.1042, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 8.1248, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.2904, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 7.6111, Accuracy: 0.8438\n",
      "Iteration 5000 - Loss: 8.5409, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 6.4926, Accuracy: 0.8438\n",
      "Iteration 6000 - Loss: 8.3285, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.4199, Accuracy: 0.7188\n",
      "Fold: 1, Train Loss: 8.4806, Train Accuracy: 0.7146, Val Loss: 8.3352, Val Accuracy: 0.7111\n",
      "Iteration 0 - Loss: 15.5398, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 9.7236, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 9.9351, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.9364, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 9.2769, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.5590, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.4919, Accuracy: 0.5938\n",
      "Iteration 3500 - Loss: 9.5013, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 7.8445, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.1393, Accuracy: 0.5312\n",
      "Iteration 5000 - Loss: 8.6849, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 7.6055, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.1041, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 5.8396, Accuracy: 0.8438\n",
      "Fold: 2, Train Loss: 8.4709, Train Accuracy: 0.7156, Val Loss: 8.3452, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5439, Accuracy: 0.4062\n",
      "Iteration 500 - Loss: 8.4938, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 9.1298, Accuracy: 0.8125\n",
      "Iteration 1500 - Loss: 7.8809, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.3318, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 7.1459, Accuracy: 0.8750\n",
      "Iteration 3000 - Loss: 7.1563, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 8.1194, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 9.2611, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 8.8235, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.1133, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 8.6015, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 7.2917, Accuracy: 0.8438\n",
      "Iteration 6500 - Loss: 7.2274, Accuracy: 0.8125\n",
      "Fold: 3, Train Loss: 8.5999, Train Accuracy: 0.7096, Val Loss: 8.0325, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 15.5391, Accuracy: 0.6250\n",
      "Iteration 500 - Loss: 11.5249, Accuracy: 0.4688\n",
      "Iteration 1000 - Loss: 7.6891, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 9.7955, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.2670, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 7.5538, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 8.7624, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 9.2263, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.4733, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 6.9302, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.3292, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 7.2407, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 8.0077, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 7.2056, Accuracy: 0.8125\n",
      "Fold: 4, Train Loss: 8.5185, Train Accuracy: 0.7106, Val Loss: 8.1185, Val Accuracy: 0.7129\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.83       0.82      \n",
      "1          0.64       0.70       0.67      \n",
      "2          0.54       0.54       0.54      \n",
      "3          0.84       0.64       0.73      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/aeadacb913a14b05a0aa7209004ec010\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5444, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 9.0480, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 9.1787, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 8.5705, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.5733, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 8.2077, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.7125, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 7.9396, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 7.4213, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.4114, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.0077, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.7881, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.8688, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.6285, Accuracy: 0.7344\n",
      "Fold: 0, Train Loss: 8.3596, Train Accuracy: 0.7109, Val Loss: 7.9491, Val Accuracy: 0.7298\n",
      "Iteration 0 - Loss: 15.5431, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 9.1139, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.9981, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.1393, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 9.2749, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.4087, Accuracy: 0.7969\n",
      "Iteration 3000 - Loss: 8.3316, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.9729, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 8.0417, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3506, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 8.6823, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 8.4253, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 9.1600, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.8174, Accuracy: 0.6875\n",
      "Fold: 1, Train Loss: 8.2353, Train Accuracy: 0.7192, Val Loss: 8.2333, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5402, Accuracy: 0.4844\n",
      "Iteration 500 - Loss: 8.1671, Accuracy: 0.7969\n",
      "Iteration 1000 - Loss: 8.6330, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.9630, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.5425, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.2343, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 7.4146, Accuracy: 0.7969\n",
      "Iteration 3500 - Loss: 8.8381, Accuracy: 0.6406\n",
      "Iteration 4000 - Loss: 8.9233, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 7.9703, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.2661, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 8.7914, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 8.5792, Accuracy: 0.6406\n",
      "Iteration 6500 - Loss: 7.6450, Accuracy: 0.7812\n",
      "Fold: 2, Train Loss: 8.2210, Train Accuracy: 0.7183, Val Loss: 8.2973, Val Accuracy: 0.6848\n",
      "Iteration 0 - Loss: 15.5383, Accuracy: 0.5161\n",
      "Iteration 500 - Loss: 9.6904, Accuracy: 0.6719\n",
      "Iteration 1000 - Loss: 9.0428, Accuracy: 0.6406\n",
      "Iteration 1500 - Loss: 9.1152, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 8.2621, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 7.8114, Accuracy: 0.7656\n",
      "Iteration 3000 - Loss: 7.7955, Accuracy: 0.7656\n",
      "Iteration 3500 - Loss: 8.5428, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 9.1180, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 8.4321, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 7.8359, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.0243, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.3193, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 7.1078, Accuracy: 0.7812\n",
      "Fold: 3, Train Loss: 8.3509, Train Accuracy: 0.7131, Val Loss: 7.9848, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.5445, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 9.1401, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.6633, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 8.7394, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 9.6853, Accuracy: 0.6094\n",
      "Iteration 2500 - Loss: 7.9476, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.9221, Accuracy: 0.6719\n",
      "Iteration 3500 - Loss: 7.6670, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 6.8447, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 8.7536, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.0000, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.3485, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 7.4143, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 7.4207, Accuracy: 0.7500\n",
      "Fold: 4, Train Loss: 8.2813, Train Accuracy: 0.7124, Val Loss: 8.1364, Val Accuracy: 0.7017\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.63       0.72       0.67      \n",
      "2          0.54       0.49       0.52      \n",
      "3          0.77       0.66       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/dd9aacaead8a4effa2a8266d1c9189f8\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5415, Accuracy: 0.4453\n",
      "Iteration 500 - Loss: 9.0567, Accuracy: 0.6641\n",
      "Iteration 1000 - Loss: 8.1790, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 8.5328, Accuracy: 0.6953\n",
      "Iteration 2000 - Loss: 7.3545, Accuracy: 0.7734\n",
      "Iteration 2500 - Loss: 8.3730, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 6.6383, Accuracy: 0.8667\n",
      "Iteration 3500 - Loss: 7.9097, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.2129, Accuracy: 0.7407\n",
      "Iteration 4500 - Loss: 8.7313, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 7.9470, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.9130, Accuracy: 0.6328\n",
      "Iteration 6000 - Loss: 8.9787, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 7.9959, Accuracy: 0.7031\n",
      "Fold: 0, Train Loss: 8.2095, Train Accuracy: 0.7139, Val Loss: 7.8196, Val Accuracy: 0.7336\n",
      "Iteration 0 - Loss: 15.5419, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 8.5786, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.0891, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 7.6253, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.7011, Accuracy: 0.7656\n",
      "Iteration 2500 - Loss: 8.5208, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.8526, Accuracy: 0.7422\n",
      "Iteration 3500 - Loss: 7.9303, Accuracy: 0.6953\n",
      "Iteration 4000 - Loss: 7.9105, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.3086, Accuracy: 0.6797\n",
      "Iteration 5000 - Loss: 7.8747, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 7.2909, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.2891, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 7.9939, Accuracy: 0.7109\n",
      "Fold: 1, Train Loss: 8.0887, Train Accuracy: 0.7209, Val Loss: 8.2508, Val Accuracy: 0.6979\n",
      "Iteration 0 - Loss: 15.5438, Accuracy: 0.5547\n",
      "Iteration 500 - Loss: 8.8393, Accuracy: 0.6953\n",
      "Iteration 1000 - Loss: 8.1005, Accuracy: 0.7266\n",
      "Iteration 1500 - Loss: 8.0091, Accuracy: 0.7422\n",
      "Iteration 2000 - Loss: 8.7761, Accuracy: 0.6719\n",
      "Iteration 2500 - Loss: 7.8192, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.3942, Accuracy: 0.7356\n",
      "Iteration 3500 - Loss: 8.5340, Accuracy: 0.6953\n",
      "Iteration 4000 - Loss: 7.9812, Accuracy: 0.7266\n",
      "Iteration 4500 - Loss: 8.2127, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.5596, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 7.5572, Accuracy: 0.7578\n",
      "Iteration 6000 - Loss: 7.4121, Accuracy: 0.7656\n",
      "Iteration 6500 - Loss: 8.0691, Accuracy: 0.7031\n",
      "Fold: 2, Train Loss: 8.0809, Train Accuracy: 0.7202, Val Loss: 8.1992, Val Accuracy: 0.7017\n",
      "Iteration 0 - Loss: 15.5456, Accuracy: 0.5391\n",
      "Iteration 500 - Loss: 8.3202, Accuracy: 0.7734\n",
      "Iteration 1000 - Loss: 8.2839, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 8.7130, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.2430, Accuracy: 0.7578\n",
      "Iteration 2500 - Loss: 8.1623, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 8.3160, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 8.1793, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.7489, Accuracy: 0.7422\n",
      "Iteration 4500 - Loss: 8.7791, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 6.8066, Accuracy: 0.8667\n",
      "Iteration 5500 - Loss: 8.2058, Accuracy: 0.7266\n",
      "Iteration 6000 - Loss: 7.7403, Accuracy: 0.7734\n",
      "Iteration 6500 - Loss: 8.2780, Accuracy: 0.7188\n",
      "Fold: 3, Train Loss: 8.2001, Train Accuracy: 0.7159, Val Loss: 7.8906, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5436, Accuracy: 0.5000\n",
      "Iteration 500 - Loss: 8.6344, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.3890, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 8.1564, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 8.1430, Accuracy: 0.6797\n",
      "Iteration 2500 - Loss: 8.6255, Accuracy: 0.6484\n",
      "Iteration 3000 - Loss: 8.6846, Accuracy: 0.6484\n",
      "Iteration 3500 - Loss: 7.8008, Accuracy: 0.6641\n",
      "Iteration 4000 - Loss: 7.8587, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 7.4051, Accuracy: 0.7578\n",
      "Iteration 5000 - Loss: 8.1210, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 7.8194, Accuracy: 0.7578\n",
      "Iteration 6000 - Loss: 7.9279, Accuracy: 0.6641\n",
      "Iteration 6500 - Loss: 7.7533, Accuracy: 0.7578\n",
      "Fold: 4, Train Loss: 8.1448, Train Accuracy: 0.7151, Val Loss: 8.0387, Val Accuracy: 0.7101\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.82       0.82      \n",
      "1          0.63       0.71       0.67      \n",
      "2          0.54       0.50       0.52      \n",
      "3          0.79       0.66       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/1f32f2b8e9f24776aa1528e44cbda3e1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 13.0167, Accuracy: 0.4375\n",
      "Iteration 1000 - Loss: 11.2255, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 11.2124, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 10.3000, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 10.9599, Accuracy: 0.5625\n",
      "Iteration 3000 - Loss: 11.2587, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 9.7967, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 10.5647, Accuracy: 0.5625\n",
      "Iteration 4500 - Loss: 8.9029, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.9350, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 8.3902, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.5304, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 7.8373, Accuracy: 0.7500\n",
      "Fold: 0, Train Loss: 10.2724, Train Accuracy: 0.6740, Val Loss: 9.1427, Val Accuracy: 0.7308\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 12.5974, Accuracy: 0.5625\n",
      "Iteration 1000 - Loss: 12.1599, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 10.6793, Accuracy: 0.5312\n",
      "Iteration 2000 - Loss: 9.6585, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 9.2547, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 8.9156, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 10.2752, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 9.9055, Accuracy: 0.5938\n",
      "Iteration 4500 - Loss: 11.1736, Accuracy: 0.5312\n",
      "Iteration 5000 - Loss: 9.1957, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.7778, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 10.4281, Accuracy: 0.5312\n",
      "Iteration 6500 - Loss: 8.7309, Accuracy: 0.6562\n",
      "Fold: 1, Train Loss: 10.1498, Train Accuracy: 0.6784, Val Loss: 9.5580, Val Accuracy: 0.6782\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 13.0249, Accuracy: 0.5000\n",
      "Iteration 1000 - Loss: 11.6630, Accuracy: 0.5312\n",
      "Iteration 1500 - Loss: 12.3925, Accuracy: 0.5312\n",
      "Iteration 2000 - Loss: 9.8932, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 9.1253, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 9.8556, Accuracy: 0.5625\n",
      "Iteration 3500 - Loss: 10.5292, Accuracy: 0.5312\n",
      "Iteration 4000 - Loss: 9.7283, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 9.7873, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.5577, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 9.9415, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.1341, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 8.8305, Accuracy: 0.6875\n",
      "Fold: 2, Train Loss: 10.1794, Train Accuracy: 0.6796, Val Loss: 9.4508, Val Accuracy: 0.6895\n",
      "Iteration 0 - Loss: 15.5426, Accuracy: 0.6667\n",
      "Iteration 500 - Loss: 12.8003, Accuracy: 0.4688\n",
      "Iteration 1000 - Loss: 11.0950, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.8423, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 10.5802, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 11.4325, Accuracy: 0.5625\n",
      "Iteration 3000 - Loss: 11.6719, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 9.9899, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 9.3927, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 10.5838, Accuracy: 0.6250\n",
      "Iteration 5000 - Loss: 9.6065, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.5806, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 10.8984, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.7369, Accuracy: 0.8438\n",
      "Fold: 3, Train Loss: 10.3250, Train Accuracy: 0.6745, Val Loss: 9.0704, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.2560, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 11.5549, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 11.4953, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 10.8179, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 10.8523, Accuracy: 0.5312\n",
      "Iteration 3000 - Loss: 10.6981, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 11.0062, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 9.4906, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 8.8484, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.5542, Accuracy: 0.6250\n",
      "Iteration 5500 - Loss: 9.4888, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 8.5694, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 9.2203, Accuracy: 0.7812\n",
      "Fold: 4, Train Loss: 10.2298, Train Accuracy: 0.6762, Val Loss: 9.2806, Val Accuracy: 0.6998\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.63       0.72       0.67      \n",
      "2          0.50       0.52       0.51      \n",
      "3          0.77       0.55       0.64      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6600 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/ddb35bebc79341f487bacb0720f589ed\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.4844\n",
      "Iteration 500 - Loss: 10.8563, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 10.9202, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 10.8509, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 10.6038, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.5023, Accuracy: 0.8438\n",
      "Iteration 3000 - Loss: 9.3256, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 9.3520, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 9.9252, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.9544, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 9.5821, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 8.8222, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.1428, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 9.3353, Accuracy: 0.6406\n",
      "Fold: 0, Train Loss: 9.6488, Train Accuracy: 0.6901, Val Loss: 8.6432, Val Accuracy: 0.7355\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.4679, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 10.7273, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 10.2988, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 10.0939, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 10.0105, Accuracy: 0.6094\n",
      "Iteration 3000 - Loss: 9.5889, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 9.1534, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.3802, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 9.6540, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 9.0114, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.7030, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.6411, Accuracy: 0.6719\n",
      "Iteration 6500 - Loss: 9.6369, Accuracy: 0.6562\n",
      "Fold: 1, Train Loss: 9.5191, Train Accuracy: 0.6972, Val Loss: 9.0239, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.8172, Accuracy: 0.5938\n",
      "Iteration 1000 - Loss: 10.5329, Accuracy: 0.6406\n",
      "Iteration 1500 - Loss: 10.4146, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 10.1578, Accuracy: 0.5625\n",
      "Iteration 2500 - Loss: 10.4822, Accuracy: 0.6094\n",
      "Iteration 3000 - Loss: 9.6450, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 9.1742, Accuracy: 0.6719\n",
      "Iteration 4000 - Loss: 9.6469, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 9.3088, Accuracy: 0.7656\n",
      "Iteration 5000 - Loss: 8.5306, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 9.0108, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.4041, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 9.0555, Accuracy: 0.6562\n",
      "Fold: 2, Train Loss: 9.5510, Train Accuracy: 0.6957, Val Loss: 8.9884, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.5000\n",
      "Iteration 500 - Loss: 10.9982, Accuracy: 0.6719\n",
      "Iteration 1000 - Loss: 9.9552, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.7731, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 10.7499, Accuracy: 0.5938\n",
      "Iteration 2500 - Loss: 9.0854, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 9.0763, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 9.1413, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 8.7457, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3655, Accuracy: 0.7656\n",
      "Iteration 5000 - Loss: 10.0475, Accuracy: 0.5312\n",
      "Iteration 5500 - Loss: 8.4583, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 8.8516, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.9518, Accuracy: 0.7031\n",
      "Fold: 3, Train Loss: 9.6841, Train Accuracy: 0.6916, Val Loss: 8.5799, Val Accuracy: 0.7186\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 10.9895, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 10.7172, Accuracy: 0.6094\n",
      "Iteration 1500 - Loss: 11.4048, Accuracy: 0.5781\n",
      "Iteration 2000 - Loss: 9.4608, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 9.2350, Accuracy: 0.6719\n",
      "Iteration 3000 - Loss: 9.4564, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.6872, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 9.3419, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.8871, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 9.4354, Accuracy: 0.6250\n",
      "Iteration 5500 - Loss: 10.2029, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 8.6566, Accuracy: 0.7656\n",
      "Iteration 6500 - Loss: 8.1778, Accuracy: 0.7500\n",
      "Fold: 4, Train Loss: 9.6022, Train Accuracy: 0.6950, Val Loss: 8.7937, Val Accuracy: 0.7111\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.81       0.81      \n",
      "1          0.64       0.71       0.67      \n",
      "2          0.52       0.53       0.53      \n",
      "3          0.79       0.59       0.68      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/23807d3e403e484794d88f772e0fd2f4\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 10.8406, Accuracy: 0.6328\n",
      "Iteration 1000 - Loss: 9.8680, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 9.7211, Accuracy: 0.6484\n",
      "Iteration 2000 - Loss: 8.4884, Accuracy: 0.7578\n",
      "Iteration 2500 - Loss: 8.8668, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 9.0084, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 9.0913, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 8.5432, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 8.0956, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.7008, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 7.7086, Accuracy: 0.8047\n",
      "Iteration 6000 - Loss: 7.8957, Accuracy: 0.7692\n",
      "Iteration 6500 - Loss: 9.1115, Accuracy: 0.7109\n",
      "Fold: 0, Train Loss: 9.1355, Train Accuracy: 0.7011, Val Loss: 8.2596, Val Accuracy: 0.7345\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5234\n",
      "Iteration 500 - Loss: 10.0272, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 9.9461, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.2374, Accuracy: 0.7109\n",
      "Iteration 2000 - Loss: 9.4270, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 9.4886, Accuracy: 0.6484\n",
      "Iteration 3000 - Loss: 8.8551, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 8.4651, Accuracy: 0.7422\n",
      "Iteration 4000 - Loss: 8.9201, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.5475, Accuracy: 0.7422\n",
      "Iteration 5000 - Loss: 8.6849, Accuracy: 0.7109\n",
      "Iteration 5500 - Loss: 8.5843, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 8.2514, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 7.9895, Accuracy: 0.7578\n",
      "Fold: 1, Train Loss: 9.0041, Train Accuracy: 0.7089, Val Loss: 8.6134, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.4922\n",
      "Iteration 500 - Loss: 11.0464, Accuracy: 0.6328\n",
      "Iteration 1000 - Loss: 8.8827, Accuracy: 0.7422\n",
      "Iteration 1500 - Loss: 9.1335, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 8.6737, Accuracy: 0.7109\n",
      "Iteration 2500 - Loss: 9.0085, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 8.9448, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.6668, Accuracy: 0.7266\n",
      "Iteration 4000 - Loss: 8.5601, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 7.9834, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 7.8105, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 8.8192, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.1720, Accuracy: 0.6797\n",
      "Iteration 6500 - Loss: 8.3164, Accuracy: 0.7422\n",
      "Fold: 2, Train Loss: 8.9974, Train Accuracy: 0.7083, Val Loss: 8.6001, Val Accuracy: 0.6998\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.5408, Accuracy: 0.6328\n",
      "Iteration 1000 - Loss: 10.2210, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 9.5952, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.2347, Accuracy: 0.7109\n",
      "Iteration 2500 - Loss: 8.9580, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 9.3470, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.9208, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 8.9826, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 9.0348, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 8.4742, Accuracy: 0.7734\n",
      "Iteration 5500 - Loss: 8.6134, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.8664, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.6795, Accuracy: 0.6797\n",
      "Fold: 3, Train Loss: 9.1378, Train Accuracy: 0.7034, Val Loss: 8.2414, Val Accuracy: 0.7251\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 10.8412, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 9.1721, Accuracy: 0.7266\n",
      "Iteration 1500 - Loss: 8.7599, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 9.2173, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.3016, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.3549, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 9.1456, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 8.3340, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 8.8274, Accuracy: 0.7422\n",
      "Iteration 5000 - Loss: 8.9196, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 8.5810, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 8.5829, Accuracy: 0.7246\n",
      "Iteration 6500 - Loss: 7.8185, Accuracy: 0.7422\n",
      "Fold: 4, Train Loss: 9.0589, Train Accuracy: 0.7061, Val Loss: 8.4123, Val Accuracy: 0.7167\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.81       0.82      \n",
      "1          0.64       0.70       0.67      \n",
      "2          0.54       0.55       0.55      \n",
      "3          0.81       0.62       0.70      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/2918971136a64d179c0ba24cb3ade3ec\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6773, Val Loss: 8.3472, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6853, Val Loss: 8.9502, Val Accuracy: 0.6660\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6839, Val Loss: 8.5689, Val Accuracy: 0.6735\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6793, Val Loss: 8.1167, Val Accuracy: 0.6857\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6857, Val Loss: 8.5079, Val Accuracy: 0.7026\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.82       0.82      \n",
      "1          0.68       0.63       0.65      \n",
      "2          0.54       0.60       0.57      \n",
      "3          0.73       0.68       0.70      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/dd627290a48b49339341d8cc5870d2fa\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6796, Val Loss: 8.5420, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6870, Val Loss: 8.7014, Val Accuracy: 0.6961\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6871, Val Loss: 8.8047, Val Accuracy: 0.6454\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6824, Val Loss: 8.3314, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6811, Val Loss: 8.4663, Val Accuracy: 0.7008\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.77       0.85       0.81      \n",
      "1          0.67       0.58       0.63      \n",
      "2          0.51       0.70       0.59      \n",
      "3          0.88       0.48       0.62      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6600 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/aeb54588180040f5ba69ffb933bd76d9\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6714, Val Loss: 8.3407, Val Accuracy: 0.7073\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6814, Val Loss: 8.7972, Val Accuracy: 0.6773\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6867, Val Loss: 8.6009, Val Accuracy: 0.6820\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6864, Val Loss: 8.3851, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6784, Val Loss: 8.4742, Val Accuracy: 0.7017\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.79       0.84       0.82      \n",
      "1          0.68       0.65       0.66      \n",
      "2          0.54       0.43       0.48      \n",
      "3          0.63       0.80       0.70      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6700 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/fece87cb8b8e4f2796811364df3961cb\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6183, Val Loss: 10.1478, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6059, Val Loss: 10.7128, Val Accuracy: 0.6238\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6030, Val Loss: 10.5561, Val Accuracy: 0.6341\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5881, Val Loss: 10.1636, Val Accuracy: 0.6717\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6310, Val Loss: 10.2806, Val Accuracy: 0.6501\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.75       0.78      \n",
      "1          0.57       0.78       0.66      \n",
      "2          0.49       0.31       0.38      \n",
      "3          0.69       0.60       0.64      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/49dc80a9ddae4d60856ece16ed9ed2f8\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6094, Val Loss: 10.1785, Val Accuracy: 0.6979\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5927, Val Loss: 10.6985, Val Accuracy: 0.6304\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5994, Val Loss: 10.5764, Val Accuracy: 0.6388\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5993, Val Loss: 10.1437, Val Accuracy: 0.6848\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6150, Val Loss: 10.2283, Val Accuracy: 0.6529\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.78       0.81      \n",
      "1          0.57       0.81       0.67      \n",
      "2          0.46       0.27       0.34      \n",
      "3          0.71       0.58       0.64      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6400 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/ab624145b9bf495eb0c82927eba190be\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6036, Val Loss: 10.3542, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6169, Val Loss: 10.6506, Val Accuracy: 0.6182\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6039, Val Loss: 10.5133, Val Accuracy: 0.6295\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6187, Val Loss: 10.0464, Val Accuracy: 0.6773\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6070, Val Loss: 10.3684, Val Accuracy: 0.6623\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.59       0.73       0.65      \n",
      "2          0.46       0.34       0.39      \n",
      "3          0.66       0.60       0.63      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6300\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/ca4a460a1c7d4413a232ebf1ca22ea08\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.4051, Val Loss: 12.9007, Val Accuracy: 0.4953\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3700, Val Loss: 13.3270, Val Accuracy: 0.4803\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.3661, Val Loss: 13.2807, Val Accuracy: 0.5169\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.4029, Val Loss: 13.0436, Val Accuracy: 0.5563\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.4281, Val Loss: 13.0147, Val Accuracy: 0.5141\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.77       0.67       0.72      \n",
      "1          0.42       0.72       0.53      \n",
      "2          0.31       0.10       0.15      \n",
      "3          0.45       0.30       0.36      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5000 | F1-Score: 0.4600\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/f248a91f39904063a1242f68dba1cd67\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.3833, Val Loss: 13.1971, Val Accuracy: 0.4953\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3973, Val Loss: 13.1353, Val Accuracy: 0.4615\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.4080, Val Loss: 13.0467, Val Accuracy: 0.4869\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.4391, Val Loss: 12.9144, Val Accuracy: 0.5563\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.4566, Val Loss: 12.7126, Val Accuracy: 0.5450\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.68       0.67       0.67      \n",
      "1          0.48       0.64       0.54      \n",
      "2          0.37       0.24       0.29      \n",
      "3          0.57       0.45       0.50      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5200 | F1-Score: 0.5100\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/525a0b4dc3a04aefa38fbf743330ed96\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.3994, Val Loss: 13.1030, Val Accuracy: 0.5272\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.4233, Val Loss: 13.0274, Val Accuracy: 0.4934\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.3987, Val Loss: 12.7381, Val Accuracy: 0.5000\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.4654, Val Loss: 12.7462, Val Accuracy: 0.5553\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.4609, Val Loss: 13.1170, Val Accuracy: 0.5469\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.56       0.66      \n",
      "1          0.48       0.77       0.59      \n",
      "2          0.36       0.22       0.27      \n",
      "3          0.49       0.38       0.43      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5300 | F1-Score: 0.5100\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/dd0be8d158e14081a8fc17a791e3c061\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6681, Val Loss: 8.3114, Val Accuracy: 0.6923\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6951, Val Loss: 8.5848, Val Accuracy: 0.6961\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6836, Val Loss: 9.0515, Val Accuracy: 0.6548\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6700, Val Loss: 8.4770, Val Accuracy: 0.6848\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6777, Val Loss: 8.4089, Val Accuracy: 0.6998\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.74       0.87       0.80      \n",
      "1          0.60       0.69       0.64      \n",
      "2          0.53       0.32       0.40      \n",
      "3          0.72       0.70       0.71      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6500 | F1-Score: 0.6400\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/978608b4131645f98d296f6c3a183f62\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6674, Val Loss: 8.1351, Val Accuracy: 0.7233\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6784, Val Loss: 8.7239, Val Accuracy: 0.6642\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6891, Val Loss: 8.6716, Val Accuracy: 0.6754\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6691, Val Loss: 8.5212, Val Accuracy: 0.6801\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6809, Val Loss: 8.6224, Val Accuracy: 0.6773\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.76       0.82       0.79      \n",
      "1          0.64       0.64       0.64      \n",
      "2          0.52       0.51       0.52      \n",
      "3          0.72       0.62       0.67      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6500 | F1-Score: 0.6500\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/6e9974fb015e44a8bd9227b2aee92d16\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6740, Val Loss: 8.1886, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6857, Val Loss: 8.8158, Val Accuracy: 0.6313\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6861, Val Loss: 8.5260, Val Accuracy: 0.6820\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6717, Val Loss: 8.2955, Val Accuracy: 0.7036\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6753, Val Loss: 8.5086, Val Accuracy: 0.6820\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.77       0.80      \n",
      "1          0.58       0.77       0.66      \n",
      "2          0.52       0.34       0.41      \n",
      "3          0.74       0.67       0.70      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6500 | F1-Score: 0.6400\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/f993b8b847094ba2b5649fd4e5b82369\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5581, Val Loss: 9.8715, Val Accuracy: 0.6604\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5739, Val Loss: 10.5997, Val Accuracy: 0.6088\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5560, Val Loss: 10.4282, Val Accuracy: 0.6538\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5379, Val Loss: 10.0213, Val Accuracy: 0.6435\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5223, Val Loss: 10.4724, Val Accuracy: 0.6445\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.79       0.80      \n",
      "1          0.56       0.76       0.65      \n",
      "2          0.49       0.32       0.39      \n",
      "3          0.72       0.55       0.62      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/51f23e0fa35f4b1aad3eb1c05de4852a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5214, Val Loss: 10.3806, Val Accuracy: 0.6417\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6149, Val Loss: 10.2452, Val Accuracy: 0.6323\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5486, Val Loss: 10.4497, Val Accuracy: 0.6098\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6050, Val Loss: 9.9712, Val Accuracy: 0.6820\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5346, Val Loss: 10.4915, Val Accuracy: 0.6276\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.74       0.83       0.78      \n",
      "1          0.57       0.60       0.59      \n",
      "2          0.45       0.51       0.48      \n",
      "3          0.81       0.42       0.55      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6100 | F1-Score: 0.6100\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/8ec6f752a9ed47168401231c47be30d6\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5277, Val Loss: 10.4702, Val Accuracy: 0.6126\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5150, Val Loss: 10.6648, Val Accuracy: 0.5882\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5639, Val Loss: 10.5827, Val Accuracy: 0.6163\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5141, Val Loss: 10.3239, Val Accuracy: 0.6482\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5413, Val Loss: 10.6020, Val Accuracy: 0.6220\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.74       0.79      \n",
      "1          0.53       0.84       0.65      \n",
      "2          0.46       0.22       0.30      \n",
      "3          0.68       0.50       0.58      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6100 | F1-Score: 0.5900\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/1d0c7b08cf974447bcaf4c3e38cb4e3a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.2969, Val Loss: 13.1020, Val Accuracy: 0.4165\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3260, Val Loss: 13.0151, Val Accuracy: 0.4174\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.3064, Val Loss: 13.2069, Val Accuracy: 0.3996\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.2563, Val Loss: 13.5995, Val Accuracy: 0.3565\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.2574, Val Loss: 13.6885, Val Accuracy: 0.4081\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.55       0.49       0.52      \n",
      "1          0.39       0.32       0.35      \n",
      "2          0.28       0.46       0.35      \n",
      "3          0.40       0.27       0.32      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.3900 | F1-Score: 0.3900\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/010d712ad772472f9b4ccf2497125077\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.3349, Val Loss: 13.4316, Val Accuracy: 0.3705\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3101, Val Loss: 13.6835, Val Accuracy: 0.3846\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.2440, Val Loss: 13.9688, Val Accuracy: 0.3668\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.3204, Val Loss: 13.8040, Val Accuracy: 0.4522\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.3633, Val Loss: 13.9038, Val Accuracy: 0.3884\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.76       0.26       0.39      \n",
      "1          0.38       0.80       0.51      \n",
      "2          0.29       0.13       0.18      \n",
      "3          0.40       0.16       0.23      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.4000 | F1-Score: 0.3600\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/71c8ee66388047d7944397c6fc4c1457\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.3294, Val Loss: 13.6584, Val Accuracy: 0.3912\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3877, Val Loss: 12.8280, Val Accuracy: 0.4756\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.3250, Val Loss: 13.2832, Val Accuracy: 0.4268\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.1784, Val Loss: 14.6940, Val Accuracy: 0.2964\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.2434, Val Loss: 13.8118, Val Accuracy: 0.3321\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.19       0.16       0.17      \n",
      "1          0.38       0.44       0.41      \n",
      "2          0.31       0.36       0.33      \n",
      "3          0.72       0.45       0.55      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.3500 | F1-Score: 0.3500\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/c046e841225442f986e1ceadfec6a4ef\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6797, Val Loss: 8.2946, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6796, Val Loss: 8.8706, Val Accuracy: 0.6689\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6920, Val Loss: 8.8008, Val Accuracy: 0.6614\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6856, Val Loss: 8.3569, Val Accuracy: 0.6886\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6821, Val Loss: 9.1303, Val Accuracy: 0.6501\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.89       0.67       0.76      \n",
      "1          0.61       0.75       0.67      \n",
      "2          0.47       0.64       0.54      \n",
      "3          0.86       0.32       0.47      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6400 | F1-Score: 0.6300\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/d04cb6c8d8ac48678921c0d755f87d19\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6804, Val Loss: 8.1819, Val Accuracy: 0.7176\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6884, Val Loss: 8.5817, Val Accuracy: 0.6904\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6923, Val Loss: 8.6592, Val Accuracy: 0.6426\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6876, Val Loss: 8.6252, Val Accuracy: 0.6529\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6797, Val Loss: 8.4927, Val Accuracy: 0.7026\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.76       0.80      \n",
      "1          0.65       0.70       0.67      \n",
      "2          0.53       0.56       0.55      \n",
      "3          0.70       0.64       0.67      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6700 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/7dcceb8f25f74f1c982623abe9e884a6\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6853, Val Loss: 8.7602, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6917, Val Loss: 9.2328, Val Accuracy: 0.6651\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.7014, Val Loss: 8.6477, Val Accuracy: 0.6998\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6827, Val Loss: 8.3293, Val Accuracy: 0.6876\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6971, Val Loss: 8.4538, Val Accuracy: 0.7026\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.82       0.81      \n",
      "1          0.68       0.63       0.65      \n",
      "2          0.50       0.73       0.60      \n",
      "3          0.94       0.40       0.56      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/44fc3fd11df54c1fa9ef581c5da8937b\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6286, Val Loss: 10.1728, Val Accuracy: 0.6717\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6417, Val Loss: 10.6265, Val Accuracy: 0.6276\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6363, Val Loss: 10.5223, Val Accuracy: 0.6707\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6281, Val Loss: 10.2721, Val Accuracy: 0.6801\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6279, Val Loss: 10.4214, Val Accuracy: 0.6585\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.82       0.81      \n",
      "1          0.58       0.71       0.64      \n",
      "2          0.46       0.34       0.39      \n",
      "3          0.68       0.59       0.63      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/9d137d63a33147dfa24cfcec30e20bdd\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6303, Val Loss: 10.1495, Val Accuracy: 0.6689\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6444, Val Loss: 10.7174, Val Accuracy: 0.6257\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6284, Val Loss: 10.5332, Val Accuracy: 0.6510\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6271, Val Loss: 10.1068, Val Accuracy: 0.6726\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6273, Val Loss: 10.4188, Val Accuracy: 0.6370\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.74       0.79      \n",
      "1          0.54       0.86       0.66      \n",
      "2          0.50       0.18       0.26      \n",
      "3          0.71       0.58       0.64      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6000\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/3c912a1701674da7a6cabc4f3339c7fc\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6299, Val Loss: 10.1477, Val Accuracy: 0.6773\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6413, Val Loss: 10.6114, Val Accuracy: 0.6323\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6357, Val Loss: 10.4900, Val Accuracy: 0.6492\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6327, Val Loss: 10.1449, Val Accuracy: 0.6961\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6343, Val Loss: 10.4344, Val Accuracy: 0.6548\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.60       0.76       0.67      \n",
      "2          0.47       0.39       0.42      \n",
      "3          0.74       0.49       0.59      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6400 | F1-Score: 0.6400\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/063a3405e8364a1b9f8b5a6c0882fab1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5400, Val Loss: 12.9189, Val Accuracy: 0.5872\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5617, Val Loss: 13.0430, Val Accuracy: 0.5553\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5544, Val Loss: 12.9998, Val Accuracy: 0.5685\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5514, Val Loss: 12.8463, Val Accuracy: 0.6257\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5494, Val Loss: 13.0305, Val Accuracy: 0.5910\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.71       0.76      \n",
      "1          0.48       0.82       0.60      \n",
      "2          0.43       0.10       0.16      \n",
      "3          0.62       0.47       0.54      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5700 | F1-Score: 0.5300\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/15895ded07f74e88a8fde5b6b1a39f0a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5534, Val Loss: 12.8747, Val Accuracy: 0.6041\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5597, Val Loss: 13.0772, Val Accuracy: 0.5460\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5606, Val Loss: 12.9783, Val Accuracy: 0.5826\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5517, Val Loss: 12.7880, Val Accuracy: 0.6201\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5471, Val Loss: 13.0358, Val Accuracy: 0.5966\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.71       0.76      \n",
      "1          0.49       0.83       0.61      \n",
      "2          0.45       0.12       0.19      \n",
      "3          0.62       0.49       0.55      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5800 | F1-Score: 0.5400\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/9ac3606873984199a1819324f285b19c\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5496, Val Loss: 12.8875, Val Accuracy: 0.6041\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5511, Val Loss: 13.0902, Val Accuracy: 0.5497\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5613, Val Loss: 12.9888, Val Accuracy: 0.5600\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5613, Val Loss: 12.8326, Val Accuracy: 0.6060\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5487, Val Loss: 13.0131, Val Accuracy: 0.6023\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.77       0.75       0.76      \n",
      "1          0.48       0.77       0.59      \n",
      "2          0.41       0.13       0.20      \n",
      "3          0.63       0.49       0.55      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'st124783-a3-model'.\n",
      "2025/03/17 23:19:46 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: st124783-a3-model, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5700 | F1-Score: 0.5400\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/9143f88ef54f4ddcae05b4dc3833cf9f\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Best Model by Accuracy: <__main__.MyLogisticRegression object at 0x16595b470> with Accuracy: 0.7000\n",
      "\n",
      " Registered Model: st124783-a3-model (Version: <ModelVersion: aliases=[], creation_timestamp=1742228386680, current_stage='None', description='', last_updated_timestamp=1742228386680, name='st124783-a3-model', run_id='1d7dc3517be246b1ba404c4fa7ae335e', run_link='', source='/Users/binit/PycharmProjects/ML_AIT_A3/mlruns/1/1d7dc3517be246b1ba404c4fa7ae335e/artifacts/model', status='READY', status_message=None, tags={}, user_id='', version='1'>)\n",
      "Model Version 1 is now in 'Staging'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'st124783-a3-model'.\n",
      "/var/folders/8j/ndxdwkv11mg2yfgyrfgqxrwr0000gn/T/ipykernel_15615/3042286121.py:89: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest_version = client.get_latest_versions(model_name, stages=[\"None\"])[0].version\n",
      "/var/folders/8j/ndxdwkv11mg2yfgyrfgqxrwr0000gn/T/ipykernel_15615/3042286121.py:91: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(name=model_name, version=latest_version, stage=\"Staging\")\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:23:56.925021Z",
     "start_time": "2025-03-17T16:23:56.923304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Best Model \n",
    "print(best_acc, best_model_acc, best_params_acc)"
   ],
   "id": "2a208c5be0180403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 <__main__.MyLogisticRegression object at 0x16595b470> {'method': 'batch', 'lr': 0.001, 'weight_init': 'xavier', 'l2': None, 'batch_size': 64}\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "22ec1581a8d8e497"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
