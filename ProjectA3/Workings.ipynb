{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:14.304883Z",
     "start_time": "2025-03-20T11:28:13.369604Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from itertools import product\n",
    "import joblib\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:15.833673Z",
     "start_time": "2025-03-20T11:28:15.831285Z"
    }
   },
   "cell_type": "code",
   "source": "path = '/Users/binit/PycharmProjects/ML_AIT_A3/pythonProject/ProjectA3/Data/Out_287.csv'",
   "id": "fe516404610dee9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:16.284249Z",
     "start_time": "2025-03-20T11:28:16.274289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(path)\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ],
   "id": "3237e24f1c4f9f4b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:17.471587Z",
     "start_time": "2025-03-20T11:28:17.460413Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(5)",
   "id": "42d328e3b5614aa1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  brand  year  selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0   BMW  2017        3200000      13663  Diesel      Dealer    Automatic   \n",
       "1   BMW  2012         900000     155000  Diesel  Individual    Automatic   \n",
       "2   BMW  2017        2950000      39000  Diesel      Dealer    Automatic   \n",
       "3   BMW  2009        1100000      60000  Diesel  Individual    Automatic   \n",
       "4   BMW  2012        1100000      80000  Diesel  Individual    Automatic   \n",
       "\n",
       "   owner  mileage  engine  max_power  seats  \n",
       "0      1    22.69  1995.0     190.00    5.0  \n",
       "1      2    16.07  1995.0     181.00    4.0  \n",
       "2      1    19.59  1995.0     187.74    5.0  \n",
       "3      2    16.07  1995.0     181.00    5.0  \n",
       "4      2    16.07  1995.0     181.00    5.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>3200000</td>\n",
       "      <td>13663</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>1</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>900000</td>\n",
       "      <td>155000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>2950000</td>\n",
       "      <td>39000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>1</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2009</td>\n",
       "      <td>1100000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>1100000</td>\n",
       "      <td>80000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:18.005578Z",
     "start_time": "2025-03-20T11:28:17.995400Z"
    }
   },
   "cell_type": "code",
   "source": "df.describe()",
   "id": "1590c97e91bade66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              year  selling_price      km_driven        owner      mileage  \\\n",
       "count  6663.000000   6.663000e+03    6663.000000  6663.000000  6663.000000   \n",
       "mean   2013.588924   5.243708e+05   73044.716644     1.499475    19.508243   \n",
       "std       3.898769   5.090416e+05   48214.360087     0.733034     3.913635   \n",
       "min    1994.000000   2.999900e+04    1000.000000     1.000000     9.000000   \n",
       "25%    2011.000000   2.500000e+05   38000.000000     1.000000    16.800000   \n",
       "50%    2014.000000   4.200000e+05   69123.000000     1.000000    19.500000   \n",
       "75%    2017.000000   6.500000e+05  100000.000000     2.000000    22.540000   \n",
       "max    2020.000000   1.000000e+07  500000.000000     4.000000    42.000000   \n",
       "\n",
       "            engine    max_power        seats  \n",
       "count  6663.000000  6663.000000  6663.000000  \n",
       "mean   1433.287858    87.871039     5.442593  \n",
       "std     492.908839    31.622824     0.988814  \n",
       "min     624.000000    34.200000     4.000000  \n",
       "25%    1197.000000    68.000000     5.000000  \n",
       "50%    1248.000000    81.860000     5.000000  \n",
       "75%    1498.000000   100.000000     5.000000  \n",
       "max    3604.000000   400.000000    14.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6.663000e+03</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2013.588924</td>\n",
       "      <td>5.243708e+05</td>\n",
       "      <td>73044.716644</td>\n",
       "      <td>1.499475</td>\n",
       "      <td>19.508243</td>\n",
       "      <td>1433.287858</td>\n",
       "      <td>87.871039</td>\n",
       "      <td>5.442593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.898769</td>\n",
       "      <td>5.090416e+05</td>\n",
       "      <td>48214.360087</td>\n",
       "      <td>0.733034</td>\n",
       "      <td>3.913635</td>\n",
       "      <td>492.908839</td>\n",
       "      <td>31.622824</td>\n",
       "      <td>0.988814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1994.000000</td>\n",
       "      <td>2.999900e+04</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>624.000000</td>\n",
       "      <td>34.200000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2011.000000</td>\n",
       "      <td>2.500000e+05</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>1197.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>4.200000e+05</td>\n",
       "      <td>69123.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>1248.000000</td>\n",
       "      <td>81.860000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>6.500000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.540000</td>\n",
       "      <td>1498.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3604.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:19.574885Z",
     "start_time": "2025-03-20T11:28:19.572657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns_drop = ['seats','owner']\n",
    "df.drop(columns=columns_drop, inplace=True)"
   ],
   "id": "ff9f4f8c83e32d74",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:20.532351Z",
     "start_time": "2025-03-20T11:28:20.526547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Binning is applied to categorize car prices:\n",
    "selling_price = df['selling_price']  # Use your dataset\n",
    "bins = [0, 250000, 500000, 750000, df['selling_price'].max()]  # Define bin edges\n",
    "labels = [0, 1, 2, 3]  # Assign category labels\n",
    "\n",
    "df['price_category_bins'] = pd.cut(df['selling_price'], bins=bins, labels=labels, include_lowest=True)\n",
    "print(df['price_category_bins'].value_counts())"
   ],
   "id": "422e418174dd8fce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_category_bins\n",
      "1    2335\n",
      "0    1719\n",
      "2    1559\n",
      "3    1050\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:46:50.527987Z",
     "start_time": "2025-03-17T11:46:50.525130Z"
    }
   },
   "cell_type": "code",
   "source": "dff = df.loc[:, df.columns.drop('selling_price')]",
   "id": "6d62e0c30337da78",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:23.268671Z",
     "start_time": "2025-03-20T11:28:23.265637Z"
    }
   },
   "cell_type": "code",
   "source": "dff = df.copy()",
   "id": "ead44e63c474af2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:25.688648Z",
     "start_time": "2025-03-20T11:28:25.674873Z"
    }
   },
   "cell_type": "code",
   "source": "dff.query(\"selling_price > 750000 and selling_price < 10000000\").describe()",
   "id": "4a57110480872ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              year  selling_price      km_driven      mileage       engine  \\\n",
       "count  1049.000000   1.049000e+03    1049.000000  1049.000000  1049.000000   \n",
       "mean   2016.457579   1.287957e+06   56991.452812    18.200114  1877.421354   \n",
       "std       2.223307   8.239498e+05   44162.316900     4.508758   529.265577   \n",
       "min    2005.000000   7.510000e+05    1000.000000     9.000000   998.000000   \n",
       "25%    2015.000000   8.500000e+05   25000.000000    15.100000  1493.000000   \n",
       "50%    2017.000000   9.750000e+05   48000.000000    17.100000  1798.000000   \n",
       "75%    2018.000000   1.350000e+06   77000.000000    22.000000  2179.000000   \n",
       "max    2020.000000   7.200000e+06  426000.000000    28.400000  3604.000000   \n",
       "\n",
       "         max_power  \n",
       "count  1049.000000  \n",
       "mean    127.896273  \n",
       "std      39.454969  \n",
       "min      62.100000  \n",
       "25%      98.600000  \n",
       "50%     120.000000  \n",
       "75%     147.940000  \n",
       "max     282.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1.049000e+03</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>1049.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2016.457579</td>\n",
       "      <td>1.287957e+06</td>\n",
       "      <td>56991.452812</td>\n",
       "      <td>18.200114</td>\n",
       "      <td>1877.421354</td>\n",
       "      <td>127.896273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.223307</td>\n",
       "      <td>8.239498e+05</td>\n",
       "      <td>44162.316900</td>\n",
       "      <td>4.508758</td>\n",
       "      <td>529.265577</td>\n",
       "      <td>39.454969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>7.510000e+05</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>62.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>8.500000e+05</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>98.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>9.750000e+05</td>\n",
       "      <td>48000.000000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>1.350000e+06</td>\n",
       "      <td>77000.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2179.000000</td>\n",
       "      <td>147.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>7.200000e+06</td>\n",
       "      <td>426000.000000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>3604.000000</td>\n",
       "      <td>282.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:29.509049Z",
     "start_time": "2025-03-20T11:28:29.499471Z"
    }
   },
   "cell_type": "code",
   "source": "dff[dff['price_category_bins']==3].describe()",
   "id": "9d5d22f9b51b4407",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              year  selling_price      km_driven      mileage       engine  \\\n",
       "count  1050.000000   1.050000e+03    1050.000000  1050.000000  1050.000000   \n",
       "mean   2016.458095   1.296254e+06   56965.746667    18.222781  1877.508571   \n",
       "std       2.222310   8.663322e+05   44149.120843     4.566068   529.020795   \n",
       "min    2005.000000   7.510000e+05    1000.000000     9.000000   998.000000   \n",
       "25%    2015.000000   8.500000e+05   25000.000000    15.100000  1493.750000   \n",
       "50%    2017.000000   9.750000e+05   48000.000000    17.100000  1877.000000   \n",
       "75%    2018.000000   1.350000e+06   76784.750000    22.000000  2179.000000   \n",
       "max    2020.000000   1.000000e+07  426000.000000    42.000000  3604.000000   \n",
       "\n",
       "         max_power  \n",
       "count  1050.000000  \n",
       "mean    128.155419  \n",
       "std      40.320284  \n",
       "min      62.100000  \n",
       "25%      98.600000  \n",
       "50%     120.000000  \n",
       "75%     147.985000  \n",
       "max     400.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1.050000e+03</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2016.458095</td>\n",
       "      <td>1.296254e+06</td>\n",
       "      <td>56965.746667</td>\n",
       "      <td>18.222781</td>\n",
       "      <td>1877.508571</td>\n",
       "      <td>128.155419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.222310</td>\n",
       "      <td>8.663322e+05</td>\n",
       "      <td>44149.120843</td>\n",
       "      <td>4.566068</td>\n",
       "      <td>529.020795</td>\n",
       "      <td>40.320284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>7.510000e+05</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>62.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>8.500000e+05</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>1493.750000</td>\n",
       "      <td>98.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>9.750000e+05</td>\n",
       "      <td>48000.000000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>1877.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>1.350000e+06</td>\n",
       "      <td>76784.750000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2179.000000</td>\n",
       "      <td>147.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>426000.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3604.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:59:41.835543Z",
     "start_time": "2025-03-18T03:59:41.832220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dff[\"km_driven\"] = np.log1p(dff[\"km_driven\"])  # \n",
    "dff[\"max_power\"] = np.log1p(dff[\"max_power\"])"
   ],
   "id": "4717c01c4f788c7e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:31.875413Z",
     "start_time": "2025-03-20T11:28:31.868967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the mean selling price for each brand and brands are encoded using their mean selling price to reduce categorical noise:\n",
    "brand_mean_price = dff.groupby(\"brand\")[\"selling_price\"].mean().to_dict()\n",
    "\n",
    "# Replace brand names with their average selling price\n",
    "dff[\"brand_encoded\"] = dff[\"brand\"].map(brand_mean_price)"
   ],
   "id": "cce84c653f501cd2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:33.273075Z",
     "start_time": "2025-03-20T11:28:33.267699Z"
    }
   },
   "cell_type": "code",
   "source": "dff.head(3)",
   "id": "b615f5be05c22efc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  brand  year  selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0   BMW  2017        3200000      13663  Diesel      Dealer    Automatic   \n",
       "1   BMW  2012         900000     155000  Diesel  Individual    Automatic   \n",
       "2   BMW  2017        2950000      39000  Diesel      Dealer    Automatic   \n",
       "\n",
       "   mileage  engine  max_power price_category_bins  brand_encoded  \n",
       "0    22.69  1995.0     190.00                   3   2.826222e+06  \n",
       "1    16.07  1995.0     181.00                   3   2.826222e+06  \n",
       "2    19.59  1995.0     187.74                   3   2.826222e+06  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>price_category_bins</th>\n",
       "      <th>brand_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>3200000</td>\n",
       "      <td>13663</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>900000</td>\n",
       "      <td>155000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>2950000</td>\n",
       "      <td>39000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:34.478826Z",
     "start_time": "2025-03-20T11:28:34.475684Z"
    }
   },
   "cell_type": "code",
   "source": "dff['seller_type'].value_counts()",
   "id": "a2e1a829ed36bf1d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seller_type\n",
       "Individual          5977\n",
       "Dealer               659\n",
       "Trustmark Dealer      27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:35.916928Z",
     "start_time": "2025-03-20T11:28:35.911380Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(2)",
   "id": "456392856378e8ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  brand  year  selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0   BMW  2017        3200000      13663  Diesel      Dealer    Automatic   \n",
       "1   BMW  2012         900000     155000  Diesel  Individual    Automatic   \n",
       "\n",
       "   mileage  engine  max_power price_category_bins  \n",
       "0    22.69  1995.0      190.0                   3  \n",
       "1    16.07  1995.0      181.0                   3  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>price_category_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>3200000</td>\n",
       "      <td>13663</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>900000</td>\n",
       "      <td>155000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:36.953727Z",
     "start_time": "2025-03-20T11:28:36.942507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, ytrain, ytest = train_test_split(\n",
    "    dff.drop(columns=['price_category_bins','selling_price', 'brand', 'fuel','mileage']),\n",
    "    dff['price_category_bins'],\n",
    "    test_size=0.2,\n",
    "    stratify=dff['price_category_bins'],  # Ensures equal class proportions\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "columns_to_encode = ['seller_type','transmission']\n",
    "scaler = MinMaxScaler()\n",
    "oh = OneHotEncoder(handle_unknown='ignore')\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', oh, columns_to_encode),  # One-hot encoding for categorical columns\n",
    "    ('scaling', scaler, ['year', 'km_driven','engine','max_power',  'brand_encoded'])  # Scaling numerical columns\n",
    "], remainder='passthrough')\n",
    "\n",
    "X_train_trf = preprocessor.fit_transform(X_train)\n",
    "X_test_trf = preprocessor.transform(X_test)\n"
   ],
   "id": "a64c4abe03b40890",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:37:54.164078Z",
     "start_time": "2025-03-18T09:37:54.162161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "joblib.dump(brand_mean_price, 'brand_means.pkl')\n",
    "print(\"brand_means.pkl has been saved.\")"
   ],
   "id": "c18f7dc51f0c739a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand_means.pkl has been saved.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:41.578650Z",
     "start_time": "2025-03-20T11:28:41.576736Z"
    }
   },
   "cell_type": "code",
   "source": "feature_names = ['seller_type_Individual', 'seller_type_Dealer','seller_type_Trustmark_Dealer','transmission_Automatic', 'transmission_Manual','year','km_driven','engine', 'max_power', 'brand_encoded']",
   "id": "49a803830c029a53",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:42.179131Z",
     "start_time": "2025-03-20T11:28:42.172658Z"
    }
   },
   "cell_type": "code",
   "source": "X_train",
   "id": "233a85b0c3b49e0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year  km_driven seller_type transmission  engine  max_power  \\\n",
       "4506  2008      80000  Individual       Manual  1086.0      63.00   \n",
       "2325  2012      81632      Dealer       Manual  1399.0      68.00   \n",
       "2404  2015     130000  Individual    Automatic  2982.0     168.50   \n",
       "5353  2012     123000  Individual       Manual  1197.0      85.80   \n",
       "943   2015      62000      Dealer       Manual  1498.0      74.96   \n",
       "...    ...        ...         ...          ...     ...        ...   \n",
       "5746  2012      60000  Individual       Manual  1598.0     103.60   \n",
       "5090  2018      15000  Individual       Manual  1197.0      81.80   \n",
       "3947  2012      50000  Individual    Automatic  2143.0     170.00   \n",
       "1174  2016      70000  Individual       Manual  1248.0      88.50   \n",
       "4605  2015      90000  Individual       Manual  2179.0     120.00   \n",
       "\n",
       "      brand_encoded  \n",
       "4506   4.720181e+05  \n",
       "2325   5.081614e+05  \n",
       "2404   9.094595e+05  \n",
       "5353   3.949493e+05  \n",
       "943    6.236416e+05  \n",
       "...             ...  \n",
       "5746   4.737470e+05  \n",
       "5090   3.949493e+05  \n",
       "3947   2.242136e+06  \n",
       "1174   3.949493e+05  \n",
       "4605   6.236416e+05  \n",
       "\n",
       "[5330 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>brand_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4506</th>\n",
       "      <td>2008</td>\n",
       "      <td>80000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>63.00</td>\n",
       "      <td>4.720181e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>2012</td>\n",
       "      <td>81632</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>5.081614e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>2015</td>\n",
       "      <td>130000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2982.0</td>\n",
       "      <td>168.50</td>\n",
       "      <td>9.094595e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>2012</td>\n",
       "      <td>123000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>85.80</td>\n",
       "      <td>3.949493e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>2015</td>\n",
       "      <td>62000</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>74.96</td>\n",
       "      <td>6.236416e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5746</th>\n",
       "      <td>2012</td>\n",
       "      <td>60000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>103.60</td>\n",
       "      <td>4.737470e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>2018</td>\n",
       "      <td>15000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>81.80</td>\n",
       "      <td>3.949493e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>2012</td>\n",
       "      <td>50000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>170.00</td>\n",
       "      <td>2.242136e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>2016</td>\n",
       "      <td>70000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>88.50</td>\n",
       "      <td>3.949493e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4605</th>\n",
       "      <td>2015</td>\n",
       "      <td>90000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>2179.0</td>\n",
       "      <td>120.00</td>\n",
       "      <td>6.236416e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5330 rows Ã— 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:43.329413Z",
     "start_time": "2025-03-20T11:28:43.327014Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_trf[:5]",
   "id": "9541244a59bc6344",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.53846154, 0.15831663, 0.15503356, 0.07873155, 0.09977443],\n",
       "       [1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.69230769, 0.16158717, 0.26006711, 0.09240022, 0.1089036 ],\n",
       "       [0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.80769231, 0.25851703, 0.79127517, 0.36714051, 0.21026424],\n",
       "       [0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.69230769, 0.24448898, 0.19228188, 0.14106069, 0.08030826],\n",
       "       [1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.80769231, 0.12224449, 0.29328859, 0.11142701, 0.1380718 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T13:25:20.108954Z",
     "start_time": "2025-03-18T13:25:20.102089Z"
    }
   },
   "cell_type": "code",
   "source": "dff[dff['brand']=='BMW'].head(5)",
   "id": "4352072bde6911d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  brand  year  selling_price  km_driven    fuel seller_type transmission  \\\n",
       "0   BMW  2017        3200000      13663  Diesel      Dealer    Automatic   \n",
       "1   BMW  2012         900000     155000  Diesel  Individual    Automatic   \n",
       "2   BMW  2017        2950000      39000  Diesel      Dealer    Automatic   \n",
       "3   BMW  2009        1100000      60000  Diesel  Individual    Automatic   \n",
       "4   BMW  2012        1100000      80000  Diesel  Individual    Automatic   \n",
       "\n",
       "   mileage  engine  max_power price_category_bins  brand_encoded  \n",
       "0    22.69  1995.0     190.00                   3   2.826222e+06  \n",
       "1    16.07  1995.0     181.00                   3   2.826222e+06  \n",
       "2    19.59  1995.0     187.74                   3   2.826222e+06  \n",
       "3    16.07  1995.0     181.00                   3   2.826222e+06  \n",
       "4    16.07  1995.0     181.00                   3   2.826222e+06  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>price_category_bins</th>\n",
       "      <th>brand_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>3200000</td>\n",
       "      <td>13663</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>900000</td>\n",
       "      <td>155000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2017</td>\n",
       "      <td>2950000</td>\n",
       "      <td>39000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>187.74</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2009</td>\n",
       "      <td>1100000</td>\n",
       "      <td>60000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMW</td>\n",
       "      <td>2012</td>\n",
       "      <td>1100000</td>\n",
       "      <td>80000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>16.07</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>181.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.826222e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:54:25.732024Z",
     "start_time": "2025-03-18T09:54:25.725983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained model\n",
    "joblib.dump(preprocessor, \"preprocessor.pkl\")\n",
    "print(\"Preprocessor saved successfully!\")"
   ],
   "id": "e685ed12d5dcdf4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor saved successfully!\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T04:00:40.565368Z",
     "start_time": "2025-03-18T04:00:40.463244Z"
    }
   },
   "cell_type": "code",
   "source": "sns.displot(data=dff, x='year', kde=True)",
   "id": "d01d7f532f00ce3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12bc5fc20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:13:44.312135Z",
     "start_time": "2025-03-16T18:13:44.196781Z"
    }
   },
   "cell_type": "code",
   "source": "sns.boxplot(data=df, x='selling_price')",
   "id": "8144e240f6c1ac91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='selling_price'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:28:48.701510Z",
     "start_time": "2025-03-20T11:28:48.681697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Customs logistic regression \n",
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self, cv:int=5, lr:float=0.1, \n",
    "                 max_iter:int=5000, weight_init:str='uniform', \n",
    "                 method:str='mini_batch', batch_size:int=64, l2:float=None,):\n",
    "        self.lr = lr\n",
    "        self.cv = cv\n",
    "        self.max_iter = max_iter\n",
    "        self.weight_init = weight_init\n",
    "        self.method = method\n",
    "        self.batch_size = batch_size\n",
    "        self.l2 = 0 if l2 is None else l2\n",
    "        \n",
    "        valid_weight_init = ['uniform', 'normal', 'xavier','ones']\n",
    "        \n",
    "        if weight_init not in valid_weight_init:\n",
    "            raise ValueError(f'weight_init must be one of {valid_weight_init}')\n",
    "        \n",
    "        valid_method = ['mini_batch', 'batch', 'stochastic']\n",
    "        \n",
    "        if method not in valid_method:\n",
    "            raise ValueError(f'method must be one of {valid_method}')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.split = KFold(n_splits=self.cv)\n",
    "        y_class = len(np.unique(y))\n",
    "\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()  # Convert sparse matrix to dense\n",
    "\n",
    "        # Ensure y is a NumPy array\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = y.to_numpy() if hasattr(y, \"to_numpy\") else np.array(y)\n",
    "        \n",
    "        # One-Hot Encode y\n",
    "        self.oh = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        y_encoded = self.oh.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "        self.losses = []\n",
    "        self.valid_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.valid_accuracies = []\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(self.split.split(X)):\n",
    "            X_train, X_val = X[train_index], X[test_index]\n",
    "            y_train, y_val = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "            X_train = self._add_intercept(X_train)\n",
    "            X_val = self._add_intercept(X_val)\n",
    "\n",
    "            self.W = self.weight_initializer(X_train, y_class)\n",
    "            self.velocity = np.zeros_like(self.W)\n",
    "\n",
    "            fold_train_losses = []\n",
    "            fold_train_accuracies = []\n",
    "\n",
    "            if self.method == 'mini_batch':\n",
    "                for i in range(self.max_iter):\n",
    "                    ix = np.random.randint(0, X_train.shape[0])\n",
    "                    X_train_batch = X_train[ix:ix + self.batch_size]\n",
    "                    y_train_batch = y_train[ix:ix + self.batch_size]\n",
    "                    loss = self.train(X_train_batch, y_train_batch)\n",
    "                    fold_train_losses.append(loss)\n",
    "\n",
    "                    _, train_pred = self.predict(X_train_batch)\n",
    "                    train_accuracy = np.mean(np.argmax(y_train_batch, axis=1) == train_pred)\n",
    "                    fold_train_accuracies.append(train_accuracy)\n",
    "\n",
    "                    if i % 500 == 0:\n",
    "                        print(f\"Iteration {i} - Loss: {loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            elif self.method == 'batch':\n",
    "                for i in range(self.max_iter):\n",
    "                    loss = self.train(X_train, y_train)\n",
    "                    fold_train_losses.append(loss)\n",
    "\n",
    "                    _, train_pred = self.predict(X_train)\n",
    "                    train_accuracy = np.mean(np.argmax(y_train, axis=1) == train_pred)\n",
    "                    fold_train_accuracies.append(train_accuracy)\n",
    "\n",
    "                    if i % 500 == 0:\n",
    "                        print(f\"Iteration {i} - Loss: {loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            elif self.method == 'stochastic':\n",
    "                for i in range(self.max_iter):\n",
    "                    idx = np.random.randint(X_train.shape[0])  # Select a random index\n",
    "                    X_sto = X_train[idx, :].reshape(1, -1)  # Get the single sample\n",
    "                    y_sto = y_train[idx].reshape(1, -1)  # Get the corresponding label\n",
    "                    \n",
    "                    loss = self.train(X_sto, y_sto)  # Train the model on this single example\n",
    "                    \n",
    "                    if not np.isnan(loss):  # Ensure the loss is valid\n",
    "                        fold_train_losses.append(loss)\n",
    "                    \n",
    "                    _, train_pred = self.predict(X_sto)  # Get predicted class\n",
    "                    train_accuracy = np.mean(np.argmax(y_sto, axis=1) == train_pred)\n",
    "                    fold_train_accuracies.append(train_accuracy)\n",
    "        \n",
    "                    if i % 500 == 0:\n",
    "                        print(f\"Iteration {i} - Loss: {loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "                    \n",
    "            # Store the average training loss & accuracy for this fold\n",
    "            avg_train_loss = np.mean(fold_train_losses)\n",
    "            avg_train_accuracy = np.mean(fold_train_accuracies)\n",
    "            self.losses.append(avg_train_loss)\n",
    "            self.train_accuracies.append(avg_train_accuracy)\n",
    "\n",
    "            val_pred = self.predict(X_val)[1]  # Get predicted class labels\n",
    "            y_val_labels = self.oh.inverse_transform(y_val)\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss = self.cross_entropy(self.softmax_(X_val @ self.W), y_val)\n",
    "            self.valid_losses.append(val_loss)\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            val_accuracy = np.mean(y_val_labels.flatten() == val_pred)\n",
    "            self.valid_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Fold: {fold}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "                \n",
    "    def train(self, X, y):\n",
    "        y_hat, _  = self.predict(X)\n",
    "\n",
    "        error = y_hat - y\n",
    "        m = max(X.shape[0], 1)  # Ensure division is valid\n",
    "\n",
    "        loss = self.cross_entropy(y_hat, y) if m > 1 else float(np.mean(y_hat))\n",
    "        \n",
    "        grad = X.T @ error  +  2 * self.l2 * self.W \n",
    "        self.velocity = 0.8 * self.velocity - self.lr * grad\n",
    "        self.W += self.velocity\n",
    "        #self.W -= self.lr * grad\n",
    "        \n",
    "        return loss if not np.isnan(loss) else 0.0 \n",
    "                \n",
    "    def predict(self, X, is_test=False):\n",
    "        if is_test:\n",
    "            X = self._add_intercept(X)\n",
    "    \n",
    "        y_hat = X @ self.W\n",
    "        y_hat = self.softmax_(y_hat)\n",
    "        y_real = np.argmax(y_hat, axis=1)\n",
    "        return y_hat, y_real\n",
    "    \n",
    "    def softmax_(self, X):\n",
    "        X_max = np.max(X, axis=1, keepdims=True)  # Find max per row\n",
    "        exp_shifted = np.exp(X - X_max)  # Shift values for numerical stability\n",
    "        return exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)  # Normalize\n",
    "    \n",
    "    def weight_initializer(self, X, num_classes):\n",
    "        if self.weight_init == 'uniform':\n",
    "            return np.random.uniform(low=-self.lr, high=self.lr, size=(X.shape[1], num_classes))\n",
    "        elif self.weight_init == 'normal':\n",
    "            return np.random.randn(X.shape[1], num_classes)\n",
    "        elif self.weight_init == 'xavier':\n",
    "            limit = np.sqrt(6 / (X.shape[1] + num_classes))\n",
    "            return np.random.uniform(low=-limit, high=limit, size=(X.shape[1], num_classes))\n",
    "        else:\n",
    "            return np.ones((X.shape[1], num_classes))\n",
    "        \n",
    "    def cross_entropy(self, y, y_hat):\n",
    "        if y_hat.size == 0 or y.size == 0:\n",
    "            return 0.0  # Return zero loss to avoid NaN issues\n",
    "    \n",
    "        m = max(y.shape[0], 1)  # Prevent division by zero\n",
    "        loss = - np.sum(y * np.log(y_hat + 1e-9))/m ## Prevent log(0)\n",
    "        return  loss + self.l2*np.sum(self.W**2)\n",
    "    \n",
    "    def _add_intercept(self,X):\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    def classification_report(self, pred, y):\n",
    "        # Ensure both pred and y are NumPy arrays\n",
    "        pred = np.array(pred).flatten()\n",
    "        y = np.array(y)\n",
    "        \n",
    "        if len(pred) == 0 or len(y) == 0:  # Prevent empty array issues\n",
    "            return 0.0  # Return zero accuracy to avoid NaN\n",
    "    \n",
    "        # Extract unique labels from both predictions and ground truth\n",
    "        labels = np.unique(np.concatenate((y, pred)))\n",
    "        num_classes = len(labels)  # Update num_classes based on unique labels\n",
    "    \n",
    "        # Initialize confusion matrix\n",
    "        cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "        \n",
    "        for true, pred_label in zip(y, pred):\n",
    "            # Get index positions of true and predicted labels in `labels`\n",
    "            true_idx = np.where(labels == true)[0]\n",
    "            pred_idx = np.where(labels == pred_label)[0]\n",
    "    \n",
    "            if true_idx.size > 0 and pred_idx.size > 0:  # Check if valid indices exist\n",
    "                cm[true_idx[0], pred_idx[0]] += 1\n",
    "    \n",
    "        # Compute metrics\n",
    "        tp = np.diag(cm)\n",
    "        fp = np.sum(cm, axis=0) - tp\n",
    "        fn = np.sum(cm, axis=1) - tp\n",
    "    \n",
    "        precision = np.divide(tp, tp + fp, out=np.zeros_like(tp, dtype=float), where=(tp + fp) > 0)\n",
    "        recall = np.divide(tp, tp + fn, out=np.zeros_like(tp, dtype=float), where=(tp + fn) > 0)\n",
    "        f1_score = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(precision, dtype=float), where=(precision + recall) > 0)\n",
    "        \n",
    "        accuracy_score = np.sum(np.diag(cm)) / np.sum(cm) if np.sum(cm) > 0 else 0.0\n",
    "        \n",
    "        # Weighted metrics\n",
    "        class_counts = np.array([(y == label).sum() for label in labels]) / len(y)\n",
    "        weighted_precision = np.sum(class_counts * precision)\n",
    "        weighted_recall = np.sum(class_counts * recall)\n",
    "        weighted_f1 = np.sum(class_counts * f1_score)\n",
    "    \n",
    "        # Print report\n",
    "        print(\"\\nClassification Report:\\n\")\n",
    "        print(\"{:<10} {:<10} {:<10} {:<10}\".format(\"Class\", \"Precision\", \"Recall\", \"F1-score\"))\n",
    "        print(\"-\" * 40)\n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"{label:<10} {precision[i]:<10.2f} {recall[i]:<10.2f} {f1_score[i]:<10.2f}\")\n",
    "    \n",
    "        return round(accuracy_score, 2), round(weighted_precision, 2), round(weighted_recall, 2), round(weighted_f1, 2)\n",
    "    \n",
    "    \n",
    "    def _coeff_and_biases(self, feature_names):\n",
    "        if not hasattr(self, \"W\"):\n",
    "            raise ValueError(\"Model is not trained yet. Fit the model before retrieving coefficients.\")\n",
    "        \n",
    "        coef = self.W[1:,:]  # Exclude bias term\n",
    "        bias = self.W[0, :]\n",
    "        print(coef.shape, len(feature_names))\n",
    "        # Create a DataFrame for easy interpretation\n",
    "        coef_df = pd.DataFrame(coef, index=feature_names, columns=[f\"Class_{i}\" for i in range(coef.shape[1])] if coef.ndim > 1 else [\"Coefficient\"])\n",
    "        \n",
    "        print(\"\\nTop Important Features:\")\n",
    "        print(coef_df.abs().sum(axis=1).sort_values(ascending=False).head(10))\n",
    "\n",
    "        return coef_df, bias"
   ],
   "id": "f5a27a736dfa7df0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T09:04:06.671288Z",
     "start_time": "2025-03-11T09:04:06.668274Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d92cdf35d5f35b46",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T15:48:34.998913Z",
     "start_time": "2025-03-17T15:48:34.890198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## remove engine, owners, seats\n",
    "num_col = df.select_dtypes(include=['float','int64']).columns.tolist()\n",
    "sns.heatmap(df[num_col].corr(), cmap='Greens', annot=True, linewidths=0.5)"
   ],
   "id": "8425033d27460262",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:59:25.075291Z",
     "start_time": "2025-03-16T17:59:25.070869Z"
    }
   },
   "cell_type": "code",
   "source": "df['selling_price'].describe()",
   "id": "e2dec6f2252dcc98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6.663000e+03\n",
       "mean     5.243708e+05\n",
       "std      5.090416e+05\n",
       "min      2.999900e+04\n",
       "25%      2.500000e+05\n",
       "50%      4.200000e+05\n",
       "75%      6.500000e+05\n",
       "max      1.000000e+07\n",
       "Name: selling_price, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T12:01:41.359775Z",
     "start_time": "2025-03-11T12:01:41.237620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.histplot(data=df, x='selling_price', kde=True, color='green')"
   ],
   "id": "719173f42f3a807e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='selling_price', ylabel='Count'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:25:46.738501Z",
     "start_time": "2025-03-17T17:25:46.736332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = np.random.randint(low=1, high=100, size=(10,4))\n",
    "print(a.shape, a.flatten().shape)"
   ],
   "id": "4a40872088efaf94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4) (40,)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:36:50.428462Z",
     "start_time": "2025-03-20T11:36:15.539181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MyLogisticRegression(weight_init='xavier', lr=0.0005, batch_size=64, max_iter=10000, method='batch', l2=0.01, cv=6)\n",
    "model.fit(X_train_trf, ytrain)"
   ],
   "id": "415a822bcc982fae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Loss: 15.6131, Accuracy: 0.3499\n",
      "Iteration 500 - Loss: 24.3434, Accuracy: 0.6949\n",
      "Iteration 1000 - Loss: 32.4752, Accuracy: 0.7052\n",
      "Iteration 1500 - Loss: 38.0710, Accuracy: 0.7084\n",
      "Iteration 2000 - Loss: 42.0216, Accuracy: 0.7079\n",
      "Iteration 2500 - Loss: 44.8339, Accuracy: 0.7068\n",
      "Iteration 3000 - Loss: 46.8474, Accuracy: 0.7068\n",
      "Iteration 3500 - Loss: 48.2986, Accuracy: 0.7059\n",
      "Iteration 4000 - Loss: 49.3515, Accuracy: 0.7057\n",
      "Iteration 4500 - Loss: 50.1202, Accuracy: 0.7059\n",
      "Iteration 5000 - Loss: 50.6847, Accuracy: 0.7059\n",
      "Iteration 5500 - Loss: 51.1012, Accuracy: 0.7059\n",
      "Iteration 6000 - Loss: 51.4099, Accuracy: 0.7059\n",
      "Iteration 6500 - Loss: 51.6398, Accuracy: 0.7061\n",
      "Iteration 7000 - Loss: 51.8115, Accuracy: 0.7059\n",
      "Iteration 7500 - Loss: 51.9401, Accuracy: 0.7055\n",
      "Iteration 8000 - Loss: 52.0368, Accuracy: 0.7055\n",
      "Iteration 8500 - Loss: 52.1097, Accuracy: 0.7055\n",
      "Iteration 9000 - Loss: 52.1647, Accuracy: 0.7055\n",
      "Iteration 9500 - Loss: 52.2063, Accuracy: 0.7055\n",
      "Fold: 0, Train Loss: 46.3386, Train Accuracy: 0.7037, Val Loss: 52.1039, Val Accuracy: 0.7255\n",
      "Iteration 0 - Loss: 15.8175, Accuracy: 0.3497\n",
      "Iteration 500 - Loss: 24.3665, Accuracy: 0.6998\n",
      "Iteration 1000 - Loss: 32.5287, Accuracy: 0.7073\n",
      "Iteration 1500 - Loss: 38.1584, Accuracy: 0.7122\n",
      "Iteration 2000 - Loss: 42.1570, Accuracy: 0.7118\n",
      "Iteration 2500 - Loss: 45.0260, Accuracy: 0.7131\n",
      "Iteration 3000 - Loss: 47.0939, Accuracy: 0.7129\n",
      "Iteration 3500 - Loss: 48.5914, Accuracy: 0.7127\n",
      "Iteration 4000 - Loss: 49.6818, Accuracy: 0.7122\n",
      "Iteration 4500 - Loss: 50.4802, Accuracy: 0.7120\n",
      "Iteration 5000 - Loss: 51.0679, Accuracy: 0.7120\n",
      "Iteration 5500 - Loss: 51.5026, Accuracy: 0.7125\n",
      "Iteration 6000 - Loss: 51.8257, Accuracy: 0.7129\n",
      "Iteration 6500 - Loss: 52.0668, Accuracy: 0.7129\n",
      "Iteration 7000 - Loss: 52.2474, Accuracy: 0.7131\n",
      "Iteration 7500 - Loss: 52.3831, Accuracy: 0.7131\n",
      "Iteration 8000 - Loss: 52.4854, Accuracy: 0.7131\n",
      "Iteration 8500 - Loss: 52.5627, Accuracy: 0.7131\n",
      "Iteration 9000 - Loss: 52.6212, Accuracy: 0.7127\n",
      "Iteration 9500 - Loss: 52.6657, Accuracy: 0.7127\n",
      "Fold: 1, Train Loss: 46.6515, Train Accuracy: 0.7098, Val Loss: 52.8286, Val Accuracy: 0.6952\n",
      "Iteration 0 - Loss: 15.1739, Accuracy: 0.3539\n",
      "Iteration 500 - Loss: 24.4405, Accuracy: 0.6981\n",
      "Iteration 1000 - Loss: 32.8835, Accuracy: 0.7100\n",
      "Iteration 1500 - Loss: 38.7779, Accuracy: 0.7134\n",
      "Iteration 2000 - Loss: 42.9860, Accuracy: 0.7127\n",
      "Iteration 2500 - Loss: 46.0123, Accuracy: 0.7116\n",
      "Iteration 3000 - Loss: 48.2013, Accuracy: 0.7116\n",
      "Iteration 3500 - Loss: 49.7945, Accuracy: 0.7127\n",
      "Iteration 4000 - Loss: 50.9615, Accuracy: 0.7121\n",
      "Iteration 4500 - Loss: 51.8214, Accuracy: 0.7123\n",
      "Iteration 5000 - Loss: 52.4586, Accuracy: 0.7121\n",
      "Iteration 5500 - Loss: 52.9329, Accuracy: 0.7125\n",
      "Iteration 6000 - Loss: 53.2877, Accuracy: 0.7125\n",
      "Iteration 6500 - Loss: 53.5541, Accuracy: 0.7116\n",
      "Iteration 7000 - Loss: 53.7548, Accuracy: 0.7121\n",
      "Iteration 7500 - Loss: 53.9066, Accuracy: 0.7118\n",
      "Iteration 8000 - Loss: 54.0216, Accuracy: 0.7118\n",
      "Iteration 8500 - Loss: 54.1090, Accuracy: 0.7118\n",
      "Iteration 9000 - Loss: 54.1757, Accuracy: 0.7118\n",
      "Iteration 9500 - Loss: 54.2265, Accuracy: 0.7118\n",
      "Fold: 2, Train Loss: 47.8291, Train Accuracy: 0.7094, Val Loss: 54.5567, Val Accuracy: 0.7050\n",
      "Iteration 0 - Loss: 14.4843, Accuracy: 0.2882\n",
      "Iteration 500 - Loss: 24.4405, Accuracy: 0.6988\n",
      "Iteration 1000 - Loss: 32.5847, Accuracy: 0.7103\n",
      "Iteration 1500 - Loss: 38.1937, Accuracy: 0.7118\n",
      "Iteration 2000 - Loss: 42.1720, Accuracy: 0.7123\n",
      "Iteration 2500 - Loss: 45.0220, Accuracy: 0.7125\n",
      "Iteration 3000 - Loss: 47.0787, Accuracy: 0.7118\n",
      "Iteration 3500 - Loss: 48.5754, Accuracy: 0.7125\n",
      "Iteration 4000 - Loss: 49.6741, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 50.4871, Accuracy: 0.7125\n",
      "Iteration 5000 - Loss: 51.0932, Accuracy: 0.7127\n",
      "Iteration 5500 - Loss: 51.5482, Accuracy: 0.7127\n",
      "Iteration 6000 - Loss: 51.8917, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 52.1525, Accuracy: 0.7139\n",
      "Iteration 7000 - Loss: 52.3514, Accuracy: 0.7136\n",
      "Iteration 7500 - Loss: 52.5037, Accuracy: 0.7134\n",
      "Iteration 8000 - Loss: 52.6207, Accuracy: 0.7134\n",
      "Iteration 8500 - Loss: 52.7109, Accuracy: 0.7136\n",
      "Iteration 9000 - Loss: 52.7807, Accuracy: 0.7136\n",
      "Iteration 9500 - Loss: 52.8347, Accuracy: 0.7139\n",
      "Fold: 3, Train Loss: 46.7172, Train Accuracy: 0.7103, Val Loss: 52.9692, Val Accuracy: 0.6937\n",
      "Iteration 0 - Loss: 16.3628, Accuracy: 0.3489\n",
      "Iteration 500 - Loss: 24.4649, Accuracy: 0.6979\n",
      "Iteration 1000 - Loss: 32.7980, Accuracy: 0.7085\n",
      "Iteration 1500 - Loss: 38.5313, Accuracy: 0.7087\n",
      "Iteration 2000 - Loss: 42.5811, Accuracy: 0.7080\n",
      "Iteration 2500 - Loss: 45.4831, Accuracy: 0.7098\n",
      "Iteration 3000 - Loss: 47.5870, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 49.1280, Accuracy: 0.7091\n",
      "Iteration 4000 - Loss: 50.2669, Accuracy: 0.7085\n",
      "Iteration 4500 - Loss: 51.1156, Accuracy: 0.7078\n",
      "Iteration 5000 - Loss: 51.7524, Accuracy: 0.7082\n",
      "Iteration 5500 - Loss: 52.2332, Accuracy: 0.7087\n",
      "Iteration 6000 - Loss: 52.5983, Accuracy: 0.7085\n",
      "Iteration 6500 - Loss: 52.8768, Accuracy: 0.7082\n",
      "Iteration 7000 - Loss: 53.0901, Accuracy: 0.7078\n",
      "Iteration 7500 - Loss: 53.2541, Accuracy: 0.7073\n",
      "Iteration 8000 - Loss: 53.3806, Accuracy: 0.7073\n",
      "Iteration 8500 - Loss: 53.4785, Accuracy: 0.7071\n",
      "Iteration 9000 - Loss: 53.5543, Accuracy: 0.7071\n",
      "Iteration 9500 - Loss: 53.6132, Accuracy: 0.7071\n",
      "Fold: 4, Train Loss: 47.2893, Train Accuracy: 0.7058, Val Loss: 53.5990, Val Accuracy: 0.7095\n",
      "Iteration 0 - Loss: 16.8682, Accuracy: 0.3467\n",
      "Iteration 500 - Loss: 24.3035, Accuracy: 0.6968\n",
      "Iteration 1000 - Loss: 32.4508, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 38.0970, Accuracy: 0.7105\n",
      "Iteration 2000 - Loss: 42.1265, Accuracy: 0.7087\n",
      "Iteration 2500 - Loss: 45.0328, Accuracy: 0.7096\n",
      "Iteration 3000 - Loss: 47.1407, Accuracy: 0.7098\n",
      "Iteration 3500 - Loss: 48.6794, Accuracy: 0.7100\n",
      "Iteration 4000 - Loss: 49.8106, Accuracy: 0.7096\n",
      "Iteration 4500 - Loss: 50.6479, Accuracy: 0.7089\n",
      "Iteration 5000 - Loss: 51.2716, Accuracy: 0.7080\n",
      "Iteration 5500 - Loss: 51.7389, Accuracy: 0.7080\n",
      "Iteration 6000 - Loss: 52.0907, Accuracy: 0.7080\n",
      "Iteration 6500 - Loss: 52.3567, Accuracy: 0.7085\n",
      "Iteration 7000 - Loss: 52.5585, Accuracy: 0.7087\n",
      "Iteration 7500 - Loss: 52.7122, Accuracy: 0.7087\n",
      "Iteration 8000 - Loss: 52.8296, Accuracy: 0.7085\n",
      "Iteration 8500 - Loss: 52.9194, Accuracy: 0.7082\n",
      "Iteration 9000 - Loss: 52.9883, Accuracy: 0.7082\n",
      "Iteration 9500 - Loss: 53.0412, Accuracy: 0.7080\n",
      "Fold: 5, Train Loss: 46.8256, Train Accuracy: 0.7062, Val Loss: 53.0200, Val Accuracy: 0.7038\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:44:39.346107Z",
     "start_time": "2025-03-20T11:44:39.340618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_ , pred = model.predict(X_test_trf, is_test=True)\n",
    "print(model.classification_report(pred, ytest))\n",
    "print(len(pred))"
   ],
   "id": "76d242e009bf632f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.80       0.81      \n",
      "1          0.63       0.71       0.67      \n",
      "2          0.54       0.56       0.55      \n",
      "3          0.83       0.63       0.72      \n",
      "(np.float64(0.68), np.float64(0.69), np.float64(0.68), np.float64(0.69))\n",
      "1333\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:47:11.432792Z",
     "start_time": "2025-03-20T11:47:11.429377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w, _ = model._coeff_and_biases(feature_names) ##feature importance \n",
    "print(w)"
   ],
   "id": "5d77e4da4b79f231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4) 10\n",
      "\n",
      "Top Important Features:\n",
      "year                            70.910353\n",
      "max_power                       62.449079\n",
      "brand_encoded                   41.988042\n",
      "transmission_Manual             20.288856\n",
      "transmission_Automatic          19.582516\n",
      "engine                          16.461540\n",
      "seller_type_Dealer              14.395618\n",
      "seller_type_Trustmark_Dealer    13.834105\n",
      "seller_type_Individual          12.393648\n",
      "km_driven                        2.546607\n",
      "dtype: float64\n",
      "                                Class_0   Class_1   Class_2    Class_3\n",
      "seller_type_Individual         4.161615  1.549013 -1.642247  -5.040773\n",
      "seller_type_Dealer             5.116462  1.720745 -1.769878  -5.788532\n",
      "seller_type_Trustmark_Dealer   5.214722  1.597802 -1.324596  -5.696985\n",
      "transmission_Automatic         7.021064  2.979144 -1.968778  -7.613530\n",
      "transmission_Manual            7.247587  2.980364 -2.217318  -7.843586\n",
      "year                         -27.107099 -8.599632  9.165188  26.038435\n",
      "km_driven                     -0.213328 -1.234838  0.123280   0.975160\n",
      "engine                        -5.951848 -1.716339  2.318818   6.474535\n",
      "max_power                    -24.256754 -7.207182  6.807851  24.177291\n",
      "brand_encoded                -18.495390 -2.662245  6.707505  14.122903\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:36:56.373163Z",
     "start_time": "2025-03-20T11:36:56.306379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_loss_accuracy(losses, accuracies, title=\"Training Progress\"):\n",
    "    \"\"\"\n",
    "    Plots training loss and accuracy over iterations.\n",
    "    Returns:\n",
    "    - None (Displays the plot)\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    ax1.set_xlabel(\"Folds\")\n",
    "    ax1.set_ylabel(\"Loss\", color=\"tab:red\")\n",
    "    ax1.plot(losses, color=\"tab:red\", label=\"Loss\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "    ax2 = ax1.twinx()  # Create a secondary y-axis\n",
    "    ax2.set_ylabel(\"Accuracy\", color=\"tab:blue\")\n",
    "    ax2.plot(accuracies, color=\"tab:blue\", label=\"Accuracy\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    plt.title(title)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_loss_accuracy(model.losses, model.train_accuracies, title=\"Training Loss & Accuracy\")"
   ],
   "id": "387c0e0a2a624913",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T09:55:39.681432Z",
     "start_time": "2025-03-18T09:55:39.678438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "joblib.dump(model, \"model.pkl\")\n",
    "print(\"Preprocessor saved successfully!\")"
   ],
   "id": "57ff20a4244ce518",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor saved successfully!\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:19:46.693075Z",
     "start_time": "2025-03-17T16:08:03.692129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlflow.set_experiment(\"MLflow_Classification_Registry\")\n",
    "\n",
    "# Lists of hyperparameters\n",
    "methods = [\"batch\", \"mini_batch\", \"stochastic\"]  \n",
    "weights = [\"xavier\", \"normal\", \"uniform\"]\n",
    "penalty = [None, 0.01]\n",
    "lr = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]  # Added batch sizes\n",
    "\n",
    "# Initialize best model tracking\n",
    "best_model_acc, best_acc, best_params_acc = None, float('-inf'), None\n",
    "best_run_id = None\n",
    "\n",
    "def run_experiment(method, weight, lr, penalty, batch_size):\n",
    "    \"\"\"Runs ML experiments, logs results in MLflow, and tracks the best models.\"\"\"\n",
    "    global best_model_acc, best_acc, best_params_acc, best_run_id\n",
    "\n",
    "    params = {\n",
    "        \"method\": method, \"lr\": lr, \"weight_init\": weight, \"l2\": penalty, \"batch_size\": batch_size\n",
    "    }\n",
    "\n",
    "    print(f\"Running MyLogisticRegression | {method} | {weight} | LR: {lr} | l2: {penalty} | Batch Size: {batch_size}\")\n",
    "\n",
    "    # Initialize model with current hyperparameters\n",
    "    model = MyLogisticRegression(\n",
    "        lr=lr, \n",
    "        max_iter=7000, \n",
    "        weight_init=weight, \n",
    "        method=method,\n",
    "        batch_size=batch_size, \n",
    "        l2=penalty\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"method-{method}-lr-{lr}-weight-{weight}-batch-{batch_size}\", nested=True):\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_trf, ytrain)\n",
    "\n",
    "        # Predictions\n",
    "        _, y_pred = model.predict(X_test_trf, is_test=True)\n",
    "\n",
    "        # Compute metrics\n",
    "        acc, precision, recall, f1_score = model.classification_report(y_pred, ytest)\n",
    "\n",
    "        # Log hyperparameters & metrics in MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics({\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1_score\": f1_score})\n",
    "\n",
    "        # Save Model Signature\n",
    "        signature = mlflow.models.infer_signature(X_train_trf, model.predict(X_train_trf, is_test=True)[1])\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature)\n",
    "\n",
    "        print(f\" Model: MyLogisticRegression | Method: {method} | Accuracy: {acc:.4f} | F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "        # Track the Best Model Based on Accuracy (HIGHER IS BETTER)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_acc = model\n",
    "            best_params_acc = params\n",
    "            best_run_id = mlflow.active_run().info.run_id  # Save the best run ID for registry\n",
    "\n",
    "\n",
    "# Iterate over all hyperparameter combinations\n",
    "combinations_ = product(methods, weights, lr, batch_sizes)\n",
    "\n",
    "# Run all experiments\n",
    "for method, weight, lr, batch_size in combinations_:\n",
    "    run_experiment(method, weight, lr, None, batch_size)\n",
    "\n",
    "# Final Results: Best Model Based on Accuracy\n",
    "print(f\"Best Model by Accuracy: {best_model_acc} with Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "# -----------------------\n",
    "# Register Best Model in MLflow Model Registry\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")  # Make sure MLflow server is running\n",
    "\n",
    "if best_run_id:\n",
    "    model_uri = f\"runs:/{best_run_id}/model\"\n",
    "    model_name = \"st124783-a3-model\"  # Replace with your student ID\n",
    "\n",
    "    # Register the best model\n",
    "    model_version = mlflow.register_model(model_uri, model_name)\n",
    "    print(f\"\\n Registered Model: {model_name} (Version: {model_version})\")\n",
    "\n",
    "    # Move the Model to 'Staging'\n",
    "    client = MlflowClient()\n",
    "    latest_version = client.get_latest_versions(model_name, stages=[\"None\"])[0].version\n",
    "\n",
    "    client.transition_model_version_stage(name=model_name, version=latest_version, stage=\"Staging\")\n",
    "    print(f\"Model Version {latest_version} is now in 'Staging'!\")"
   ],
   "id": "72f96d9b29a1d53d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/17 23:08:03 INFO mlflow.tracking.fluent: Experiment with name 'MLflow_Classification_Registry' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MyLogisticRegression | batch | xavier | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.4983, Accuracy: 0.4641\n",
      "Iteration 500 - Loss: 8.3558, Accuracy: 0.6794\n",
      "Iteration 1000 - Loss: 6.3595, Accuracy: 0.6789\n",
      "Iteration 1500 - Loss: 7.3733, Accuracy: 0.6100\n",
      "Iteration 2000 - Loss: 8.0450, Accuracy: 0.5424\n",
      "Iteration 2500 - Loss: 10.0565, Accuracy: 0.5227\n",
      "Iteration 3000 - Loss: 8.5419, Accuracy: 0.6677\n",
      "Iteration 3500 - Loss: 6.9325, Accuracy: 0.6041\n",
      "Iteration 4000 - Loss: 9.0302, Accuracy: 0.5910\n",
      "Iteration 4500 - Loss: 7.6723, Accuracy: 0.6393\n",
      "Iteration 5000 - Loss: 8.2311, Accuracy: 0.6606\n",
      "Iteration 5500 - Loss: 7.4938, Accuracy: 0.6651\n",
      "Iteration 6000 - Loss: 9.5955, Accuracy: 0.6489\n",
      "Iteration 6500 - Loss: 7.0547, Accuracy: 0.6360\n",
      "Fold: 0, Train Loss: 8.1863, Train Accuracy: 0.6055, Val Loss: 7.1282, Val Accuracy: 0.6595\n",
      "Iteration 0 - Loss: 16.5833, Accuracy: 0.4308\n",
      "Iteration 500 - Loss: 7.4965, Accuracy: 0.6520\n",
      "Iteration 1000 - Loss: 9.0381, Accuracy: 0.5471\n",
      "Iteration 1500 - Loss: 6.2497, Accuracy: 0.6438\n",
      "Iteration 2000 - Loss: 9.7010, Accuracy: 0.6295\n",
      "Iteration 2500 - Loss: 8.5572, Accuracy: 0.6095\n",
      "Iteration 3000 - Loss: 8.6277, Accuracy: 0.6041\n",
      "Iteration 3500 - Loss: 8.6449, Accuracy: 0.5598\n",
      "Iteration 4000 - Loss: 8.9519, Accuracy: 0.6062\n",
      "Iteration 4500 - Loss: 8.4718, Accuracy: 0.5476\n",
      "Iteration 5000 - Loss: 7.0150, Accuracy: 0.6670\n",
      "Iteration 5500 - Loss: 9.4160, Accuracy: 0.5516\n",
      "Iteration 6000 - Loss: 8.9239, Accuracy: 0.5483\n",
      "Iteration 6500 - Loss: 6.9563, Accuracy: 0.6435\n",
      "Fold: 1, Train Loss: 8.0893, Train Accuracy: 0.6102, Val Loss: 7.7931, Val Accuracy: 0.6285\n",
      "Iteration 0 - Loss: 14.7750, Accuracy: 0.3935\n",
      "Iteration 500 - Loss: 8.0770, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 10.1438, Accuracy: 0.6067\n",
      "Iteration 1500 - Loss: 7.5532, Accuracy: 0.6147\n",
      "Iteration 2000 - Loss: 6.7299, Accuracy: 0.5905\n",
      "Iteration 2500 - Loss: 8.4786, Accuracy: 0.7127\n",
      "Iteration 3000 - Loss: 7.4542, Accuracy: 0.6140\n",
      "Iteration 3500 - Loss: 7.3668, Accuracy: 0.5917\n",
      "Iteration 4000 - Loss: 7.8785, Accuracy: 0.5912\n",
      "Iteration 4500 - Loss: 7.1436, Accuracy: 0.6177\n",
      "Iteration 5000 - Loss: 7.6572, Accuracy: 0.5223\n",
      "Iteration 5500 - Loss: 6.8945, Accuracy: 0.6531\n",
      "Iteration 6000 - Loss: 9.5149, Accuracy: 0.6198\n",
      "Iteration 6500 - Loss: 8.2839, Accuracy: 0.6550\n",
      "Fold: 2, Train Loss: 8.0557, Train Accuracy: 0.6118, Val Loss: 7.4937, Val Accuracy: 0.6379\n",
      "Iteration 0 - Loss: 16.7099, Accuracy: 0.4233\n",
      "Iteration 500 - Loss: 7.2399, Accuracy: 0.6405\n",
      "Iteration 1000 - Loss: 6.6880, Accuracy: 0.6095\n",
      "Iteration 1500 - Loss: 7.3765, Accuracy: 0.6602\n",
      "Iteration 2000 - Loss: 8.1798, Accuracy: 0.5359\n",
      "Iteration 2500 - Loss: 8.0342, Accuracy: 0.6196\n",
      "Iteration 3000 - Loss: 9.4437, Accuracy: 0.6377\n",
      "Iteration 3500 - Loss: 9.5908, Accuracy: 0.6229\n",
      "Iteration 4000 - Loss: 7.3388, Accuracy: 0.6280\n",
      "Iteration 4500 - Loss: 8.2192, Accuracy: 0.6337\n",
      "Iteration 5000 - Loss: 8.8824, Accuracy: 0.5692\n",
      "Iteration 5500 - Loss: 8.4753, Accuracy: 0.6234\n",
      "Iteration 6000 - Loss: 9.0143, Accuracy: 0.5689\n",
      "Iteration 6500 - Loss: 9.0540, Accuracy: 0.4967\n",
      "Fold: 3, Train Loss: 8.1867, Train Accuracy: 0.6054, Val Loss: 7.6830, Val Accuracy: 0.6266\n",
      "Iteration 0 - Loss: 15.8489, Accuracy: 0.3806\n",
      "Iteration 500 - Loss: 8.5183, Accuracy: 0.5849\n",
      "Iteration 1000 - Loss: 9.3844, Accuracy: 0.6259\n",
      "Iteration 1500 - Loss: 7.0738, Accuracy: 0.6440\n",
      "Iteration 2000 - Loss: 6.9537, Accuracy: 0.6871\n",
      "Iteration 2500 - Loss: 8.9287, Accuracy: 0.6030\n",
      "Iteration 3000 - Loss: 8.6367, Accuracy: 0.6262\n",
      "Iteration 3500 - Loss: 7.3628, Accuracy: 0.6098\n",
      "Iteration 4000 - Loss: 6.8149, Accuracy: 0.5734\n",
      "Iteration 4500 - Loss: 7.8470, Accuracy: 0.5403\n",
      "Iteration 5000 - Loss: 9.4251, Accuracy: 0.6037\n",
      "Iteration 5500 - Loss: 7.1329, Accuracy: 0.6515\n",
      "Iteration 6000 - Loss: 7.4530, Accuracy: 0.6198\n",
      "Iteration 6500 - Loss: 7.1090, Accuracy: 0.6039\n",
      "Fold: 4, Train Loss: 8.1415, Train Accuracy: 0.6076, Val Loss: 7.6755, Val Accuracy: 0.6313\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.86       0.69       0.77      \n",
      "1          0.54       0.88       0.67      \n",
      "2          0.45       0.02       0.03      \n",
      "3          0.63       0.85       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6200 | F1-Score: 0.5500\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/d49a43162f424ba792210fa71557f1a1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.2319, Accuracy: 0.4024\n",
      "Iteration 500 - Loss: 9.3729, Accuracy: 0.6076\n",
      "Iteration 1000 - Loss: 7.1711, Accuracy: 0.6067\n",
      "Iteration 1500 - Loss: 7.7935, Accuracy: 0.6222\n",
      "Iteration 2000 - Loss: 8.3169, Accuracy: 0.5722\n",
      "Iteration 2500 - Loss: 8.0148, Accuracy: 0.6886\n",
      "Iteration 3000 - Loss: 8.0824, Accuracy: 0.5849\n",
      "Iteration 3500 - Loss: 8.1802, Accuracy: 0.5223\n",
      "Iteration 4000 - Loss: 8.8163, Accuracy: 0.6384\n",
      "Iteration 4500 - Loss: 8.3606, Accuracy: 0.6550\n",
      "Iteration 5000 - Loss: 8.1689, Accuracy: 0.5532\n",
      "Iteration 5500 - Loss: 9.1189, Accuracy: 0.6611\n",
      "Iteration 6000 - Loss: 7.3679, Accuracy: 0.5910\n",
      "Iteration 6500 - Loss: 8.9668, Accuracy: 0.6175\n",
      "Fold: 0, Train Loss: 8.1863, Train Accuracy: 0.6054, Val Loss: 9.5588, Val Accuracy: 0.5385\n",
      "Iteration 0 - Loss: 16.0344, Accuracy: 0.4723\n",
      "Iteration 500 - Loss: 9.2086, Accuracy: 0.6646\n",
      "Iteration 1000 - Loss: 7.6223, Accuracy: 0.6606\n",
      "Iteration 1500 - Loss: 8.2770, Accuracy: 0.6222\n",
      "Iteration 2000 - Loss: 7.4342, Accuracy: 0.5012\n",
      "Iteration 2500 - Loss: 7.9712, Accuracy: 0.6590\n",
      "Iteration 3000 - Loss: 7.7938, Accuracy: 0.5514\n",
      "Iteration 3500 - Loss: 7.6430, Accuracy: 0.5955\n",
      "Iteration 4000 - Loss: 8.4258, Accuracy: 0.6501\n",
      "Iteration 4500 - Loss: 9.3283, Accuracy: 0.5521\n",
      "Iteration 5000 - Loss: 9.1253, Accuracy: 0.5518\n",
      "Iteration 5500 - Loss: 6.7036, Accuracy: 0.5525\n",
      "Iteration 6000 - Loss: 6.9146, Accuracy: 0.6477\n",
      "Iteration 6500 - Loss: 7.9122, Accuracy: 0.6445\n",
      "Fold: 1, Train Loss: 8.0855, Train Accuracy: 0.6104, Val Loss: 7.6187, Val Accuracy: 0.6341\n",
      "Iteration 0 - Loss: 15.8746, Accuracy: 0.4925\n",
      "Iteration 500 - Loss: 6.7237, Accuracy: 0.6506\n",
      "Iteration 1000 - Loss: 9.9990, Accuracy: 0.5980\n",
      "Iteration 1500 - Loss: 8.3380, Accuracy: 0.5997\n",
      "Iteration 2000 - Loss: 8.0636, Accuracy: 0.5626\n",
      "Iteration 2500 - Loss: 7.5820, Accuracy: 0.6881\n",
      "Iteration 3000 - Loss: 8.0198, Accuracy: 0.5741\n",
      "Iteration 3500 - Loss: 7.4667, Accuracy: 0.5260\n",
      "Iteration 4000 - Loss: 9.2164, Accuracy: 0.6386\n",
      "Iteration 4500 - Loss: 7.5117, Accuracy: 0.6011\n",
      "Iteration 5000 - Loss: 8.0754, Accuracy: 0.6536\n",
      "Iteration 5500 - Loss: 6.2195, Accuracy: 0.6266\n",
      "Iteration 6000 - Loss: 9.7825, Accuracy: 0.6041\n",
      "Iteration 6500 - Loss: 8.4189, Accuracy: 0.6067\n",
      "Fold: 2, Train Loss: 8.0422, Train Accuracy: 0.6125, Val Loss: 8.7059, Val Accuracy: 0.5779\n",
      "Iteration 0 - Loss: 16.1641, Accuracy: 0.4184\n",
      "Iteration 500 - Loss: 9.3178, Accuracy: 0.5239\n",
      "Iteration 1000 - Loss: 8.0907, Accuracy: 0.5138\n",
      "Iteration 1500 - Loss: 7.4117, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 7.9064, Accuracy: 0.6135\n",
      "Iteration 2500 - Loss: 7.7195, Accuracy: 0.5516\n",
      "Iteration 3000 - Loss: 7.9851, Accuracy: 0.6032\n",
      "Iteration 3500 - Loss: 6.7868, Accuracy: 0.6705\n",
      "Iteration 4000 - Loss: 9.5519, Accuracy: 0.6510\n",
      "Iteration 4500 - Loss: 7.1623, Accuracy: 0.6222\n",
      "Iteration 5000 - Loss: 10.2322, Accuracy: 0.6053\n",
      "Iteration 5500 - Loss: 6.7633, Accuracy: 0.6363\n",
      "Iteration 6000 - Loss: 8.5238, Accuracy: 0.6032\n",
      "Iteration 6500 - Loss: 8.6938, Accuracy: 0.6100\n",
      "Fold: 3, Train Loss: 8.1913, Train Accuracy: 0.6052, Val Loss: 8.7354, Val Accuracy: 0.5826\n",
      "Iteration 0 - Loss: 15.1186, Accuracy: 0.3940\n",
      "Iteration 500 - Loss: 8.8195, Accuracy: 0.5521\n",
      "Iteration 1000 - Loss: 8.5170, Accuracy: 0.5673\n",
      "Iteration 1500 - Loss: 8.5749, Accuracy: 0.6492\n",
      "Iteration 2000 - Loss: 7.5799, Accuracy: 0.6449\n",
      "Iteration 2500 - Loss: 8.3813, Accuracy: 0.5835\n",
      "Iteration 3000 - Loss: 8.1969, Accuracy: 0.5288\n",
      "Iteration 3500 - Loss: 9.0650, Accuracy: 0.5636\n",
      "Iteration 4000 - Loss: 8.3513, Accuracy: 0.5687\n",
      "Iteration 4500 - Loss: 7.1910, Accuracy: 0.6203\n",
      "Iteration 5000 - Loss: 7.0378, Accuracy: 0.6487\n",
      "Iteration 5500 - Loss: 8.4886, Accuracy: 0.6137\n",
      "Iteration 6000 - Loss: 7.9308, Accuracy: 0.6168\n",
      "Iteration 6500 - Loss: 9.1339, Accuracy: 0.6224\n",
      "Fold: 4, Train Loss: 8.1426, Train Accuracy: 0.6076, Val Loss: 7.3829, Val Accuracy: 0.6407\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.73       0.88       0.79      \n",
      "1          0.59       0.65       0.62      \n",
      "2          0.57       0.16       0.25      \n",
      "3          0.59       0.87       0.71      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6300 | F1-Score: 0.5900\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/b2c44a3eae3d4e35a3cb4f1cdfb414f0\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 16.2660, Accuracy: 0.4798\n",
      "Iteration 500 - Loss: 7.6973, Accuracy: 0.6288\n",
      "Iteration 1000 - Loss: 9.0740, Accuracy: 0.6264\n",
      "Iteration 1500 - Loss: 7.7975, Accuracy: 0.6703\n",
      "Iteration 2000 - Loss: 7.8078, Accuracy: 0.4791\n",
      "Iteration 2500 - Loss: 9.6599, Accuracy: 0.6543\n",
      "Iteration 3000 - Loss: 9.0136, Accuracy: 0.5385\n",
      "Iteration 3500 - Loss: 6.8551, Accuracy: 0.6302\n",
      "Iteration 4000 - Loss: 7.3149, Accuracy: 0.6468\n",
      "Iteration 4500 - Loss: 8.1667, Accuracy: 0.6370\n",
      "Iteration 5000 - Loss: 7.9208, Accuracy: 0.6768\n",
      "Iteration 5500 - Loss: 9.3598, Accuracy: 0.6119\n",
      "Iteration 6000 - Loss: 8.5586, Accuracy: 0.5697\n",
      "Iteration 6500 - Loss: 9.1104, Accuracy: 0.5136\n",
      "Fold: 0, Train Loss: 8.1852, Train Accuracy: 0.6055, Val Loss: 8.8387, Val Accuracy: 0.5722\n",
      "Iteration 0 - Loss: 14.1520, Accuracy: 0.3523\n",
      "Iteration 500 - Loss: 6.8160, Accuracy: 0.5818\n",
      "Iteration 1000 - Loss: 10.2829, Accuracy: 0.6269\n",
      "Iteration 1500 - Loss: 9.9085, Accuracy: 0.6536\n",
      "Iteration 2000 - Loss: 7.7863, Accuracy: 0.6717\n",
      "Iteration 2500 - Loss: 7.5406, Accuracy: 0.6435\n",
      "Iteration 3000 - Loss: 7.8316, Accuracy: 0.5910\n",
      "Iteration 3500 - Loss: 7.7320, Accuracy: 0.6053\n",
      "Iteration 4000 - Loss: 10.7342, Accuracy: 0.6548\n",
      "Iteration 4500 - Loss: 9.3527, Accuracy: 0.5957\n",
      "Iteration 5000 - Loss: 7.5673, Accuracy: 0.6494\n",
      "Iteration 5500 - Loss: 8.5534, Accuracy: 0.5938\n",
      "Iteration 6000 - Loss: 8.7640, Accuracy: 0.6642\n",
      "Iteration 6500 - Loss: 8.1359, Accuracy: 0.6492\n",
      "Fold: 1, Train Loss: 8.0927, Train Accuracy: 0.6100, Val Loss: 8.2725, Val Accuracy: 0.5994\n",
      "Iteration 0 - Loss: 15.8416, Accuracy: 0.4003\n",
      "Iteration 500 - Loss: 7.2023, Accuracy: 0.6044\n",
      "Iteration 1000 - Loss: 7.3180, Accuracy: 0.5192\n",
      "Iteration 1500 - Loss: 8.2192, Accuracy: 0.5762\n",
      "Iteration 2000 - Loss: 8.0490, Accuracy: 0.6939\n",
      "Iteration 2500 - Loss: 9.3209, Accuracy: 0.6534\n",
      "Iteration 3000 - Loss: 9.9483, Accuracy: 0.6421\n",
      "Iteration 3500 - Loss: 7.5398, Accuracy: 0.5765\n",
      "Iteration 4000 - Loss: 8.3166, Accuracy: 0.5774\n",
      "Iteration 4500 - Loss: 7.4135, Accuracy: 0.6255\n",
      "Iteration 5000 - Loss: 8.3323, Accuracy: 0.6053\n",
      "Iteration 5500 - Loss: 7.3470, Accuracy: 0.5877\n",
      "Iteration 6000 - Loss: 9.2339, Accuracy: 0.5915\n",
      "Iteration 6500 - Loss: 9.2368, Accuracy: 0.6069\n",
      "Fold: 2, Train Loss: 8.0502, Train Accuracy: 0.6120, Val Loss: 7.7033, Val Accuracy: 0.6304\n",
      "Iteration 0 - Loss: 16.3871, Accuracy: 0.4751\n",
      "Iteration 500 - Loss: 7.2462, Accuracy: 0.6632\n",
      "Iteration 1000 - Loss: 9.5495, Accuracy: 0.6102\n",
      "Iteration 1500 - Loss: 8.0410, Accuracy: 0.6191\n",
      "Iteration 2000 - Loss: 7.2613, Accuracy: 0.5331\n",
      "Iteration 2500 - Loss: 7.8311, Accuracy: 0.5490\n",
      "Iteration 3000 - Loss: 6.9113, Accuracy: 0.6644\n",
      "Iteration 3500 - Loss: 8.8951, Accuracy: 0.5823\n",
      "Iteration 4000 - Loss: 9.1935, Accuracy: 0.5912\n",
      "Iteration 4500 - Loss: 7.6119, Accuracy: 0.5532\n",
      "Iteration 5000 - Loss: 7.7661, Accuracy: 0.5706\n",
      "Iteration 5500 - Loss: 7.5613, Accuracy: 0.5736\n",
      "Iteration 6000 - Loss: 8.3234, Accuracy: 0.5478\n",
      "Iteration 6500 - Loss: 9.0433, Accuracy: 0.6407\n",
      "Fold: 3, Train Loss: 8.1891, Train Accuracy: 0.6053, Val Loss: 7.0202, Val Accuracy: 0.6576\n",
      "Iteration 0 - Loss: 16.4192, Accuracy: 0.4156\n",
      "Iteration 500 - Loss: 8.8208, Accuracy: 0.6707\n",
      "Iteration 1000 - Loss: 8.6604, Accuracy: 0.5145\n",
      "Iteration 1500 - Loss: 8.9144, Accuracy: 0.5331\n",
      "Iteration 2000 - Loss: 9.2425, Accuracy: 0.5748\n",
      "Iteration 2500 - Loss: 8.9829, Accuracy: 0.6400\n",
      "Iteration 3000 - Loss: 8.2919, Accuracy: 0.6112\n",
      "Iteration 3500 - Loss: 7.5034, Accuracy: 0.6032\n",
      "Iteration 4000 - Loss: 8.1687, Accuracy: 0.5793\n",
      "Iteration 4500 - Loss: 10.2007, Accuracy: 0.5626\n",
      "Iteration 5000 - Loss: 7.3075, Accuracy: 0.5525\n",
      "Iteration 5500 - Loss: 7.2014, Accuracy: 0.6553\n",
      "Iteration 6000 - Loss: 8.1275, Accuracy: 0.6318\n",
      "Iteration 6500 - Loss: 8.0457, Accuracy: 0.5174\n",
      "Fold: 4, Train Loss: 8.1404, Train Accuracy: 0.6077, Val Loss: 9.0568, Val Accuracy: 0.5647\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.53       0.99       0.69      \n",
      "1          1.00       0.00       0.01      \n",
      "2          0.47       0.75       0.58      \n",
      "3          0.74       0.66       0.70      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.5400 | F1-Score: 0.4300\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/a1786fc90b8f4f7c84fa143221fe4158\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 14.4272, Accuracy: 0.4512\n",
      "Iteration 500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Fold: 0, Train Loss: 8.1048, Train Accuracy: 0.6967, Val Loss: 7.9758, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 16.0542, Accuracy: 0.4613\n",
      "Iteration 500 - Loss: 7.9027, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9907, Train Accuracy: 0.7035, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 16.4328, Accuracy: 0.4130\n",
      "Iteration 500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Fold: 2, Train Loss: 7.9567, Train Accuracy: 0.7124, Val Loss: 8.3286, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 16.6628, Accuracy: 0.4350\n",
      "Iteration 500 - Loss: 8.1733, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1728, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1005, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 14.7016, Accuracy: 0.4034\n",
      "Iteration 500 - Loss: 7.9620, Accuracy: 0.7104\n",
      "Iteration 1000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 1500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Fold: 4, Train Loss: 8.0305, Train Accuracy: 0.7082, Val Loss: 8.0708, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.76       0.80      \n",
      "1          0.60       0.77       0.67      \n",
      "2          0.54       0.38       0.44      \n",
      "3          0.74       0.73       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/72a1a9311c8b43d18ad05f5495f91e81\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 16.0640, Accuracy: 0.4606\n",
      "Iteration 500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Fold: 0, Train Loss: 8.1046, Train Accuracy: 0.6966, Val Loss: 7.8284, Val Accuracy: 0.7139\n",
      "Iteration 0 - Loss: 15.4179, Accuracy: 0.4998\n",
      "Iteration 500 - Loss: 8.0753, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.0746, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 8.0745, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Fold: 1, Train Loss: 7.9903, Train Accuracy: 0.7036, Val Loss: 8.2928, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 16.3993, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 7.8989, Accuracy: 0.7160\n",
      "Iteration 1000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 1500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 2000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 2500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 3000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 3500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 4000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 4500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 5000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 5500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 6000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 6500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Fold: 2, Train Loss: 7.9566, Train Accuracy: 0.7125, Val Loss: 8.1494, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 15.2067, Accuracy: 0.4013\n",
      "Iteration 500 - Loss: 8.1735, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1729, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 16.3713, Accuracy: 0.4524\n",
      "Iteration 500 - Loss: 8.0945, Accuracy: 0.7064\n",
      "Iteration 1000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Fold: 4, Train Loss: 8.0297, Train Accuracy: 0.7083, Val Loss: 8.1073, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.83       0.81      \n",
      "1          0.71       0.63       0.67      \n",
      "2          0.54       0.73       0.62      \n",
      "3          0.86       0.58       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/1d7dc3517be246b1ba404c4fa7ae335e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 14.8631, Accuracy: 0.4285\n",
      "Iteration 500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Fold: 0, Train Loss: 8.1049, Train Accuracy: 0.6966, Val Loss: 7.8284, Val Accuracy: 0.7139\n",
      "Iteration 0 - Loss: 14.1556, Accuracy: 0.4156\n",
      "Iteration 500 - Loss: 7.9028, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9912, Train Accuracy: 0.7035, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 13.9354, Accuracy: 0.3766\n",
      "Iteration 500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Fold: 2, Train Loss: 7.9579, Train Accuracy: 0.7124, Val Loss: 8.3286, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.0507, Accuracy: 0.4573\n",
      "Iteration 500 - Loss: 8.6677, Accuracy: 0.6351\n",
      "Iteration 1000 - Loss: 7.8189, Accuracy: 0.6163\n",
      "Iteration 1500 - Loss: 8.0776, Accuracy: 0.6939\n",
      "Iteration 2000 - Loss: 8.6667, Accuracy: 0.6351\n",
      "Iteration 2500 - Loss: 7.8186, Accuracy: 0.6163\n",
      "Iteration 3000 - Loss: 8.0773, Accuracy: 0.6939\n",
      "Iteration 3500 - Loss: 8.6666, Accuracy: 0.6351\n",
      "Iteration 4000 - Loss: 7.8185, Accuracy: 0.6163\n",
      "Iteration 4500 - Loss: 8.0772, Accuracy: 0.6939\n",
      "Iteration 5000 - Loss: 8.6665, Accuracy: 0.6351\n",
      "Iteration 5500 - Loss: 7.8184, Accuracy: 0.6163\n",
      "Iteration 6000 - Loss: 8.0772, Accuracy: 0.6939\n",
      "Iteration 6500 - Loss: 8.6665, Accuracy: 0.6351\n",
      "Fold: 3, Train Loss: 8.1912, Train Accuracy: 0.6484, Val Loss: 7.6701, Val Accuracy: 0.7111\n",
      "Iteration 0 - Loss: 14.5835, Accuracy: 0.5056\n",
      "Iteration 500 - Loss: 8.6032, Accuracy: 0.6377\n",
      "Iteration 1000 - Loss: 7.7634, Accuracy: 0.6222\n",
      "Iteration 1500 - Loss: 7.9815, Accuracy: 0.7000\n",
      "Iteration 2000 - Loss: 8.6028, Accuracy: 0.6379\n",
      "Iteration 2500 - Loss: 7.7633, Accuracy: 0.6222\n",
      "Iteration 3000 - Loss: 7.9815, Accuracy: 0.7000\n",
      "Iteration 3500 - Loss: 8.6028, Accuracy: 0.6379\n",
      "Iteration 4000 - Loss: 7.7633, Accuracy: 0.6222\n",
      "Iteration 4500 - Loss: 7.9815, Accuracy: 0.7000\n",
      "Iteration 5000 - Loss: 8.6028, Accuracy: 0.6379\n",
      "Iteration 5500 - Loss: 7.7633, Accuracy: 0.6222\n",
      "Iteration 6000 - Loss: 7.9815, Accuracy: 0.7000\n",
      "Iteration 6500 - Loss: 8.6028, Accuracy: 0.6379\n",
      "Fold: 4, Train Loss: 8.1182, Train Accuracy: 0.6535, Val Loss: 7.8771, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.81       0.81      \n",
      "1          0.62       0.71       0.66      \n",
      "2          0.55       0.39       0.46      \n",
      "3          0.68       0.76       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/84182eb24e334a68826bc2a69f8c01b2\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.6308, Accuracy: 0.3213\n",
      "Iteration 500 - Loss: 8.1549, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0923, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7141\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1226, Train Accuracy: 0.7135, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 14.6721, Accuracy: 0.4371\n",
      "Iteration 500 - Loss: 8.0435, Accuracy: 0.7212\n",
      "Iteration 1000 - Loss: 7.9796, Accuracy: 0.7207\n",
      "Iteration 1500 - Loss: 7.9720, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9707, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9703, Accuracy: 0.7223\n",
      "Iteration 3000 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9698, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0087, Train Accuracy: 0.7215, Val Loss: 8.2419, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 14.8303, Accuracy: 0.3415\n",
      "Iteration 500 - Loss: 8.0183, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9534, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9842, Train Accuracy: 0.7206, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.6082, Accuracy: 0.2953\n",
      "Iteration 500 - Loss: 8.1531, Accuracy: 0.7167\n",
      "Iteration 1000 - Loss: 8.0896, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0820, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0805, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0799, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0796, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7167\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0789, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1186, Train Accuracy: 0.7162, Val Loss: 7.9512, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 14.5021, Accuracy: 0.3619\n",
      "Iteration 500 - Loss: 8.0934, Accuracy: 0.7151\n",
      "Iteration 1000 - Loss: 8.0270, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0190, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0178, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7146\n",
      "Iteration 3000 - Loss: 8.0173, Accuracy: 0.7146\n",
      "Iteration 3500 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0579, Train Accuracy: 0.7139, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/8d3feb09e74d46c7be02ec744e27430f\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 16.1292, Accuracy: 0.2756\n",
      "Iteration 500 - Loss: 8.1542, Accuracy: 0.7141\n",
      "Iteration 1000 - Loss: 8.0921, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0852, Accuracy: 0.7139\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1221, Train Accuracy: 0.7134, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 13.8691, Accuracy: 0.3961\n",
      "Iteration 500 - Loss: 8.0431, Accuracy: 0.7205\n",
      "Iteration 1000 - Loss: 7.9793, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9716, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9704, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0087, Train Accuracy: 0.7214, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 14.1718, Accuracy: 0.4308\n",
      "Iteration 500 - Loss: 8.0196, Accuracy: 0.7212\n",
      "Iteration 1000 - Loss: 7.9536, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9844, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 16.4374, Accuracy: 0.2263\n",
      "Iteration 500 - Loss: 8.1530, Accuracy: 0.7165\n",
      "Iteration 1000 - Loss: 8.0898, Accuracy: 0.7169\n",
      "Iteration 1500 - Loss: 8.0822, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0807, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0801, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0797, Accuracy: 0.7169\n",
      "Iteration 3500 - Loss: 8.0794, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0792, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0789, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1187, Train Accuracy: 0.7162, Val Loss: 7.9510, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 15.4388, Accuracy: 0.3180\n",
      "Iteration 500 - Loss: 8.0893, Accuracy: 0.7155\n",
      "Iteration 1000 - Loss: 8.0263, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0188, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0176, Accuracy: 0.7144\n",
      "Iteration 2500 - Loss: 8.0173, Accuracy: 0.7141\n",
      "Iteration 3000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 3500 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 4000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 4500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 5500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0552, Train Accuracy: 0.7139, Val Loss: 8.0755, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/0fe04e84b926446992a1513eabc3fe20\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | xavier | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.8387, Accuracy: 0.3014\n",
      "Iteration 500 - Loss: 8.1559, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0925, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7139\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1233, Train Accuracy: 0.7134, Val Loss: 7.8730, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 16.0647, Accuracy: 0.1890\n",
      "Iteration 500 - Loss: 8.0422, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9788, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9713, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 2500 - Loss: 7.9698, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9692, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9692, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0085, Train Accuracy: 0.7214, Val Loss: 8.2421, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 16.2991, Accuracy: 0.1958\n",
      "Iteration 500 - Loss: 8.0178, Accuracy: 0.7202\n",
      "Iteration 1000 - Loss: 7.9533, Accuracy: 0.7214\n",
      "Iteration 1500 - Loss: 7.9458, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9835, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 16.2767, Accuracy: 0.2772\n",
      "Iteration 500 - Loss: 8.1549, Accuracy: 0.7169\n",
      "Iteration 1000 - Loss: 8.0902, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0823, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0807, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0801, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0797, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0794, Accuracy: 0.7167\n",
      "Iteration 4000 - Loss: 8.0792, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0789, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1201, Train Accuracy: 0.7161, Val Loss: 7.9509, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 14.9576, Accuracy: 0.3419\n",
      "Iteration 500 - Loss: 8.0912, Accuracy: 0.7151\n",
      "Iteration 1000 - Loss: 8.0269, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0191, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0179, Accuracy: 0.7144\n",
      "Iteration 2500 - Loss: 8.0175, Accuracy: 0.7146\n",
      "Iteration 3000 - Loss: 8.0174, Accuracy: 0.7146\n",
      "Iteration 3500 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0567, Train Accuracy: 0.7139, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/3491c0dec62c4192ad571ccd1f64d32f\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 13.9103, Accuracy: 0.3771\n",
      "Iteration 500 - Loss: 7.4736, Accuracy: 0.5685\n",
      "Iteration 1000 - Loss: 8.2167, Accuracy: 0.6691\n",
      "Iteration 1500 - Loss: 7.7265, Accuracy: 0.5488\n",
      "Iteration 2000 - Loss: 8.5029, Accuracy: 0.6039\n",
      "Iteration 2500 - Loss: 8.7129, Accuracy: 0.6402\n",
      "Iteration 3000 - Loss: 7.4561, Accuracy: 0.6076\n",
      "Iteration 3500 - Loss: 8.0293, Accuracy: 0.5755\n",
      "Iteration 4000 - Loss: 7.9094, Accuracy: 0.5572\n",
      "Iteration 4500 - Loss: 7.8586, Accuracy: 0.6649\n",
      "Iteration 5000 - Loss: 11.0692, Accuracy: 0.6306\n",
      "Iteration 5500 - Loss: 7.6518, Accuracy: 0.6635\n",
      "Iteration 6000 - Loss: 6.6052, Accuracy: 0.5901\n",
      "Iteration 6500 - Loss: 9.8455, Accuracy: 0.6538\n",
      "Fold: 0, Train Loss: 8.1805, Train Accuracy: 0.6057, Val Loss: 10.4032, Val Accuracy: 0.4972\n",
      "Iteration 0 - Loss: 16.1177, Accuracy: 0.4876\n",
      "Iteration 500 - Loss: 8.4744, Accuracy: 0.6470\n",
      "Iteration 1000 - Loss: 9.1866, Accuracy: 0.5943\n",
      "Iteration 1500 - Loss: 6.7139, Accuracy: 0.6625\n",
      "Iteration 2000 - Loss: 6.9765, Accuracy: 0.6100\n",
      "Iteration 2500 - Loss: 8.6229, Accuracy: 0.6576\n",
      "Iteration 3000 - Loss: 8.4800, Accuracy: 0.6417\n",
      "Iteration 3500 - Loss: 10.0404, Accuracy: 0.6377\n",
      "Iteration 4000 - Loss: 7.8871, Accuracy: 0.6130\n",
      "Iteration 4500 - Loss: 8.0345, Accuracy: 0.6023\n",
      "Iteration 5000 - Loss: 8.2747, Accuracy: 0.6782\n",
      "Iteration 5500 - Loss: 7.0253, Accuracy: 0.6034\n",
      "Iteration 6000 - Loss: 7.1663, Accuracy: 0.6295\n",
      "Iteration 6500 - Loss: 8.3383, Accuracy: 0.6100\n",
      "Fold: 1, Train Loss: 8.0931, Train Accuracy: 0.6100, Val Loss: 8.4464, Val Accuracy: 0.5938\n",
      "Iteration 0 - Loss: 17.6064, Accuracy: 0.3931\n",
      "Iteration 500 - Loss: 7.4004, Accuracy: 0.6098\n",
      "Iteration 1000 - Loss: 7.5219, Accuracy: 0.6684\n",
      "Iteration 1500 - Loss: 6.3351, Accuracy: 0.6597\n",
      "Iteration 2000 - Loss: 7.5747, Accuracy: 0.5877\n",
      "Iteration 2500 - Loss: 7.7093, Accuracy: 0.6642\n",
      "Iteration 3000 - Loss: 7.6315, Accuracy: 0.6086\n",
      "Iteration 3500 - Loss: 8.2956, Accuracy: 0.5701\n",
      "Iteration 4000 - Loss: 7.7169, Accuracy: 0.6283\n",
      "Iteration 4500 - Loss: 9.8199, Accuracy: 0.5368\n",
      "Iteration 5000 - Loss: 7.1905, Accuracy: 0.6189\n",
      "Iteration 5500 - Loss: 7.6421, Accuracy: 0.5643\n",
      "Iteration 6000 - Loss: 7.4844, Accuracy: 0.6224\n",
      "Iteration 6500 - Loss: 7.4597, Accuracy: 0.6313\n",
      "Fold: 2, Train Loss: 8.0477, Train Accuracy: 0.6122, Val Loss: 7.0990, Val Accuracy: 0.6576\n",
      "Iteration 0 - Loss: 17.0515, Accuracy: 0.4489\n",
      "Iteration 500 - Loss: 9.3224, Accuracy: 0.6388\n",
      "Iteration 1000 - Loss: 9.8948, Accuracy: 0.5769\n",
      "Iteration 1500 - Loss: 9.1401, Accuracy: 0.6510\n",
      "Iteration 2000 - Loss: 9.4320, Accuracy: 0.5816\n",
      "Iteration 2500 - Loss: 8.2975, Accuracy: 0.5650\n",
      "Iteration 3000 - Loss: 7.2886, Accuracy: 0.6037\n",
      "Iteration 3500 - Loss: 10.0403, Accuracy: 0.6234\n",
      "Iteration 4000 - Loss: 9.2611, Accuracy: 0.6067\n",
      "Iteration 4500 - Loss: 7.2029, Accuracy: 0.5772\n",
      "Iteration 5000 - Loss: 7.1478, Accuracy: 0.6714\n",
      "Iteration 5500 - Loss: 8.3385, Accuracy: 0.5849\n",
      "Iteration 6000 - Loss: 8.1084, Accuracy: 0.6135\n",
      "Iteration 6500 - Loss: 10.6418, Accuracy: 0.6447\n",
      "Fold: 3, Train Loss: 8.1891, Train Accuracy: 0.6053, Val Loss: 8.2974, Val Accuracy: 0.6032\n",
      "Iteration 0 - Loss: 15.8704, Accuracy: 0.3771\n",
      "Iteration 500 - Loss: 6.4985, Accuracy: 0.5610\n",
      "Iteration 1000 - Loss: 6.9006, Accuracy: 0.6365\n",
      "Iteration 1500 - Loss: 7.6791, Accuracy: 0.5779\n",
      "Iteration 2000 - Loss: 8.1286, Accuracy: 0.6119\n",
      "Iteration 2500 - Loss: 7.6045, Accuracy: 0.5568\n",
      "Iteration 3000 - Loss: 8.9111, Accuracy: 0.5432\n",
      "Iteration 3500 - Loss: 10.2830, Accuracy: 0.6140\n",
      "Iteration 4000 - Loss: 6.9396, Accuracy: 0.6822\n",
      "Iteration 4500 - Loss: 8.9429, Accuracy: 0.6468\n",
      "Iteration 5000 - Loss: 8.8015, Accuracy: 0.6248\n",
      "Iteration 5500 - Loss: 8.7632, Accuracy: 0.6241\n",
      "Iteration 6000 - Loss: 8.1084, Accuracy: 0.6212\n",
      "Iteration 6500 - Loss: 7.6769, Accuracy: 0.5387\n",
      "Fold: 4, Train Loss: 8.1466, Train Accuracy: 0.6074, Val Loss: 7.4668, Val Accuracy: 0.6388\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.69       0.92       0.79      \n",
      "1          0.61       0.49       0.54      \n",
      "2          0.47       0.36       0.41      \n",
      "3          0.62       0.78       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6200 | F1-Score: 0.6000\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/e3601584012c4907ab17177945755b76\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 17.8297, Accuracy: 0.4167\n",
      "Iteration 500 - Loss: 7.1121, Accuracy: 0.5969\n",
      "Iteration 1000 - Loss: 8.5647, Accuracy: 0.6327\n",
      "Iteration 1500 - Loss: 7.1752, Accuracy: 0.6904\n",
      "Iteration 2000 - Loss: 8.8162, Accuracy: 0.5331\n",
      "Iteration 2500 - Loss: 8.6089, Accuracy: 0.6292\n",
      "Iteration 3000 - Loss: 6.6982, Accuracy: 0.5896\n",
      "Iteration 3500 - Loss: 6.5730, Accuracy: 0.5589\n",
      "Iteration 4000 - Loss: 6.9483, Accuracy: 0.6384\n",
      "Iteration 4500 - Loss: 8.3216, Accuracy: 0.5617\n",
      "Iteration 5000 - Loss: 7.8892, Accuracy: 0.5497\n",
      "Iteration 5500 - Loss: 9.0709, Accuracy: 0.5990\n",
      "Iteration 6000 - Loss: 8.4505, Accuracy: 0.5987\n",
      "Iteration 6500 - Loss: 7.5757, Accuracy: 0.6323\n",
      "Fold: 0, Train Loss: 8.1826, Train Accuracy: 0.6056, Val Loss: 8.9129, Val Accuracy: 0.5675\n",
      "Iteration 0 - Loss: 17.9309, Accuracy: 0.4702\n",
      "Iteration 500 - Loss: 6.3746, Accuracy: 0.6203\n",
      "Iteration 1000 - Loss: 9.2545, Accuracy: 0.6222\n",
      "Iteration 1500 - Loss: 7.8443, Accuracy: 0.6128\n",
      "Iteration 2000 - Loss: 8.1341, Accuracy: 0.5786\n",
      "Iteration 2500 - Loss: 7.1857, Accuracy: 0.5966\n",
      "Iteration 3000 - Loss: 7.5591, Accuracy: 0.6367\n",
      "Iteration 3500 - Loss: 7.5337, Accuracy: 0.6689\n",
      "Iteration 4000 - Loss: 8.8841, Accuracy: 0.6217\n",
      "Iteration 4500 - Loss: 8.7177, Accuracy: 0.6107\n",
      "Iteration 5000 - Loss: 7.3586, Accuracy: 0.6660\n",
      "Iteration 5500 - Loss: 7.3203, Accuracy: 0.6252\n",
      "Iteration 6000 - Loss: 7.7166, Accuracy: 0.6480\n",
      "Iteration 6500 - Loss: 7.9214, Accuracy: 0.6144\n",
      "Fold: 1, Train Loss: 8.0918, Train Accuracy: 0.6101, Val Loss: 7.3515, Val Accuracy: 0.6463\n",
      "Iteration 0 - Loss: 18.3109, Accuracy: 0.4599\n",
      "Iteration 500 - Loss: 7.9722, Accuracy: 0.5865\n",
      "Iteration 1000 - Loss: 7.6627, Accuracy: 0.5636\n",
      "Iteration 1500 - Loss: 6.9616, Accuracy: 0.6541\n",
      "Iteration 2000 - Loss: 7.2708, Accuracy: 0.6564\n",
      "Iteration 2500 - Loss: 7.0990, Accuracy: 0.5971\n",
      "Iteration 3000 - Loss: 7.8329, Accuracy: 0.5673\n",
      "Iteration 3500 - Loss: 7.5651, Accuracy: 0.6257\n",
      "Iteration 4000 - Loss: 8.9667, Accuracy: 0.5983\n",
      "Iteration 4500 - Loss: 7.1994, Accuracy: 0.5919\n",
      "Iteration 5000 - Loss: 6.9844, Accuracy: 0.6724\n",
      "Iteration 5500 - Loss: 7.5888, Accuracy: 0.5708\n",
      "Iteration 6000 - Loss: 9.2415, Accuracy: 0.6576\n",
      "Iteration 6500 - Loss: 9.2006, Accuracy: 0.6541\n",
      "Fold: 2, Train Loss: 8.0467, Train Accuracy: 0.6122, Val Loss: 7.3850, Val Accuracy: 0.6426\n",
      "Iteration 0 - Loss: 17.5729, Accuracy: 0.3745\n",
      "Iteration 500 - Loss: 6.3376, Accuracy: 0.5565\n",
      "Iteration 1000 - Loss: 9.1795, Accuracy: 0.6316\n",
      "Iteration 1500 - Loss: 9.5691, Accuracy: 0.6496\n",
      "Iteration 2000 - Loss: 8.0981, Accuracy: 0.6208\n",
      "Iteration 2500 - Loss: 9.4354, Accuracy: 0.5905\n",
      "Iteration 3000 - Loss: 9.8877, Accuracy: 0.6159\n",
      "Iteration 3500 - Loss: 7.5716, Accuracy: 0.6325\n",
      "Iteration 4000 - Loss: 6.5393, Accuracy: 0.6564\n",
      "Iteration 4500 - Loss: 8.0037, Accuracy: 0.6407\n",
      "Iteration 5000 - Loss: 7.7637, Accuracy: 0.5385\n",
      "Iteration 5500 - Loss: 9.8580, Accuracy: 0.6330\n",
      "Iteration 6000 - Loss: 6.5565, Accuracy: 0.6937\n",
      "Iteration 6500 - Loss: 9.5111, Accuracy: 0.6696\n",
      "Fold: 3, Train Loss: 8.1932, Train Accuracy: 0.6051, Val Loss: 6.7520, Val Accuracy: 0.6717\n",
      "Iteration 0 - Loss: 15.2398, Accuracy: 0.3959\n",
      "Iteration 500 - Loss: 9.6608, Accuracy: 0.5443\n",
      "Iteration 1000 - Loss: 9.3366, Accuracy: 0.6632\n",
      "Iteration 1500 - Loss: 9.5632, Accuracy: 0.6470\n",
      "Iteration 2000 - Loss: 8.5304, Accuracy: 0.6693\n",
      "Iteration 2500 - Loss: 7.8880, Accuracy: 0.5335\n",
      "Iteration 3000 - Loss: 8.4421, Accuracy: 0.5729\n",
      "Iteration 3500 - Loss: 7.9868, Accuracy: 0.6916\n",
      "Iteration 4000 - Loss: 8.0623, Accuracy: 0.6069\n",
      "Iteration 4500 - Loss: 9.3113, Accuracy: 0.6262\n",
      "Iteration 5000 - Loss: 8.0493, Accuracy: 0.5830\n",
      "Iteration 5500 - Loss: 9.5347, Accuracy: 0.5211\n",
      "Iteration 6000 - Loss: 7.9459, Accuracy: 0.6212\n",
      "Iteration 6500 - Loss: 7.8495, Accuracy: 0.6152\n",
      "Fold: 4, Train Loss: 8.1454, Train Accuracy: 0.6074, Val Loss: 7.6823, Val Accuracy: 0.6341\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.97       0.49       0.65      \n",
      "1          0.54       0.84       0.66      \n",
      "2          0.50       0.47       0.49      \n",
      "3          0.84       0.54       0.66      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6200 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/2db2bc3a6b08497f9b51d17103a38185\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 16.7325, Accuracy: 0.3898\n",
      "Iteration 500 - Loss: 7.5324, Accuracy: 0.5399\n",
      "Iteration 1000 - Loss: 8.8781, Accuracy: 0.5856\n",
      "Iteration 1500 - Loss: 9.0810, Accuracy: 0.5610\n",
      "Iteration 2000 - Loss: 8.2480, Accuracy: 0.5288\n",
      "Iteration 2500 - Loss: 7.0746, Accuracy: 0.5558\n",
      "Iteration 3000 - Loss: 7.5875, Accuracy: 0.5176\n",
      "Iteration 3500 - Loss: 8.1608, Accuracy: 0.6524\n",
      "Iteration 4000 - Loss: 7.1010, Accuracy: 0.5804\n",
      "Iteration 4500 - Loss: 7.8710, Accuracy: 0.5708\n",
      "Iteration 5000 - Loss: 8.1867, Accuracy: 0.6517\n",
      "Iteration 5500 - Loss: 10.3348, Accuracy: 0.6227\n",
      "Iteration 6000 - Loss: 9.6265, Accuracy: 0.5675\n",
      "Iteration 6500 - Loss: 8.6374, Accuracy: 0.6435\n",
      "Fold: 0, Train Loss: 8.1854, Train Accuracy: 0.6055, Val Loss: 6.7201, Val Accuracy: 0.6829\n",
      "Iteration 0 - Loss: 14.4606, Accuracy: 0.4773\n",
      "Iteration 500 - Loss: 9.4960, Accuracy: 0.6395\n",
      "Iteration 1000 - Loss: 7.5742, Accuracy: 0.5607\n",
      "Iteration 1500 - Loss: 8.6168, Accuracy: 0.6311\n",
      "Iteration 2000 - Loss: 6.6616, Accuracy: 0.6283\n",
      "Iteration 2500 - Loss: 9.7804, Accuracy: 0.6449\n",
      "Iteration 3000 - Loss: 6.6152, Accuracy: 0.6351\n",
      "Iteration 3500 - Loss: 7.8365, Accuracy: 0.6367\n",
      "Iteration 4000 - Loss: 7.7380, Accuracy: 0.6710\n",
      "Iteration 4500 - Loss: 6.8369, Accuracy: 0.5368\n",
      "Iteration 5000 - Loss: 7.1604, Accuracy: 0.6372\n",
      "Iteration 5500 - Loss: 7.7035, Accuracy: 0.6018\n",
      "Iteration 6000 - Loss: 9.0514, Accuracy: 0.6020\n",
      "Iteration 6500 - Loss: 8.2486, Accuracy: 0.5962\n",
      "Fold: 1, Train Loss: 8.0976, Train Accuracy: 0.6097, Val Loss: 9.1503, Val Accuracy: 0.5544\n",
      "Iteration 0 - Loss: 17.6412, Accuracy: 0.4386\n",
      "Iteration 500 - Loss: 7.9819, Accuracy: 0.6154\n",
      "Iteration 1000 - Loss: 9.0543, Accuracy: 0.6470\n",
      "Iteration 1500 - Loss: 6.8895, Accuracy: 0.5879\n",
      "Iteration 2000 - Loss: 9.7679, Accuracy: 0.6473\n",
      "Iteration 2500 - Loss: 7.5577, Accuracy: 0.6595\n",
      "Iteration 3000 - Loss: 10.0853, Accuracy: 0.6337\n",
      "Iteration 3500 - Loss: 7.5596, Accuracy: 0.5687\n",
      "Iteration 4000 - Loss: 10.3229, Accuracy: 0.6815\n",
      "Iteration 4500 - Loss: 9.4223, Accuracy: 0.5847\n",
      "Iteration 5000 - Loss: 6.5493, Accuracy: 0.6112\n",
      "Iteration 5500 - Loss: 7.1956, Accuracy: 0.6975\n",
      "Iteration 6000 - Loss: 8.6083, Accuracy: 0.6316\n",
      "Iteration 6500 - Loss: 6.9325, Accuracy: 0.6283\n",
      "Fold: 2, Train Loss: 8.0461, Train Accuracy: 0.6123, Val Loss: 9.4192, Val Accuracy: 0.5478\n",
      "Iteration 0 - Loss: 15.6755, Accuracy: 0.4003\n",
      "Iteration 500 - Loss: 7.5742, Accuracy: 0.5671\n",
      "Iteration 1000 - Loss: 6.7521, Accuracy: 0.6555\n",
      "Iteration 1500 - Loss: 8.2809, Accuracy: 0.6116\n",
      "Iteration 2000 - Loss: 9.1908, Accuracy: 0.6008\n",
      "Iteration 2500 - Loss: 7.9285, Accuracy: 0.6121\n",
      "Iteration 3000 - Loss: 8.0219, Accuracy: 0.5985\n",
      "Iteration 3500 - Loss: 8.3533, Accuracy: 0.5802\n",
      "Iteration 4000 - Loss: 9.6798, Accuracy: 0.5443\n",
      "Iteration 4500 - Loss: 7.9580, Accuracy: 0.6609\n",
      "Iteration 5000 - Loss: 10.8838, Accuracy: 0.5823\n",
      "Iteration 5500 - Loss: 7.0699, Accuracy: 0.6198\n",
      "Iteration 6000 - Loss: 8.7663, Accuracy: 0.6076\n",
      "Iteration 6500 - Loss: 9.5018, Accuracy: 0.6417\n",
      "Fold: 3, Train Loss: 8.1928, Train Accuracy: 0.6051, Val Loss: 9.3999, Val Accuracy: 0.5441\n",
      "Iteration 0 - Loss: 17.2361, Accuracy: 0.3572\n",
      "Iteration 500 - Loss: 8.3356, Accuracy: 0.5631\n",
      "Iteration 1000 - Loss: 8.7232, Accuracy: 0.6492\n",
      "Iteration 1500 - Loss: 8.7252, Accuracy: 0.6508\n",
      "Iteration 2000 - Loss: 8.2281, Accuracy: 0.5314\n",
      "Iteration 2500 - Loss: 7.7390, Accuracy: 0.5530\n",
      "Iteration 3000 - Loss: 7.5228, Accuracy: 0.4838\n",
      "Iteration 3500 - Loss: 9.5838, Accuracy: 0.6102\n",
      "Iteration 4000 - Loss: 9.3152, Accuracy: 0.5183\n",
      "Iteration 4500 - Loss: 9.2146, Accuracy: 0.6309\n",
      "Iteration 5000 - Loss: 7.0885, Accuracy: 0.6198\n",
      "Iteration 5500 - Loss: 7.5566, Accuracy: 0.5584\n",
      "Iteration 6000 - Loss: 8.2010, Accuracy: 0.6166\n",
      "Iteration 6500 - Loss: 10.9157, Accuracy: 0.6487\n",
      "Fold: 4, Train Loss: 8.1368, Train Accuracy: 0.6079, Val Loss: 7.4966, Val Accuracy: 0.6407\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.63       0.95       0.76      \n",
      "1          0.66       0.44       0.53      \n",
      "2          0.54       0.42       0.47      \n",
      "3          0.63       0.80       0.70      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6200 | F1-Score: 0.6000\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/b26587ff497c4540b639e6b131e91f58\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 14.9316, Accuracy: 0.3433\n",
      "Iteration 500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Fold: 0, Train Loss: 8.1035, Train Accuracy: 0.6966, Val Loss: 7.8284, Val Accuracy: 0.7139\n",
      "Iteration 0 - Loss: 16.4114, Accuracy: 0.3335\n",
      "Iteration 500 - Loss: 7.9030, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9889, Train Accuracy: 0.7036, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 11.8786, Accuracy: 0.3841\n",
      "Iteration 500 - Loss: 7.8990, Accuracy: 0.7160\n",
      "Iteration 1000 - Loss: 7.8989, Accuracy: 0.7160\n",
      "Iteration 1500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 2000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 2500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 3000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 3500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 4000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 4500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 5000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 5500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 6000 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Iteration 6500 - Loss: 7.8989, Accuracy: 0.7158\n",
      "Fold: 2, Train Loss: 7.9579, Train Accuracy: 0.7123, Val Loss: 8.1494, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 14.0098, Accuracy: 0.3654\n",
      "Iteration 500 - Loss: 8.1732, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1723, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1723, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1007, Train Accuracy: 0.6968, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 17.6463, Accuracy: 0.4611\n",
      "Iteration 500 - Loss: 8.0944, Accuracy: 0.7064\n",
      "Iteration 1000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Fold: 4, Train Loss: 8.0287, Train Accuracy: 0.7082, Val Loss: 8.1073, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.83       0.81      \n",
      "1          0.71       0.63       0.67      \n",
      "2          0.54       0.73       0.62      \n",
      "3          0.86       0.58       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/c12567669a7c497b80cb0e749d82ed6e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 16.4122, Accuracy: 0.4683\n",
      "Iteration 500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Fold: 0, Train Loss: 8.1036, Train Accuracy: 0.6967, Val Loss: 7.9758, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 16.2806, Accuracy: 0.5113\n",
      "Iteration 500 - Loss: 7.9029, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9891, Train Accuracy: 0.7037, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 16.2893, Accuracy: 0.3192\n",
      "Iteration 500 - Loss: 8.0119, Accuracy: 0.7090\n",
      "Iteration 1000 - Loss: 8.0116, Accuracy: 0.7094\n",
      "Iteration 1500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Fold: 2, Train Loss: 7.9566, Train Accuracy: 0.7123, Val Loss: 8.3286, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.8452, Accuracy: 0.4681\n",
      "Iteration 500 - Loss: 8.0262, Accuracy: 0.7012\n",
      "Iteration 1000 - Loss: 8.0258, Accuracy: 0.7012\n",
      "Iteration 1500 - Loss: 8.0256, Accuracy: 0.7012\n",
      "Iteration 2000 - Loss: 8.0255, Accuracy: 0.7012\n",
      "Iteration 2500 - Loss: 8.0255, Accuracy: 0.7012\n",
      "Iteration 3000 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 3500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 4000 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 4500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 5000 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Iteration 5500 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Iteration 6000 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Iteration 6500 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6963, Val Loss: 7.8488, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 16.0782, Accuracy: 0.3745\n",
      "Iteration 500 - Loss: 8.0945, Accuracy: 0.7064\n",
      "Iteration 1000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Fold: 4, Train Loss: 8.0295, Train Accuracy: 0.7082, Val Loss: 8.1073, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.83       0.81      \n",
      "1          0.71       0.63       0.67      \n",
      "2          0.54       0.73       0.62      \n",
      "3          0.86       0.58       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/338453d3ac9840b59bdcffa3c2d3a300\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5396, Accuracy: 0.4770\n",
      "Iteration 500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 1500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 2500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 3500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 4500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 5500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6000 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Iteration 6500 - Loss: 8.0203, Accuracy: 0.6982\n",
      "Fold: 0, Train Loss: 8.1038, Train Accuracy: 0.6967, Val Loss: 7.8284, Val Accuracy: 0.7139\n",
      "Iteration 0 - Loss: 16.8843, Accuracy: 0.3621\n",
      "Iteration 500 - Loss: 8.0752, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.0746, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 8.0745, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.0744, Accuracy: 0.7031\n",
      "Fold: 1, Train Loss: 7.9893, Train Accuracy: 0.7037, Val Loss: 8.2928, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 16.3244, Accuracy: 0.4576\n",
      "Iteration 500 - Loss: 8.0116, Accuracy: 0.7094\n",
      "Iteration 1000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 1500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 2500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 3500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 4500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 5500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6000 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Iteration 6500 - Loss: 8.0115, Accuracy: 0.7094\n",
      "Fold: 2, Train Loss: 7.9564, Train Accuracy: 0.7124, Val Loss: 8.3286, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 14.9709, Accuracy: 0.3752\n",
      "Iteration 500 - Loss: 8.0266, Accuracy: 0.7015\n",
      "Iteration 1000 - Loss: 8.0259, Accuracy: 0.7015\n",
      "Iteration 1500 - Loss: 8.0257, Accuracy: 0.7012\n",
      "Iteration 2000 - Loss: 8.0256, Accuracy: 0.7012\n",
      "Iteration 2500 - Loss: 8.0255, Accuracy: 0.7012\n",
      "Iteration 3000 - Loss: 8.0255, Accuracy: 0.7012\n",
      "Iteration 3500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 4000 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 4500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 5000 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 5500 - Loss: 8.0254, Accuracy: 0.7012\n",
      "Iteration 6000 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Iteration 6500 - Loss: 8.0253, Accuracy: 0.7012\n",
      "Fold: 3, Train Loss: 8.1003, Train Accuracy: 0.6969, Val Loss: 7.8498, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 14.6904, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 8.0946, Accuracy: 0.7064\n",
      "Iteration 1000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 1500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 2500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 3500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 4500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 5500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6000 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Iteration 6500 - Loss: 8.0943, Accuracy: 0.7064\n",
      "Fold: 4, Train Loss: 8.0299, Train Accuracy: 0.7083, Val Loss: 8.1073, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.83       0.81      \n",
      "1          0.71       0.63       0.67      \n",
      "2          0.54       0.73       0.62      \n",
      "3          0.86       0.58       0.69      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/49fb09869358422b9bf3caba630425c6\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 18.6307, Accuracy: 0.0708\n",
      "Iteration 500 - Loss: 8.1520, Accuracy: 0.7134\n",
      "Iteration 1000 - Loss: 8.0922, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0852, Accuracy: 0.7139\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7141\n",
      "Iteration 2500 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1208, Train Accuracy: 0.7133, Val Loss: 7.8728, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 15.3871, Accuracy: 0.3016\n",
      "Iteration 500 - Loss: 8.0433, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9800, Accuracy: 0.7207\n",
      "Iteration 1500 - Loss: 7.9724, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9711, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9706, Accuracy: 0.7223\n",
      "Iteration 3000 - Loss: 7.9703, Accuracy: 0.7223\n",
      "Iteration 3500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9700, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9698, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0091, Train Accuracy: 0.7213, Val Loss: 8.2417, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 14.0793, Accuracy: 0.3499\n",
      "Iteration 500 - Loss: 8.0181, Accuracy: 0.7209\n",
      "Iteration 1000 - Loss: 7.9533, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9457, Accuracy: 0.7209\n",
      "Iteration 2000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9827, Train Accuracy: 0.7205, Val Loss: 8.2310, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.8129, Accuracy: 0.2371\n",
      "Iteration 500 - Loss: 8.1588, Accuracy: 0.7165\n",
      "Iteration 1000 - Loss: 8.0911, Accuracy: 0.7174\n",
      "Iteration 1500 - Loss: 8.0826, Accuracy: 0.7169\n",
      "Iteration 2000 - Loss: 8.0808, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0801, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0797, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7167\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7167\n",
      "Iteration 4500 - Loss: 8.0789, Accuracy: 0.7167\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7167\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0785, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1208, Train Accuracy: 0.7158, Val Loss: 7.9511, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 13.8590, Accuracy: 0.3408\n",
      "Iteration 500 - Loss: 8.0915, Accuracy: 0.7155\n",
      "Iteration 1000 - Loss: 8.0262, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0187, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0176, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0173, Accuracy: 0.7146\n",
      "Iteration 3500 - Loss: 8.0173, Accuracy: 0.7146\n",
      "Iteration 4000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 5000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 6500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Fold: 4, Train Loss: 8.0581, Train Accuracy: 0.7137, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/b71bc1bcf10e49dbaec311715c18274a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 14.9818, Accuracy: 0.2866\n",
      "Iteration 500 - Loss: 8.1566, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0925, Accuracy: 0.7136\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7136\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7136\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1238, Train Accuracy: 0.7133, Val Loss: 7.8728, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 16.0460, Accuracy: 0.2047\n",
      "Iteration 500 - Loss: 8.0405, Accuracy: 0.7200\n",
      "Iteration 1000 - Loss: 7.9781, Accuracy: 0.7205\n",
      "Iteration 1500 - Loss: 7.9710, Accuracy: 0.7221\n",
      "Iteration 2000 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 2500 - Loss: 7.9698, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0098, Train Accuracy: 0.7210, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 17.4287, Accuracy: 0.1686\n",
      "Iteration 500 - Loss: 8.0162, Accuracy: 0.7212\n",
      "Iteration 1000 - Loss: 7.9516, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9447, Accuracy: 0.7209\n",
      "Iteration 2000 - Loss: 7.9441, Accuracy: 0.7209\n",
      "Iteration 2500 - Loss: 7.9442, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9442, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9443, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9847, Train Accuracy: 0.7203, Val Loss: 8.2310, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 13.7173, Accuracy: 0.3872\n",
      "Iteration 500 - Loss: 8.1523, Accuracy: 0.7167\n",
      "Iteration 1000 - Loss: 8.0909, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0831, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0814, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0806, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0801, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0797, Accuracy: 0.7167\n",
      "Iteration 4000 - Loss: 8.0794, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0792, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0789, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1152, Train Accuracy: 0.7161, Val Loss: 7.9501, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 14.7365, Accuracy: 0.3290\n",
      "Iteration 500 - Loss: 8.0811, Accuracy: 0.7148\n",
      "Iteration 1000 - Loss: 8.0231, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Iteration 2000 - Loss: 8.0163, Accuracy: 0.7148\n",
      "Iteration 2500 - Loss: 8.0163, Accuracy: 0.7148\n",
      "Iteration 3000 - Loss: 8.0164, Accuracy: 0.7148\n",
      "Iteration 3500 - Loss: 8.0165, Accuracy: 0.7148\n",
      "Iteration 4000 - Loss: 8.0166, Accuracy: 0.7146\n",
      "Iteration 4500 - Loss: 8.0166, Accuracy: 0.7146\n",
      "Iteration 5000 - Loss: 8.0167, Accuracy: 0.7146\n",
      "Iteration 5500 - Loss: 8.0167, Accuracy: 0.7146\n",
      "Iteration 6000 - Loss: 8.0167, Accuracy: 0.7146\n",
      "Iteration 6500 - Loss: 8.0167, Accuracy: 0.7146\n",
      "Fold: 4, Train Loss: 8.0515, Train Accuracy: 0.7140, Val Loss: 8.0758, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/406e2491005443cbb6a89205d5252904\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | normal | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.3138, Accuracy: 0.3323\n",
      "Iteration 500 - Loss: 8.1557, Accuracy: 0.7132\n",
      "Iteration 1000 - Loss: 8.0928, Accuracy: 0.7136\n",
      "Iteration 1500 - Loss: 8.0855, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0844, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1221, Train Accuracy: 0.7132, Val Loss: 7.8729, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 13.8625, Accuracy: 0.3440\n",
      "Iteration 500 - Loss: 8.0429, Accuracy: 0.7200\n",
      "Iteration 1000 - Loss: 7.9784, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9708, Accuracy: 0.7221\n",
      "Iteration 2000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 2500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9692, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9692, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9691, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0082, Train Accuracy: 0.7212, Val Loss: 8.2424, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 16.5926, Accuracy: 0.2261\n",
      "Iteration 500 - Loss: 8.0195, Accuracy: 0.7212\n",
      "Iteration 1000 - Loss: 7.9534, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9458, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9447, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9444, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9837, Train Accuracy: 0.7205, Val Loss: 8.2311, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 14.8037, Accuracy: 0.3152\n",
      "Iteration 500 - Loss: 8.1520, Accuracy: 0.7167\n",
      "Iteration 1000 - Loss: 8.0887, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0813, Accuracy: 0.7169\n",
      "Iteration 2000 - Loss: 8.0799, Accuracy: 0.7169\n",
      "Iteration 2500 - Loss: 8.0795, Accuracy: 0.7169\n",
      "Iteration 3000 - Loss: 8.0792, Accuracy: 0.7169\n",
      "Iteration 3500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0786, Accuracy: 0.7172\n",
      "Iteration 5500 - Loss: 8.0785, Accuracy: 0.7172\n",
      "Iteration 6000 - Loss: 8.0784, Accuracy: 0.7172\n",
      "Iteration 6500 - Loss: 8.0784, Accuracy: 0.7172\n",
      "Fold: 3, Train Loss: 8.1196, Train Accuracy: 0.7161, Val Loss: 7.9524, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 14.2205, Accuracy: 0.3860\n",
      "Iteration 500 - Loss: 8.0890, Accuracy: 0.7153\n",
      "Iteration 1000 - Loss: 8.0264, Accuracy: 0.7141\n",
      "Iteration 1500 - Loss: 8.0191, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0180, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0178, Accuracy: 0.7146\n",
      "Iteration 3000 - Loss: 8.0176, Accuracy: 0.7146\n",
      "Iteration 3500 - Loss: 8.0175, Accuracy: 0.7146\n",
      "Iteration 4000 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 4500 - Loss: 8.0173, Accuracy: 0.7146\n",
      "Iteration 5000 - Loss: 8.0172, Accuracy: 0.7146\n",
      "Iteration 5500 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Fold: 4, Train Loss: 8.0542, Train Accuracy: 0.7141, Val Loss: 8.0752, Val Accuracy: 0.7139\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/c1f26965ef5341aca5c3ad01bfd760f4\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5690, Accuracy: 0.4998\n",
      "Iteration 500 - Loss: 8.2672, Accuracy: 0.6360\n",
      "Iteration 1000 - Loss: 7.1318, Accuracy: 0.5633\n",
      "Iteration 1500 - Loss: 7.8229, Accuracy: 0.6667\n",
      "Iteration 2000 - Loss: 7.6225, Accuracy: 0.6435\n",
      "Iteration 2500 - Loss: 7.5931, Accuracy: 0.4665\n",
      "Iteration 3000 - Loss: 7.8435, Accuracy: 0.6060\n",
      "Iteration 3500 - Loss: 7.6291, Accuracy: 0.5962\n",
      "Iteration 4000 - Loss: 8.3781, Accuracy: 0.6379\n",
      "Iteration 4500 - Loss: 8.1927, Accuracy: 0.5851\n",
      "Iteration 5000 - Loss: 7.9868, Accuracy: 0.6614\n",
      "Iteration 5500 - Loss: 8.5416, Accuracy: 0.6538\n",
      "Iteration 6000 - Loss: 8.1869, Accuracy: 0.6008\n",
      "Iteration 6500 - Loss: 10.4090, Accuracy: 0.6128\n",
      "Fold: 0, Train Loss: 8.1869, Train Accuracy: 0.6054, Val Loss: 8.7326, Val Accuracy: 0.5797\n",
      "Iteration 0 - Loss: 15.5251, Accuracy: 0.5038\n",
      "Iteration 500 - Loss: 7.5670, Accuracy: 0.6503\n",
      "Iteration 1000 - Loss: 9.4123, Accuracy: 0.6384\n",
      "Iteration 1500 - Loss: 10.1644, Accuracy: 0.6367\n",
      "Iteration 2000 - Loss: 6.3164, Accuracy: 0.6395\n",
      "Iteration 2500 - Loss: 9.9492, Accuracy: 0.6698\n",
      "Iteration 3000 - Loss: 8.7348, Accuracy: 0.5251\n",
      "Iteration 3500 - Loss: 8.5848, Accuracy: 0.6506\n",
      "Iteration 4000 - Loss: 9.4843, Accuracy: 0.6543\n",
      "Iteration 4500 - Loss: 6.2733, Accuracy: 0.6438\n",
      "Iteration 5000 - Loss: 7.5791, Accuracy: 0.5436\n",
      "Iteration 5500 - Loss: 7.9136, Accuracy: 0.5741\n",
      "Iteration 6000 - Loss: 7.4644, Accuracy: 0.6646\n",
      "Iteration 6500 - Loss: 8.2711, Accuracy: 0.6098\n",
      "Fold: 1, Train Loss: 8.0958, Train Accuracy: 0.6098, Val Loss: 9.0460, Val Accuracy: 0.5619\n",
      "Iteration 0 - Loss: 15.5147, Accuracy: 0.5108\n",
      "Iteration 500 - Loss: 8.4445, Accuracy: 0.6123\n",
      "Iteration 1000 - Loss: 6.4799, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.9220, Accuracy: 0.6503\n",
      "Iteration 2000 - Loss: 8.1223, Accuracy: 0.5288\n",
      "Iteration 2500 - Loss: 8.5500, Accuracy: 0.6599\n",
      "Iteration 3000 - Loss: 7.6294, Accuracy: 0.6114\n",
      "Iteration 3500 - Loss: 7.7760, Accuracy: 0.6023\n",
      "Iteration 4000 - Loss: 7.6897, Accuracy: 0.6407\n",
      "Iteration 4500 - Loss: 8.1864, Accuracy: 0.6341\n",
      "Iteration 5000 - Loss: 7.7927, Accuracy: 0.6123\n",
      "Iteration 5500 - Loss: 8.5220, Accuracy: 0.5500\n",
      "Iteration 6000 - Loss: 8.1932, Accuracy: 0.6257\n",
      "Iteration 6500 - Loss: 9.0870, Accuracy: 0.5650\n",
      "Fold: 2, Train Loss: 8.0451, Train Accuracy: 0.6123, Val Loss: 8.6965, Val Accuracy: 0.5816\n",
      "Iteration 0 - Loss: 15.5350, Accuracy: 0.5009\n",
      "Iteration 500 - Loss: 8.3296, Accuracy: 0.6902\n",
      "Iteration 1000 - Loss: 8.3748, Accuracy: 0.5434\n",
      "Iteration 1500 - Loss: 8.8254, Accuracy: 0.5952\n",
      "Iteration 2000 - Loss: 8.4075, Accuracy: 0.6639\n",
      "Iteration 2500 - Loss: 8.0660, Accuracy: 0.5579\n",
      "Iteration 3000 - Loss: 6.8774, Accuracy: 0.5804\n",
      "Iteration 3500 - Loss: 7.9578, Accuracy: 0.6147\n",
      "Iteration 4000 - Loss: 6.3721, Accuracy: 0.5844\n",
      "Iteration 4500 - Loss: 6.3400, Accuracy: 0.6477\n",
      "Iteration 5000 - Loss: 6.7934, Accuracy: 0.6384\n",
      "Iteration 5500 - Loss: 8.8424, Accuracy: 0.6123\n",
      "Iteration 6000 - Loss: 8.5839, Accuracy: 0.6388\n",
      "Iteration 6500 - Loss: 7.5149, Accuracy: 0.5861\n",
      "Fold: 3, Train Loss: 8.1905, Train Accuracy: 0.6053, Val Loss: 6.6200, Val Accuracy: 0.6820\n",
      "Iteration 0 - Loss: 15.5494, Accuracy: 0.5012\n",
      "Iteration 500 - Loss: 8.6684, Accuracy: 0.6424\n",
      "Iteration 1000 - Loss: 7.7154, Accuracy: 0.5607\n",
      "Iteration 1500 - Loss: 7.6474, Accuracy: 0.5645\n",
      "Iteration 2000 - Loss: 7.2598, Accuracy: 0.5983\n",
      "Iteration 2500 - Loss: 9.3288, Accuracy: 0.5575\n",
      "Iteration 3000 - Loss: 8.1078, Accuracy: 0.5915\n",
      "Iteration 3500 - Loss: 6.3340, Accuracy: 0.6796\n",
      "Iteration 4000 - Loss: 7.8539, Accuracy: 0.5689\n",
      "Iteration 4500 - Loss: 6.9437, Accuracy: 0.5434\n",
      "Iteration 5000 - Loss: 7.7101, Accuracy: 0.6098\n",
      "Iteration 5500 - Loss: 7.9671, Accuracy: 0.5692\n",
      "Iteration 6000 - Loss: 7.0538, Accuracy: 0.5753\n",
      "Iteration 6500 - Loss: 7.3923, Accuracy: 0.6656\n",
      "Fold: 4, Train Loss: 8.1397, Train Accuracy: 0.6077, Val Loss: 7.1885, Val Accuracy: 0.6557\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.73       0.85       0.78      \n",
      "1          0.64       0.54       0.59      \n",
      "2          0.48       0.42       0.45      \n",
      "3          0.60       0.76       0.67      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6300 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/c7948461e7bc46c5912802845db996bc\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5503, Accuracy: 0.5021\n",
      "Iteration 500 - Loss: 8.2772, Accuracy: 0.6116\n",
      "Iteration 1000 - Loss: 7.3692, Accuracy: 0.6241\n",
      "Iteration 1500 - Loss: 7.7666, Accuracy: 0.6105\n",
      "Iteration 2000 - Loss: 9.2469, Accuracy: 0.6339\n",
      "Iteration 2500 - Loss: 7.7631, Accuracy: 0.6520\n",
      "Iteration 3000 - Loss: 8.7153, Accuracy: 0.5879\n",
      "Iteration 3500 - Loss: 7.6537, Accuracy: 0.6006\n",
      "Iteration 4000 - Loss: 9.1431, Accuracy: 0.5788\n",
      "Iteration 4500 - Loss: 9.6048, Accuracy: 0.6187\n",
      "Iteration 5000 - Loss: 6.8403, Accuracy: 0.5328\n",
      "Iteration 5500 - Loss: 9.2222, Accuracy: 0.5856\n",
      "Iteration 6000 - Loss: 8.6368, Accuracy: 0.6161\n",
      "Iteration 6500 - Loss: 9.5417, Accuracy: 0.5621\n",
      "Fold: 0, Train Loss: 8.1868, Train Accuracy: 0.6054, Val Loss: 7.5114, Val Accuracy: 0.6379\n",
      "Iteration 0 - Loss: 15.5581, Accuracy: 0.5096\n",
      "Iteration 500 - Loss: 7.7943, Accuracy: 0.6245\n",
      "Iteration 1000 - Loss: 10.1900, Accuracy: 0.6567\n",
      "Iteration 1500 - Loss: 6.5506, Accuracy: 0.6313\n",
      "Iteration 2000 - Loss: 9.0395, Accuracy: 0.5903\n",
      "Iteration 2500 - Loss: 9.5518, Accuracy: 0.5389\n",
      "Iteration 3000 - Loss: 7.2610, Accuracy: 0.6768\n",
      "Iteration 3500 - Loss: 10.2914, Accuracy: 0.5441\n",
      "Iteration 4000 - Loss: 9.0048, Accuracy: 0.5561\n",
      "Iteration 4500 - Loss: 8.2113, Accuracy: 0.6320\n",
      "Iteration 5000 - Loss: 7.7962, Accuracy: 0.6388\n",
      "Iteration 5500 - Loss: 8.2807, Accuracy: 0.6553\n",
      "Iteration 6000 - Loss: 7.8167, Accuracy: 0.5307\n",
      "Iteration 6500 - Loss: 8.2282, Accuracy: 0.6426\n",
      "Fold: 1, Train Loss: 8.0943, Train Accuracy: 0.6099, Val Loss: 7.5106, Val Accuracy: 0.6379\n",
      "Iteration 0 - Loss: 15.5646, Accuracy: 0.5089\n",
      "Iteration 500 - Loss: 9.5958, Accuracy: 0.6752\n",
      "Iteration 1000 - Loss: 9.4617, Accuracy: 0.5331\n",
      "Iteration 1500 - Loss: 7.2189, Accuracy: 0.5966\n",
      "Iteration 2000 - Loss: 7.5547, Accuracy: 0.6909\n",
      "Iteration 2500 - Loss: 9.6171, Accuracy: 0.6062\n",
      "Iteration 3000 - Loss: 8.3513, Accuracy: 0.6215\n",
      "Iteration 3500 - Loss: 8.2606, Accuracy: 0.5300\n",
      "Iteration 4000 - Loss: 7.6838, Accuracy: 0.6313\n",
      "Iteration 4500 - Loss: 7.1107, Accuracy: 0.5929\n",
      "Iteration 5000 - Loss: 9.2961, Accuracy: 0.5652\n",
      "Iteration 5500 - Loss: 10.1037, Accuracy: 0.6395\n",
      "Iteration 6000 - Loss: 7.2904, Accuracy: 0.6273\n",
      "Iteration 6500 - Loss: 7.6290, Accuracy: 0.6487\n",
      "Fold: 2, Train Loss: 8.0459, Train Accuracy: 0.6123, Val Loss: 7.5850, Val Accuracy: 0.6360\n",
      "Iteration 0 - Loss: 15.5322, Accuracy: 0.4981\n",
      "Iteration 500 - Loss: 9.6921, Accuracy: 0.6224\n",
      "Iteration 1000 - Loss: 7.7755, Accuracy: 0.5933\n",
      "Iteration 1500 - Loss: 9.9208, Accuracy: 0.6320\n",
      "Iteration 2000 - Loss: 7.6031, Accuracy: 0.6836\n",
      "Iteration 2500 - Loss: 6.7401, Accuracy: 0.6184\n",
      "Iteration 3000 - Loss: 6.5166, Accuracy: 0.6606\n",
      "Iteration 3500 - Loss: 7.4780, Accuracy: 0.5797\n",
      "Iteration 4000 - Loss: 7.2850, Accuracy: 0.6302\n",
      "Iteration 4500 - Loss: 8.3013, Accuracy: 0.5553\n",
      "Iteration 5000 - Loss: 7.9772, Accuracy: 0.5544\n",
      "Iteration 5500 - Loss: 8.5758, Accuracy: 0.6147\n",
      "Iteration 6000 - Loss: 9.8068, Accuracy: 0.5912\n",
      "Iteration 6500 - Loss: 7.4989, Accuracy: 0.5368\n",
      "Fold: 3, Train Loss: 8.1937, Train Accuracy: 0.6051, Val Loss: 8.5590, Val Accuracy: 0.5901\n",
      "Iteration 0 - Loss: 15.5340, Accuracy: 0.5023\n",
      "Iteration 500 - Loss: 7.0746, Accuracy: 0.5701\n",
      "Iteration 1000 - Loss: 7.5098, Accuracy: 0.5687\n",
      "Iteration 1500 - Loss: 8.5868, Accuracy: 0.5908\n",
      "Iteration 2000 - Loss: 9.8102, Accuracy: 0.6419\n",
      "Iteration 2500 - Loss: 8.4892, Accuracy: 0.6590\n",
      "Iteration 3000 - Loss: 7.8266, Accuracy: 0.6238\n",
      "Iteration 3500 - Loss: 6.4506, Accuracy: 0.5732\n",
      "Iteration 4000 - Loss: 8.8260, Accuracy: 0.6353\n",
      "Iteration 4500 - Loss: 6.8789, Accuracy: 0.6393\n",
      "Iteration 5000 - Loss: 8.9888, Accuracy: 0.5957\n",
      "Iteration 5500 - Loss: 8.9709, Accuracy: 0.5779\n",
      "Iteration 6000 - Loss: 6.8505, Accuracy: 0.5500\n",
      "Iteration 6500 - Loss: 9.4861, Accuracy: 0.5868\n",
      "Fold: 4, Train Loss: 8.1391, Train Accuracy: 0.6077, Val Loss: 7.5297, Val Accuracy: 0.6351\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.73       0.85       0.78      \n",
      "1          0.72       0.44       0.54      \n",
      "2          0.48       0.86       0.61      \n",
      "3          0.90       0.40       0.56      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6400 | F1-Score: 0.6300\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/ec292aab84b542f0abb02a4fd73ea369\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5118, Accuracy: 0.5007\n",
      "Iteration 500 - Loss: 8.1447, Accuracy: 0.6215\n",
      "Iteration 1000 - Loss: 7.1878, Accuracy: 0.6428\n",
      "Iteration 1500 - Loss: 7.1882, Accuracy: 0.6353\n",
      "Iteration 2000 - Loss: 9.3922, Accuracy: 0.5926\n",
      "Iteration 2500 - Loss: 8.1097, Accuracy: 0.6726\n",
      "Iteration 3000 - Loss: 9.3013, Accuracy: 0.6907\n",
      "Iteration 3500 - Loss: 6.8028, Accuracy: 0.6266\n",
      "Iteration 4000 - Loss: 7.0237, Accuracy: 0.6468\n",
      "Iteration 4500 - Loss: 9.1569, Accuracy: 0.6419\n",
      "Iteration 5000 - Loss: 9.7936, Accuracy: 0.6466\n",
      "Iteration 5500 - Loss: 8.4419, Accuracy: 0.6503\n",
      "Iteration 6000 - Loss: 8.4848, Accuracy: 0.5298\n",
      "Iteration 6500 - Loss: 7.5519, Accuracy: 0.5708\n",
      "Fold: 0, Train Loss: 8.1816, Train Accuracy: 0.6056, Val Loss: 9.6429, Val Accuracy: 0.5310\n",
      "Iteration 0 - Loss: 15.5117, Accuracy: 0.5101\n",
      "Iteration 500 - Loss: 8.7171, Accuracy: 0.6407\n",
      "Iteration 1000 - Loss: 7.9057, Accuracy: 0.6712\n",
      "Iteration 1500 - Loss: 6.9779, Accuracy: 0.6116\n",
      "Iteration 2000 - Loss: 9.3821, Accuracy: 0.6609\n",
      "Iteration 2500 - Loss: 7.7422, Accuracy: 0.5227\n",
      "Iteration 3000 - Loss: 7.4429, Accuracy: 0.6278\n",
      "Iteration 3500 - Loss: 7.4329, Accuracy: 0.5962\n",
      "Iteration 4000 - Loss: 7.3763, Accuracy: 0.5652\n",
      "Iteration 4500 - Loss: 9.3855, Accuracy: 0.6468\n",
      "Iteration 5000 - Loss: 8.4141, Accuracy: 0.6004\n",
      "Iteration 5500 - Loss: 7.4585, Accuracy: 0.5631\n",
      "Iteration 6000 - Loss: 8.8803, Accuracy: 0.6280\n",
      "Iteration 6500 - Loss: 7.5084, Accuracy: 0.6440\n",
      "Fold: 1, Train Loss: 8.0905, Train Accuracy: 0.6101, Val Loss: 8.6574, Val Accuracy: 0.5826\n",
      "Iteration 0 - Loss: 15.5859, Accuracy: 0.5103\n",
      "Iteration 500 - Loss: 7.0807, Accuracy: 0.6163\n",
      "Iteration 1000 - Loss: 8.5261, Accuracy: 0.6330\n",
      "Iteration 1500 - Loss: 8.4081, Accuracy: 0.6869\n",
      "Iteration 2000 - Loss: 8.8909, Accuracy: 0.6672\n",
      "Iteration 2500 - Loss: 7.6884, Accuracy: 0.6278\n",
      "Iteration 3000 - Loss: 7.0954, Accuracy: 0.6262\n",
      "Iteration 3500 - Loss: 9.6350, Accuracy: 0.6438\n",
      "Iteration 4000 - Loss: 7.0172, Accuracy: 0.6346\n",
      "Iteration 4500 - Loss: 9.6987, Accuracy: 0.6541\n",
      "Iteration 5000 - Loss: 7.3892, Accuracy: 0.6133\n",
      "Iteration 5500 - Loss: 8.6471, Accuracy: 0.6402\n",
      "Iteration 6000 - Loss: 7.6126, Accuracy: 0.6428\n",
      "Iteration 6500 - Loss: 7.0542, Accuracy: 0.5596\n",
      "Fold: 2, Train Loss: 8.0497, Train Accuracy: 0.6121, Val Loss: 7.7473, Val Accuracy: 0.6304\n",
      "Iteration 0 - Loss: 15.5187, Accuracy: 0.4932\n",
      "Iteration 500 - Loss: 8.0872, Accuracy: 0.5141\n",
      "Iteration 1000 - Loss: 8.0187, Accuracy: 0.5579\n",
      "Iteration 1500 - Loss: 7.8815, Accuracy: 0.5643\n",
      "Iteration 2000 - Loss: 8.1670, Accuracy: 0.5842\n",
      "Iteration 2500 - Loss: 8.9537, Accuracy: 0.5591\n",
      "Iteration 3000 - Loss: 8.8089, Accuracy: 0.6180\n",
      "Iteration 3500 - Loss: 7.4003, Accuracy: 0.6081\n",
      "Iteration 4000 - Loss: 8.5964, Accuracy: 0.6529\n",
      "Iteration 4500 - Loss: 8.3987, Accuracy: 0.6316\n",
      "Iteration 5000 - Loss: 8.4715, Accuracy: 0.5800\n",
      "Iteration 5500 - Loss: 8.2224, Accuracy: 0.6508\n",
      "Iteration 6000 - Loss: 10.2429, Accuracy: 0.5640\n",
      "Iteration 6500 - Loss: 8.1588, Accuracy: 0.5328\n",
      "Fold: 3, Train Loss: 8.1917, Train Accuracy: 0.6052, Val Loss: 6.8734, Val Accuracy: 0.6651\n",
      "Iteration 0 - Loss: 15.5264, Accuracy: 0.5091\n",
      "Iteration 500 - Loss: 7.6290, Accuracy: 0.5706\n",
      "Iteration 1000 - Loss: 6.8841, Accuracy: 0.5826\n",
      "Iteration 1500 - Loss: 8.2006, Accuracy: 0.6499\n",
      "Iteration 2000 - Loss: 7.9680, Accuracy: 0.5978\n",
      "Iteration 2500 - Loss: 6.9725, Accuracy: 0.5910\n",
      "Iteration 3000 - Loss: 7.9254, Accuracy: 0.6393\n",
      "Iteration 3500 - Loss: 6.2388, Accuracy: 0.6499\n",
      "Iteration 4000 - Loss: 8.4980, Accuracy: 0.6707\n",
      "Iteration 4500 - Loss: 7.1849, Accuracy: 0.6499\n",
      "Iteration 5000 - Loss: 7.0720, Accuracy: 0.6273\n",
      "Iteration 5500 - Loss: 6.7554, Accuracy: 0.5678\n",
      "Iteration 6000 - Loss: 9.5136, Accuracy: 0.5976\n",
      "Iteration 6500 - Loss: 6.8754, Accuracy: 0.6585\n",
      "Fold: 4, Train Loss: 8.1401, Train Accuracy: 0.6077, Val Loss: 8.2215, Val Accuracy: 0.6023\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.73       0.86       0.79      \n",
      "1          0.63       0.24       0.35      \n",
      "2          0.44       0.79       0.56      \n",
      "3          0.75       0.66       0.70      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.5900 | F1-Score: 0.5700\n",
      "ðŸƒ View run method-batch-lr-0.01-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/7a342f01fd3145909ca4db11d6518b77\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5405, Accuracy: 0.5016\n",
      "Iteration 500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Fold: 0, Train Loss: 8.1052, Train Accuracy: 0.6964, Val Loss: 7.9758, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 15.5429, Accuracy: 0.5122\n",
      "Iteration 500 - Loss: 7.9028, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9906, Train Accuracy: 0.7036, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.5064, Accuracy: 0.6459\n",
      "Iteration 1000 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 1500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 2000 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 2500 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 3000 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 3500 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 4000 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 4500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 5000 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 5500 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 6000 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 6500 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Fold: 2, Train Loss: 8.0413, Train Accuracy: 0.6581, Val Loss: 7.9481, Val Accuracy: 0.6829\n",
      "Iteration 0 - Loss: 15.5433, Accuracy: 0.4967\n",
      "Iteration 500 - Loss: 8.1734, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1729, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5052\n",
      "Iteration 500 - Loss: 7.9621, Accuracy: 0.7104\n",
      "Iteration 1000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 1500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Fold: 4, Train Loss: 8.0305, Train Accuracy: 0.7081, Val Loss: 8.0708, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.76       0.80      \n",
      "1          0.60       0.77       0.67      \n",
      "2          0.54       0.38       0.44      \n",
      "3          0.74       0.73       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/3101f8fde7744b75b70c1871afb0bc50\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5420, Accuracy: 0.5009\n",
      "Iteration 500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 1500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 2500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 3500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 4500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 5500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6000 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Iteration 6500 - Loss: 8.1850, Accuracy: 0.6956\n",
      "Fold: 0, Train Loss: 8.1056, Train Accuracy: 0.6961, Val Loss: 7.9758, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 15.5400, Accuracy: 0.5120\n",
      "Iteration 500 - Loss: 7.9028, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9910, Train Accuracy: 0.7034, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 15.5407, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 1000 - Loss: 8.5063, Accuracy: 0.6459\n",
      "Iteration 1500 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 2000 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 2500 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 3000 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 3500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 4000 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 4500 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 5000 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Iteration 5500 - Loss: 8.5062, Accuracy: 0.6459\n",
      "Iteration 6000 - Loss: 7.6895, Accuracy: 0.6264\n",
      "Iteration 6500 - Loss: 7.9192, Accuracy: 0.7022\n",
      "Fold: 2, Train Loss: 8.0411, Train Accuracy: 0.6582, Val Loss: 8.8413, Val Accuracy: 0.6079\n",
      "Iteration 0 - Loss: 15.5421, Accuracy: 0.4953\n",
      "Iteration 500 - Loss: 8.1734, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1729, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 15.5416, Accuracy: 0.5056\n",
      "Iteration 500 - Loss: 7.9621, Accuracy: 0.7104\n",
      "Iteration 1000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 1500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Fold: 4, Train Loss: 8.0305, Train Accuracy: 0.7081, Val Loss: 8.0708, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.76       0.80      \n",
      "1          0.60       0.77       0.67      \n",
      "2          0.54       0.38       0.44      \n",
      "3          0.74       0.73       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/fe4366d797e241ff91e3c031576d4b3b\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5407, Accuracy: 0.5012\n",
      "Iteration 500 - Loss: 8.6981, Accuracy: 0.6372\n",
      "Iteration 1000 - Loss: 7.8005, Accuracy: 0.6130\n",
      "Iteration 1500 - Loss: 8.0592, Accuracy: 0.6961\n",
      "Iteration 2000 - Loss: 8.6980, Accuracy: 0.6372\n",
      "Iteration 2500 - Loss: 7.8005, Accuracy: 0.6130\n",
      "Iteration 3000 - Loss: 8.0592, Accuracy: 0.6961\n",
      "Iteration 3500 - Loss: 8.6980, Accuracy: 0.6372\n",
      "Iteration 4000 - Loss: 7.8005, Accuracy: 0.6130\n",
      "Iteration 4500 - Loss: 8.0592, Accuracy: 0.6961\n",
      "Iteration 5000 - Loss: 8.6980, Accuracy: 0.6372\n",
      "Iteration 5500 - Loss: 7.8005, Accuracy: 0.6130\n",
      "Iteration 6000 - Loss: 8.0592, Accuracy: 0.6961\n",
      "Iteration 6500 - Loss: 8.6980, Accuracy: 0.6372\n",
      "Fold: 0, Train Loss: 8.1886, Train Accuracy: 0.6488, Val Loss: 7.6711, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.5419, Accuracy: 0.5120\n",
      "Iteration 500 - Loss: 7.9028, Accuracy: 0.7045\n",
      "Iteration 1000 - Loss: 7.9025, Accuracy: 0.7045\n",
      "Iteration 1500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 2500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 4500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 5500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6000 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 7.9024, Accuracy: 0.7045\n",
      "Fold: 1, Train Loss: 7.9906, Train Accuracy: 0.7036, Val Loss: 8.2031, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 15.5407, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.3822, Accuracy: 0.6424\n",
      "Iteration 1000 - Loss: 7.7120, Accuracy: 0.6152\n",
      "Iteration 1500 - Loss: 7.9676, Accuracy: 0.7115\n",
      "Iteration 2000 - Loss: 8.5634, Accuracy: 0.6412\n",
      "Iteration 2500 - Loss: 7.6420, Accuracy: 0.6363\n",
      "Iteration 3000 - Loss: 7.9400, Accuracy: 0.7045\n",
      "Iteration 3500 - Loss: 8.4268, Accuracy: 0.6461\n",
      "Iteration 4000 - Loss: 7.7437, Accuracy: 0.6367\n",
      "Iteration 4500 - Loss: 7.8998, Accuracy: 0.6947\n",
      "Iteration 5000 - Loss: 8.3824, Accuracy: 0.6428\n",
      "Iteration 5500 - Loss: 7.7120, Accuracy: 0.6152\n",
      "Iteration 6000 - Loss: 7.9676, Accuracy: 0.7115\n",
      "Iteration 6500 - Loss: 8.5634, Accuracy: 0.6412\n",
      "Fold: 2, Train Loss: 8.0338, Train Accuracy: 0.6587, Val Loss: 7.8591, Val Accuracy: 0.6942\n",
      "Iteration 0 - Loss: 15.5398, Accuracy: 0.4958\n",
      "Iteration 500 - Loss: 8.1734, Accuracy: 0.6930\n",
      "Iteration 1000 - Loss: 8.1729, Accuracy: 0.6930\n",
      "Iteration 1500 - Loss: 8.1727, Accuracy: 0.6930\n",
      "Iteration 2000 - Loss: 8.1726, Accuracy: 0.6930\n",
      "Iteration 2500 - Loss: 8.1725, Accuracy: 0.6930\n",
      "Iteration 3000 - Loss: 8.1725, Accuracy: 0.6928\n",
      "Iteration 3500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 4500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 5500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6000 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Iteration 6500 - Loss: 8.1724, Accuracy: 0.6928\n",
      "Fold: 3, Train Loss: 8.1014, Train Accuracy: 0.6969, Val Loss: 8.1339, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 15.5441, Accuracy: 0.5054\n",
      "Iteration 500 - Loss: 7.9621, Accuracy: 0.7104\n",
      "Iteration 1000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 1500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 2500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 3500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 4500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 5500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6000 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Iteration 6500 - Loss: 7.9620, Accuracy: 0.7106\n",
      "Fold: 4, Train Loss: 8.0305, Train Accuracy: 0.7081, Val Loss: 8.0708, Val Accuracy: 0.6932\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.76       0.80      \n",
      "1          0.60       0.77       0.67      \n",
      "2          0.54       0.38       0.44      \n",
      "3          0.74       0.73       0.73      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-batch-lr-0.001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/29304057806c44409f4698c9fe780285\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5014\n",
      "Iteration 500 - Loss: 8.1550, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0923, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1230, Train Accuracy: 0.7136, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.5120\n",
      "Iteration 500 - Loss: 8.0422, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9791, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9716, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9704, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0083, Train Accuracy: 0.7216, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.5120\n",
      "Iteration 500 - Loss: 8.0194, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9536, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9846, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.4953\n",
      "Iteration 500 - Loss: 8.1535, Accuracy: 0.7169\n",
      "Iteration 1000 - Loss: 8.0896, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0819, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0805, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0799, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0796, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1192, Train Accuracy: 0.7163, Val Loss: 7.9512, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.5056\n",
      "Iteration 500 - Loss: 8.0914, Accuracy: 0.7153\n",
      "Iteration 1000 - Loss: 8.0267, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0189, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0177, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 4500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0566, Train Accuracy: 0.7140, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/1f9ac20a581d4bf7b026525e77f237d1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5014\n",
      "Iteration 500 - Loss: 8.1550, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0923, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1230, Train Accuracy: 0.7136, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5122\n",
      "Iteration 500 - Loss: 8.0422, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9791, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9716, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9704, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0083, Train Accuracy: 0.7216, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.0194, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9536, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9846, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.4955\n",
      "Iteration 500 - Loss: 8.1535, Accuracy: 0.7169\n",
      "Iteration 1000 - Loss: 8.0896, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0819, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0805, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0799, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0796, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1192, Train Accuracy: 0.7163, Val Loss: 7.9512, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.5056\n",
      "Iteration 500 - Loss: 8.0914, Accuracy: 0.7153\n",
      "Iteration 1000 - Loss: 8.0267, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0189, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0177, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 4500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0566, Train Accuracy: 0.7140, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/d1437f74c90e466383b195eb8d1cbcad\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | batch | uniform | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5428, Accuracy: 0.5012\n",
      "Iteration 500 - Loss: 8.1550, Accuracy: 0.7139\n",
      "Iteration 1000 - Loss: 8.0923, Accuracy: 0.7144\n",
      "Iteration 1500 - Loss: 8.0853, Accuracy: 0.7141\n",
      "Iteration 2000 - Loss: 8.0843, Accuracy: 0.7139\n",
      "Iteration 2500 - Loss: 8.0842, Accuracy: 0.7139\n",
      "Iteration 3000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 3500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 4500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 5500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6000 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Iteration 6500 - Loss: 8.0841, Accuracy: 0.7139\n",
      "Fold: 0, Train Loss: 8.1230, Train Accuracy: 0.7136, Val Loss: 7.8730, Val Accuracy: 0.7289\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.0422, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9791, Accuracy: 0.7209\n",
      "Iteration 1500 - Loss: 7.9716, Accuracy: 0.7223\n",
      "Iteration 2000 - Loss: 7.9704, Accuracy: 0.7223\n",
      "Iteration 2500 - Loss: 7.9701, Accuracy: 0.7221\n",
      "Iteration 3000 - Loss: 7.9699, Accuracy: 0.7221\n",
      "Iteration 3500 - Loss: 7.9697, Accuracy: 0.7221\n",
      "Iteration 4000 - Loss: 7.9696, Accuracy: 0.7221\n",
      "Iteration 4500 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5000 - Loss: 7.9695, Accuracy: 0.7221\n",
      "Iteration 5500 - Loss: 7.9694, Accuracy: 0.7221\n",
      "Iteration 6000 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Iteration 6500 - Loss: 7.9693, Accuracy: 0.7221\n",
      "Fold: 1, Train Loss: 8.0083, Train Accuracy: 0.7216, Val Loss: 8.2420, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5426, Accuracy: 0.5117\n",
      "Iteration 500 - Loss: 8.0194, Accuracy: 0.7207\n",
      "Iteration 1000 - Loss: 7.9536, Accuracy: 0.7212\n",
      "Iteration 1500 - Loss: 7.9459, Accuracy: 0.7212\n",
      "Iteration 2000 - Loss: 7.9448, Accuracy: 0.7212\n",
      "Iteration 2500 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3000 - Loss: 7.9446, Accuracy: 0.7212\n",
      "Iteration 3500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 4500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 5500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6000 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Iteration 6500 - Loss: 7.9445, Accuracy: 0.7212\n",
      "Fold: 2, Train Loss: 7.9846, Train Accuracy: 0.7207, Val Loss: 8.2312, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5426, Accuracy: 0.4953\n",
      "Iteration 500 - Loss: 8.1535, Accuracy: 0.7169\n",
      "Iteration 1000 - Loss: 8.0896, Accuracy: 0.7172\n",
      "Iteration 1500 - Loss: 8.0819, Accuracy: 0.7167\n",
      "Iteration 2000 - Loss: 8.0805, Accuracy: 0.7167\n",
      "Iteration 2500 - Loss: 8.0799, Accuracy: 0.7167\n",
      "Iteration 3000 - Loss: 8.0796, Accuracy: 0.7167\n",
      "Iteration 3500 - Loss: 8.0793, Accuracy: 0.7169\n",
      "Iteration 4000 - Loss: 8.0791, Accuracy: 0.7169\n",
      "Iteration 4500 - Loss: 8.0790, Accuracy: 0.7169\n",
      "Iteration 5000 - Loss: 8.0788, Accuracy: 0.7169\n",
      "Iteration 5500 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6000 - Loss: 8.0787, Accuracy: 0.7169\n",
      "Iteration 6500 - Loss: 8.0786, Accuracy: 0.7169\n",
      "Fold: 3, Train Loss: 8.1192, Train Accuracy: 0.7163, Val Loss: 7.9512, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5054\n",
      "Iteration 500 - Loss: 8.0914, Accuracy: 0.7153\n",
      "Iteration 1000 - Loss: 8.0267, Accuracy: 0.7139\n",
      "Iteration 1500 - Loss: 8.0189, Accuracy: 0.7146\n",
      "Iteration 2000 - Loss: 8.0177, Accuracy: 0.7146\n",
      "Iteration 2500 - Loss: 8.0174, Accuracy: 0.7144\n",
      "Iteration 3000 - Loss: 8.0172, Accuracy: 0.7144\n",
      "Iteration 3500 - Loss: 8.0171, Accuracy: 0.7144\n",
      "Iteration 4000 - Loss: 8.0171, Accuracy: 0.7141\n",
      "Iteration 4500 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5000 - Loss: 8.0170, Accuracy: 0.7141\n",
      "Iteration 5500 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6000 - Loss: 8.0170, Accuracy: 0.7144\n",
      "Iteration 6500 - Loss: 8.0169, Accuracy: 0.7144\n",
      "Fold: 4, Train Loss: 8.0566, Train Accuracy: 0.7140, Val Loss: 8.0754, Val Accuracy: 0.7148\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.65       0.69       0.67      \n",
      "2          0.55       0.58       0.56      \n",
      "3          0.80       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-batch-lr-0.0001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/5f3483c2db2e41bfb994b2608a9f3339\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.6000, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 6.0833, Accuracy: 0.8750\n",
      "Iteration 1000 - Loss: 9.9602, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 10.0697, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 9.3078, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 7.1958, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.3696, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 7.5318, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 7.1664, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 6.7986, Accuracy: 0.8438\n",
      "Iteration 5000 - Loss: 8.1780, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 8.1168, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 8.4316, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 9.2428, Accuracy: 0.6562\n",
      "Fold: 0, Train Loss: 8.1459, Train Accuracy: 0.7035, Val Loss: 7.8779, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 15.2652, Accuracy: 0.3125\n",
      "Iteration 500 - Loss: 8.8270, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 5.0506, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 7.4322, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 6.6870, Accuracy: 0.8125\n",
      "Iteration 2500 - Loss: 8.0853, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 9.5114, Accuracy: 0.5625\n",
      "Iteration 3500 - Loss: 8.9361, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 7.8235, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3310, Accuracy: 0.5938\n",
      "Iteration 5000 - Loss: 6.8331, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 7.5512, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 7.4603, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.0043, Accuracy: 0.6875\n",
      "Fold: 1, Train Loss: 8.0504, Train Accuracy: 0.7107, Val Loss: 8.1096, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 16.1741, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 9.3679, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 7.8483, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 7.7944, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.1568, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 7.5931, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.8468, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.5786, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 8.2344, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.0948, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 9.4822, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 7.2956, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 7.9574, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 8.1849, Accuracy: 0.7188\n",
      "Fold: 2, Train Loss: 8.0169, Train Accuracy: 0.7136, Val Loss: 8.2873, Val Accuracy: 0.6942\n",
      "Iteration 0 - Loss: 16.8534, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 8.7117, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 6.6978, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 7.1218, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 8.6186, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 10.0522, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 9.0413, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.3738, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 6.1363, Accuracy: 0.8750\n",
      "Iteration 4500 - Loss: 8.8610, Accuracy: 0.6250\n",
      "Iteration 5000 - Loss: 9.1371, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.5012, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 6.7357, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.0261, Accuracy: 0.7500\n",
      "Fold: 3, Train Loss: 8.1682, Train Accuracy: 0.7057, Val Loss: 7.7529, Val Accuracy: 0.6876\n",
      "Iteration 0 - Loss: 13.8114, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 7.5409, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.4497, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 6.7108, Accuracy: 0.7812\n",
      "Iteration 2000 - Loss: 8.5908, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.2274, Accuracy: 0.5625\n",
      "Iteration 3000 - Loss: 7.4210, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 5.4358, Accuracy: 0.8750\n",
      "Iteration 4000 - Loss: 6.9756, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.0004, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.7816, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 6.6715, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.4929, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 7.0651, Accuracy: 0.7500\n",
      "Fold: 4, Train Loss: 8.0827, Train Accuracy: 0.7087, Val Loss: 8.0171, Val Accuracy: 0.6829\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.86       0.72       0.79      \n",
      "1          0.59       0.79       0.67      \n",
      "2          0.54       0.38       0.45      \n",
      "3          0.74       0.70       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6600 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/24247c1332824905b3c68c7fa0ef6106\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 14.8920, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 8.2152, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 7.1862, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 8.0606, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.9623, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 8.5947, Accuracy: 0.5938\n",
      "Iteration 3000 - Loss: 7.8289, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.9042, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.2112, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 8.5643, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.7274, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 6.3692, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.3874, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 6.6549, Accuracy: 0.7812\n",
      "Fold: 0, Train Loss: 8.1131, Train Accuracy: 0.7031, Val Loss: 7.9876, Val Accuracy: 0.7148\n",
      "Iteration 0 - Loss: 17.3783, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 7.0905, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.2485, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 7.2930, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 8.1906, Accuracy: 0.7656\n",
      "Iteration 2500 - Loss: 8.4824, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.8539, Accuracy: 0.6406\n",
      "Iteration 3500 - Loss: 8.5475, Accuracy: 0.6094\n",
      "Iteration 4000 - Loss: 9.1676, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.5496, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 6.8893, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 8.1675, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.8710, Accuracy: 0.7969\n",
      "Iteration 6500 - Loss: 7.9694, Accuracy: 0.6562\n",
      "Fold: 1, Train Loss: 8.0104, Train Accuracy: 0.7101, Val Loss: 8.1578, Val Accuracy: 0.7073\n",
      "Iteration 0 - Loss: 14.5629, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 7.9273, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.1697, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.8860, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 7.4765, Accuracy: 0.7344\n",
      "Iteration 2500 - Loss: 7.1846, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.0751, Accuracy: 0.8281\n",
      "Iteration 3500 - Loss: 7.0447, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.1493, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 6.7738, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 7.2777, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 7.6655, Accuracy: 0.8281\n",
      "Iteration 6000 - Loss: 8.7892, Accuracy: 0.6094\n",
      "Iteration 6500 - Loss: 7.8632, Accuracy: 0.6719\n",
      "Fold: 2, Train Loss: 7.9592, Train Accuracy: 0.7126, Val Loss: 8.3791, Val Accuracy: 0.6576\n",
      "Iteration 0 - Loss: 16.5516, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 8.0695, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.8893, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 7.7612, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 6.5365, Accuracy: 0.8125\n",
      "Iteration 2500 - Loss: 6.8059, Accuracy: 0.7857\n",
      "Iteration 3000 - Loss: 8.0868, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.7368, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.0021, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 9.1124, Accuracy: 0.6250\n",
      "Iteration 5000 - Loss: 7.3478, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 8.8776, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.0314, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.4850, Accuracy: 0.7969\n",
      "Fold: 3, Train Loss: 8.0842, Train Accuracy: 0.7084, Val Loss: 7.9175, Val Accuracy: 0.6848\n",
      "Iteration 0 - Loss: 16.9269, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 8.2340, Accuracy: 0.6406\n",
      "Iteration 1000 - Loss: 8.5890, Accuracy: 0.6094\n",
      "Iteration 1500 - Loss: 6.8634, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 7.7858, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.3473, Accuracy: 0.6719\n",
      "Iteration 3000 - Loss: 8.3042, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.1064, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 9.1633, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.1913, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 7.0764, Accuracy: 0.8281\n",
      "Iteration 5500 - Loss: 7.9321, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.6878, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.1057, Accuracy: 0.6719\n",
      "Fold: 4, Train Loss: 8.0688, Train Accuracy: 0.7078, Val Loss: 8.4215, Val Accuracy: 0.6604\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.70       0.88       0.78      \n",
      "1          0.70       0.48       0.57      \n",
      "2          0.50       0.75       0.60      \n",
      "3          0.87       0.46       0.60      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6400 | F1-Score: 0.6400\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/ec40e07d532943adbd01dd4425808516\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 13.4581, Accuracy: 0.5078\n",
      "Iteration 500 - Loss: 8.6881, Accuracy: 0.7109\n",
      "Iteration 1000 - Loss: 8.4124, Accuracy: 0.6172\n",
      "Iteration 1500 - Loss: 8.2720, Accuracy: 0.7578\n",
      "Iteration 2000 - Loss: 7.6792, Accuracy: 0.7734\n",
      "Iteration 2500 - Loss: 7.4314, Accuracy: 0.7656\n",
      "Iteration 3000 - Loss: 6.5873, Accuracy: 0.8047\n",
      "Iteration 3500 - Loss: 7.8849, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 8.6310, Accuracy: 0.6406\n",
      "Iteration 4500 - Loss: 8.4796, Accuracy: 0.6406\n",
      "Iteration 5000 - Loss: 8.9451, Accuracy: 0.6484\n",
      "Iteration 5500 - Loss: 7.7514, Accuracy: 0.7422\n",
      "Iteration 6000 - Loss: 8.0156, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.3944, Accuracy: 0.6641\n",
      "Fold: 0, Train Loss: 8.1074, Train Accuracy: 0.7001, Val Loss: 7.9016, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.1025, Accuracy: 0.4609\n",
      "Iteration 500 - Loss: 7.1484, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 7.7844, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 8.0455, Accuracy: 0.7818\n",
      "Iteration 2000 - Loss: 8.2810, Accuracy: 0.6797\n",
      "Iteration 2500 - Loss: 8.1812, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 8.4307, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.8522, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.2511, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 7.0989, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 7.4721, Accuracy: 0.7109\n",
      "Iteration 5500 - Loss: 8.3787, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.7430, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.4252, Accuracy: 0.7578\n",
      "Fold: 1, Train Loss: 7.9971, Train Accuracy: 0.7068, Val Loss: 8.2494, Val Accuracy: 0.6754\n",
      "Iteration 0 - Loss: 15.4581, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 7.5794, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.5090, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 7.4324, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.5353, Accuracy: 0.6641\n",
      "Iteration 2500 - Loss: 7.6599, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 7.7451, Accuracy: 0.7422\n",
      "Iteration 3500 - Loss: 7.8242, Accuracy: 0.7578\n",
      "Iteration 4000 - Loss: 7.5611, Accuracy: 0.7266\n",
      "Iteration 4500 - Loss: 8.6399, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 8.2745, Accuracy: 0.7473\n",
      "Iteration 5500 - Loss: 7.8149, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 7.7507, Accuracy: 0.7266\n",
      "Iteration 6500 - Loss: 7.8283, Accuracy: 0.7031\n",
      "Fold: 2, Train Loss: 7.9661, Train Accuracy: 0.7099, Val Loss: 8.0274, Val Accuracy: 0.6829\n",
      "Iteration 0 - Loss: 16.5271, Accuracy: 0.3672\n",
      "Iteration 500 - Loss: 8.4816, Accuracy: 0.6765\n",
      "Iteration 1000 - Loss: 7.7891, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 8.2683, Accuracy: 0.7109\n",
      "Iteration 2000 - Loss: 7.9783, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 7.9265, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 7.1443, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 7.9280, Accuracy: 0.6641\n",
      "Iteration 4000 - Loss: 7.1627, Accuracy: 0.7109\n",
      "Iteration 4500 - Loss: 8.2149, Accuracy: 0.6953\n",
      "Iteration 5000 - Loss: 8.2479, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.4842, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 8.9081, Accuracy: 0.7109\n",
      "Iteration 6500 - Loss: 8.4505, Accuracy: 0.6719\n",
      "Fold: 3, Train Loss: 8.0868, Train Accuracy: 0.7047, Val Loss: 7.8256, Val Accuracy: 0.7092\n",
      "Iteration 0 - Loss: 15.3574, Accuracy: 0.4545\n",
      "Iteration 500 - Loss: 8.0813, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.5843, Accuracy: 0.6641\n",
      "Iteration 1500 - Loss: 8.5808, Accuracy: 0.7109\n",
      "Iteration 2000 - Loss: 7.5551, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 7.9137, Accuracy: 0.7578\n",
      "Iteration 3000 - Loss: 8.0588, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 8.0883, Accuracy: 0.6719\n",
      "Iteration 4000 - Loss: 7.6430, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 7.8822, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 7.7226, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 7.9348, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 7.8903, Accuracy: 0.7045\n",
      "Iteration 6500 - Loss: 8.6776, Accuracy: 0.6328\n",
      "Fold: 4, Train Loss: 8.0480, Train Accuracy: 0.7059, Val Loss: 7.8741, Val Accuracy: 0.6914\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.86       0.76       0.81      \n",
      "1          0.60       0.82       0.69      \n",
      "2          0.52       0.43       0.47      \n",
      "3          0.84       0.54       0.66      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/02a66f67534241b2a4389734f48fff85\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 16.0322, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 9.2486, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 10.9722, Accuracy: 0.5312\n",
      "Iteration 1500 - Loss: 8.6501, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.5841, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.9799, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.5128, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 7.9301, Accuracy: 0.8750\n",
      "Iteration 4000 - Loss: 6.3687, Accuracy: 0.8438\n",
      "Iteration 4500 - Loss: 9.4007, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 6.8880, Accuracy: 0.8438\n",
      "Iteration 5500 - Loss: 8.8893, Accuracy: 0.5938\n",
      "Iteration 6000 - Loss: 8.5946, Accuracy: 0.5938\n",
      "Iteration 6500 - Loss: 8.4620, Accuracy: 0.6562\n",
      "Fold: 0, Train Loss: 8.5781, Train Accuracy: 0.7082, Val Loss: 7.9379, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 15.0109, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 8.5356, Accuracy: 0.8438\n",
      "Iteration 1000 - Loss: 9.3333, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 9.7912, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 7.0013, Accuracy: 0.8438\n",
      "Iteration 2500 - Loss: 8.1908, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.2324, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.3474, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 8.1992, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 9.2193, Accuracy: 0.5000\n",
      "Iteration 5000 - Loss: 8.4066, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 7.4369, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.2684, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 7.8696, Accuracy: 0.7500\n",
      "Fold: 1, Train Loss: 8.4669, Train Accuracy: 0.7138, Val Loss: 8.3207, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 14.7251, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 9.2208, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 9.5933, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 9.3963, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 8.4236, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.1101, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.9588, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 7.0383, Accuracy: 0.8125\n",
      "Iteration 4000 - Loss: 7.4306, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 7.7747, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.1420, Accuracy: 0.6250\n",
      "Iteration 5500 - Loss: 8.8407, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 7.3334, Accuracy: 0.8125\n",
      "Iteration 6500 - Loss: 6.3324, Accuracy: 0.8438\n",
      "Fold: 2, Train Loss: 8.4686, Train Accuracy: 0.7144, Val Loss: 8.2927, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 14.9630, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 8.9075, Accuracy: 0.7812\n",
      "Iteration 1000 - Loss: 8.9851, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 9.2528, Accuracy: 0.8438\n",
      "Iteration 2000 - Loss: 8.7076, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.0701, Accuracy: 0.8750\n",
      "Iteration 3000 - Loss: 6.6169, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 9.6993, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 8.2880, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 9.4728, Accuracy: 0.5312\n",
      "Iteration 5000 - Loss: 6.6505, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 7.4232, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.4616, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.1498, Accuracy: 0.6875\n",
      "Fold: 3, Train Loss: 8.6247, Train Accuracy: 0.7084, Val Loss: 7.9929, Val Accuracy: 0.7111\n",
      "Iteration 0 - Loss: 16.8037, Accuracy: 0.1250\n",
      "Iteration 500 - Loss: 10.6698, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 7.7061, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 9.5496, Accuracy: 0.5312\n",
      "Iteration 2000 - Loss: 8.9670, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.9848, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.0553, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 7.9920, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 6.8221, Accuracy: 0.8438\n",
      "Iteration 4500 - Loss: 7.0713, Accuracy: 0.8125\n",
      "Iteration 5000 - Loss: 6.9868, Accuracy: 0.8438\n",
      "Iteration 5500 - Loss: 9.4522, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 6.3601, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 8.3027, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 8.5468, Train Accuracy: 0.7090, Val Loss: 8.1396, Val Accuracy: 0.7176\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.75       0.80      \n",
      "1          0.63       0.72       0.68      \n",
      "2          0.55       0.57       0.56      \n",
      "3          0.82       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/2e745c2c712c49008454f482018401e1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.8248, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 9.5490, Accuracy: 0.5938\n",
      "Iteration 1000 - Loss: 7.4933, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 8.6337, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 9.2516, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.8998, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.8366, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 9.8811, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 7.1287, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 8.1796, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.1154, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 7.6514, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.3492, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.9028, Accuracy: 0.6562\n",
      "Fold: 0, Train Loss: 8.3667, Train Accuracy: 0.7101, Val Loss: 7.9014, Val Accuracy: 0.7195\n",
      "Iteration 0 - Loss: 16.4011, Accuracy: 0.1094\n",
      "Iteration 500 - Loss: 8.4555, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.7908, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 8.3359, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 7.4130, Accuracy: 0.7344\n",
      "Iteration 2500 - Loss: 7.9251, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.3083, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 7.4192, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 9.0522, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 8.0283, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.7993, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 7.0469, Accuracy: 0.8438\n",
      "Iteration 6000 - Loss: 7.4445, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 7.0621, Accuracy: 0.7812\n",
      "Fold: 1, Train Loss: 8.2460, Train Accuracy: 0.7166, Val Loss: 8.2125, Val Accuracy: 0.6914\n",
      "Iteration 0 - Loss: 14.4335, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 8.4313, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.7600, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 7.3978, Accuracy: 0.7812\n",
      "Iteration 2000 - Loss: 8.2296, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 7.7901, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 8.4358, Accuracy: 0.6719\n",
      "Iteration 3500 - Loss: 8.0340, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 6.7481, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 9.0562, Accuracy: 0.6406\n",
      "Iteration 5000 - Loss: 7.4907, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 7.8461, Accuracy: 0.7969\n",
      "Iteration 6000 - Loss: 8.4123, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 7.8756, Accuracy: 0.7812\n",
      "Fold: 2, Train Loss: 8.2339, Train Accuracy: 0.7170, Val Loss: 8.2667, Val Accuracy: 0.6998\n",
      "Iteration 0 - Loss: 15.8599, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 9.1843, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.7346, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 8.6373, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.7102, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.6295, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.6874, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 7.1620, Accuracy: 0.8281\n",
      "Iteration 4000 - Loss: 7.8656, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 7.9667, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 7.1508, Accuracy: 0.8125\n",
      "Iteration 5500 - Loss: 8.5609, Accuracy: 0.6406\n",
      "Iteration 6000 - Loss: 7.4613, Accuracy: 0.7969\n",
      "Iteration 6500 - Loss: 7.7689, Accuracy: 0.7500\n",
      "Fold: 3, Train Loss: 8.3624, Train Accuracy: 0.7131, Val Loss: 7.9439, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 15.4037, Accuracy: 0.2969\n",
      "Iteration 500 - Loss: 9.2721, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 7.3520, Accuracy: 0.8906\n",
      "Iteration 1500 - Loss: 8.4415, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 8.8189, Accuracy: 0.6406\n",
      "Iteration 2500 - Loss: 8.3701, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.8733, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.5047, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.2774, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 7.3794, Accuracy: 0.7656\n",
      "Iteration 5000 - Loss: 8.3336, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 6.8092, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.5407, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 8.1345, Accuracy: 0.6250\n",
      "Fold: 4, Train Loss: 8.3089, Train Accuracy: 0.7113, Val Loss: 8.0525, Val Accuracy: 0.7120\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.82       0.82      \n",
      "1          0.64       0.70       0.67      \n",
      "2          0.54       0.54       0.54      \n",
      "3          0.81       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/d67ef0dc22d1448b8176b76590102957\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.7822, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 8.4825, Accuracy: 0.6797\n",
      "Iteration 1000 - Loss: 9.0063, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 8.8510, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.6771, Accuracy: 0.6641\n",
      "Iteration 2500 - Loss: 8.2602, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 8.1297, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.1791, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 8.2258, Accuracy: 0.6953\n",
      "Iteration 4500 - Loss: 7.9635, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.1914, Accuracy: 0.7429\n",
      "Iteration 5500 - Loss: 8.3352, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 8.2673, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 8.5399, Accuracy: 0.7266\n",
      "Fold: 0, Train Loss: 8.2226, Train Accuracy: 0.7128, Val Loss: 7.9901, Val Accuracy: 0.7251\n",
      "Iteration 0 - Loss: 15.5390, Accuracy: 0.2344\n",
      "Iteration 500 - Loss: 8.9608, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 8.9670, Accuracy: 0.6953\n",
      "Iteration 1500 - Loss: 7.7744, Accuracy: 0.7891\n",
      "Iteration 2000 - Loss: 8.1373, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 7.6065, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.7329, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.5611, Accuracy: 0.6953\n",
      "Iteration 4000 - Loss: 8.5314, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 8.5870, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 7.4877, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 7.2591, Accuracy: 0.7578\n",
      "Iteration 6000 - Loss: 8.4503, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 7.3539, Accuracy: 0.7422\n",
      "Fold: 1, Train Loss: 8.1091, Train Accuracy: 0.7196, Val Loss: 8.2517, Val Accuracy: 0.7167\n",
      "Iteration 0 - Loss: 14.2947, Accuracy: 0.4062\n",
      "Iteration 500 - Loss: 8.6848, Accuracy: 0.7109\n",
      "Iteration 1000 - Loss: 7.5431, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 8.1196, Accuracy: 0.7422\n",
      "Iteration 2000 - Loss: 7.9052, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 7.8393, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.0567, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.0912, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 7.2412, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.5580, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 8.2982, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 7.4997, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.1837, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.1730, Accuracy: 0.6923\n",
      "Fold: 2, Train Loss: 8.0743, Train Accuracy: 0.7200, Val Loss: 8.3345, Val Accuracy: 0.6942\n",
      "Iteration 0 - Loss: 15.9561, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 8.8998, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 8.0409, Accuracy: 0.7422\n",
      "Iteration 1500 - Loss: 8.8137, Accuracy: 0.6953\n",
      "Iteration 2000 - Loss: 8.6358, Accuracy: 0.6719\n",
      "Iteration 2500 - Loss: 7.8952, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 8.5135, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 8.4084, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.8417, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 8.3732, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 8.8349, Accuracy: 0.6328\n",
      "Iteration 5500 - Loss: 8.4574, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.1012, Accuracy: 0.7266\n",
      "Iteration 6500 - Loss: 8.5602, Accuracy: 0.6953\n",
      "Fold: 3, Train Loss: 8.2149, Train Accuracy: 0.7154, Val Loss: 7.8769, Val Accuracy: 0.7186\n",
      "Iteration 0 - Loss: 16.2650, Accuracy: 0.2656\n",
      "Iteration 500 - Loss: 8.8814, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.2843, Accuracy: 0.7578\n",
      "Iteration 1500 - Loss: 8.6078, Accuracy: 0.6406\n",
      "Iteration 2000 - Loss: 8.4534, Accuracy: 0.6953\n",
      "Iteration 2500 - Loss: 7.5090, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 7.5254, Accuracy: 0.7422\n",
      "Iteration 3500 - Loss: 7.7502, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 7.8603, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 7.2026, Accuracy: 0.8047\n",
      "Iteration 5000 - Loss: 8.4120, Accuracy: 0.7266\n",
      "Iteration 5500 - Loss: 7.3826, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 8.4055, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 7.3743, Accuracy: 0.7656\n",
      "Fold: 4, Train Loss: 8.1561, Train Accuracy: 0.7141, Val Loss: 8.0847, Val Accuracy: 0.7158\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.80       0.81      \n",
      "1          0.66       0.68       0.67      \n",
      "2          0.55       0.60       0.57      \n",
      "3          0.82       0.65       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/3bb50accfc97414d9673d88108911a9d\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 16.2907, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 12.4455, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 11.8484, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 10.6781, Accuracy: 0.8125\n",
      "Iteration 2000 - Loss: 10.0166, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.6996, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 10.3356, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 9.2653, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 8.6885, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.5803, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 9.0380, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 9.8530, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.3263, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.6549, Accuracy: 0.8125\n",
      "Fold: 0, Train Loss: 10.2970, Train Accuracy: 0.6631, Val Loss: 9.1293, Val Accuracy: 0.7280\n",
      "Iteration 0 - Loss: 15.4537, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 11.4214, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 10.5695, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 10.7510, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 9.3591, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 11.2006, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.2388, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 9.2147, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 10.2454, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.5349, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 11.0300, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 9.2117, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.3183, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 10.4042, Accuracy: 0.5312\n",
      "Fold: 1, Train Loss: 10.1149, Train Accuracy: 0.6740, Val Loss: 9.5436, Val Accuracy: 0.6754\n",
      "Iteration 0 - Loss: 14.5090, Accuracy: 0.3438\n",
      "Iteration 500 - Loss: 11.5193, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 10.8496, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 10.3230, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 10.4623, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 9.1454, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 9.8455, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 10.4364, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 11.3621, Accuracy: 0.5312\n",
      "Iteration 4500 - Loss: 9.6037, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.7701, Accuracy: 0.8125\n",
      "Iteration 5500 - Loss: 9.4664, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.5280, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 9.0313, Accuracy: 0.7500\n",
      "Fold: 2, Train Loss: 10.1077, Train Accuracy: 0.6719, Val Loss: 9.4474, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 16.2806, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 12.5389, Accuracy: 0.4688\n",
      "Iteration 1000 - Loss: 11.5952, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 10.3631, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 10.2080, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 11.5540, Accuracy: 0.5625\n",
      "Iteration 3000 - Loss: 10.2921, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 9.7252, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 9.3447, Accuracy: 0.7812\n",
      "Iteration 4500 - Loss: 10.0381, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.3078, Accuracy: 0.8438\n",
      "Iteration 5500 - Loss: 10.3030, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 9.8832, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.5775, Accuracy: 0.8000\n",
      "Fold: 3, Train Loss: 10.4472, Train Accuracy: 0.6577, Val Loss: 9.1228, Val Accuracy: 0.7111\n",
      "Iteration 0 - Loss: 15.2570, Accuracy: 0.3750\n",
      "Iteration 500 - Loss: 12.4434, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 11.2494, Accuracy: 0.4688\n",
      "Iteration 1500 - Loss: 11.2166, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 10.5079, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 9.1957, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 9.0529, Accuracy: 0.9062\n",
      "Iteration 3500 - Loss: 8.4829, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 8.4810, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 8.0332, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.2708, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 10.2239, Accuracy: 0.5625\n",
      "Iteration 6000 - Loss: 10.0075, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 9.1611, Accuracy: 0.7188\n",
      "Fold: 4, Train Loss: 10.1895, Train Accuracy: 0.6745, Val Loss: 9.2645, Val Accuracy: 0.7008\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.63       0.72       0.67      \n",
      "2          0.50       0.49       0.49      \n",
      "3          0.75       0.57       0.65      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6600 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/4be62dc29ce849619b980c2a90a21aa7\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 14.4026, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.5745, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 10.9319, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 9.7272, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 9.6419, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 9.3569, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 9.5113, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 10.2415, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.8548, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 9.0051, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.7946, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 9.5713, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.1467, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.2839, Accuracy: 0.7812\n",
      "Fold: 0, Train Loss: 9.6435, Train Accuracy: 0.6842, Val Loss: 8.6508, Val Accuracy: 0.7336\n",
      "Iteration 0 - Loss: 15.1421, Accuracy: 0.3594\n",
      "Iteration 500 - Loss: 10.5122, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 10.4386, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 10.3022, Accuracy: 0.6406\n",
      "Iteration 2000 - Loss: 9.7308, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.7303, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.5930, Accuracy: 0.6406\n",
      "Iteration 3500 - Loss: 9.3081, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 9.8588, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 7.9694, Accuracy: 0.8594\n",
      "Iteration 5000 - Loss: 7.7956, Accuracy: 0.7969\n",
      "Iteration 5500 - Loss: 8.7220, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 9.2041, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 8.1202, Accuracy: 0.8281\n",
      "Fold: 1, Train Loss: 9.5429, Train Accuracy: 0.6922, Val Loss: 9.0396, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 16.4294, Accuracy: 0.1719\n",
      "Iteration 500 - Loss: 10.2223, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 10.8715, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 9.7816, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 9.9587, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 9.5922, Accuracy: 0.6786\n",
      "Iteration 3000 - Loss: 9.4552, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 10.0257, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 9.1198, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 10.2151, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.2938, Accuracy: 0.6094\n",
      "Iteration 5500 - Loss: 8.0290, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.3833, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 9.0874, Accuracy: 0.7031\n",
      "Fold: 2, Train Loss: 9.5969, Train Accuracy: 0.6856, Val Loss: 9.0016, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 16.2894, Accuracy: 0.1562\n",
      "Iteration 500 - Loss: 11.8229, Accuracy: 0.6406\n",
      "Iteration 1000 - Loss: 10.3180, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.1686, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 9.6710, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 9.6717, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.8479, Accuracy: 0.7656\n",
      "Iteration 3500 - Loss: 8.4151, Accuracy: 0.7969\n",
      "Iteration 4000 - Loss: 9.8944, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.7282, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.9888, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 8.9430, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 9.1241, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 9.3949, Accuracy: 0.7031\n",
      "Fold: 3, Train Loss: 9.7028, Train Accuracy: 0.6857, Val Loss: 8.6456, Val Accuracy: 0.7158\n",
      "Iteration 0 - Loss: 16.0311, Accuracy: 0.2344\n",
      "Iteration 500 - Loss: 10.6452, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 10.5090, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.3124, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.1077, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 9.9522, Accuracy: 0.6250\n",
      "Iteration 3000 - Loss: 8.6000, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 9.2593, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 9.1447, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.1084, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.8029, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 9.4682, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 8.5517, Accuracy: 0.7969\n",
      "Iteration 6500 - Loss: 8.9994, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 9.5654, Train Accuracy: 0.6918, Val Loss: 8.7824, Val Accuracy: 0.7045\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.63       0.73       0.68      \n",
      "2          0.53       0.53       0.53      \n",
      "3          0.79       0.60       0.68      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/5adabb22fc4e46f8bea268d9e3f65a5e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | xavier | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 16.6674, Accuracy: 0.1094\n",
      "Iteration 500 - Loss: 10.1788, Accuracy: 0.6797\n",
      "Iteration 1000 - Loss: 9.3205, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 9.2692, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 9.6477, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 9.5994, Accuracy: 0.6172\n",
      "Iteration 3000 - Loss: 8.8594, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.9145, Accuracy: 0.7266\n",
      "Iteration 4000 - Loss: 7.8767, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.2307, Accuracy: 0.7578\n",
      "Iteration 5000 - Loss: 8.4913, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 9.2429, Accuracy: 0.6406\n",
      "Iteration 6000 - Loss: 8.7720, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 8.5862, Accuracy: 0.6797\n",
      "Fold: 0, Train Loss: 9.1258, Train Accuracy: 0.6988, Val Loss: 8.2692, Val Accuracy: 0.7355\n",
      "Iteration 0 - Loss: 15.3018, Accuracy: 0.2969\n",
      "Iteration 500 - Loss: 10.1944, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 9.3323, Accuracy: 0.7344\n",
      "Iteration 1500 - Loss: 8.9208, Accuracy: 0.7266\n",
      "Iteration 2000 - Loss: 9.0305, Accuracy: 0.6953\n",
      "Iteration 2500 - Loss: 8.6505, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.2528, Accuracy: 0.7422\n",
      "Iteration 3500 - Loss: 7.5773, Accuracy: 0.8047\n",
      "Iteration 4000 - Loss: 8.2466, Accuracy: 0.7734\n",
      "Iteration 4500 - Loss: 9.4801, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 9.0126, Accuracy: 0.7266\n",
      "Iteration 5500 - Loss: 7.5134, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.5907, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.6898, Accuracy: 0.8281\n",
      "Fold: 1, Train Loss: 8.9830, Train Accuracy: 0.7080, Val Loss: 8.6101, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.9365, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 11.0723, Accuracy: 0.5703\n",
      "Iteration 1000 - Loss: 10.2602, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 9.5373, Accuracy: 0.6933\n",
      "Iteration 2000 - Loss: 9.7917, Accuracy: 0.6328\n",
      "Iteration 2500 - Loss: 9.1370, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.8589, Accuracy: 0.7414\n",
      "Iteration 3500 - Loss: 8.4999, Accuracy: 0.6953\n",
      "Iteration 4000 - Loss: 8.2442, Accuracy: 0.7734\n",
      "Iteration 4500 - Loss: 8.1004, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 8.7128, Accuracy: 0.7109\n",
      "Iteration 5500 - Loss: 7.7045, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 7.9175, Accuracy: 0.7891\n",
      "Iteration 6500 - Loss: 7.8697, Accuracy: 0.7500\n",
      "Fold: 2, Train Loss: 9.0320, Train Accuracy: 0.7039, Val Loss: 8.5968, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 15.7576, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 10.2820, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 10.1482, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 9.9344, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.8644, Accuracy: 0.6797\n",
      "Iteration 2500 - Loss: 8.8164, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.0624, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 9.7319, Accuracy: 0.5781\n",
      "Iteration 4000 - Loss: 8.7831, Accuracy: 0.7155\n",
      "Iteration 4500 - Loss: 8.8804, Accuracy: 0.6797\n",
      "Iteration 5000 - Loss: 8.5502, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 9.1689, Accuracy: 0.6406\n",
      "Iteration 6000 - Loss: 8.0101, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 7.7093, Accuracy: 0.7969\n",
      "Fold: 3, Train Loss: 9.1343, Train Accuracy: 0.7004, Val Loss: 8.2253, Val Accuracy: 0.7223\n",
      "Iteration 0 - Loss: 15.2127, Accuracy: 0.3359\n",
      "Iteration 500 - Loss: 10.2217, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 9.3980, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 9.0814, Accuracy: 0.7266\n",
      "Iteration 2000 - Loss: 8.8250, Accuracy: 0.7266\n",
      "Iteration 2500 - Loss: 8.6360, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 8.9009, Accuracy: 0.7377\n",
      "Iteration 3500 - Loss: 9.2335, Accuracy: 0.6719\n",
      "Iteration 4000 - Loss: 9.1337, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 7.8525, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.4670, Accuracy: 0.7266\n",
      "Iteration 5500 - Loss: 7.9809, Accuracy: 0.7578\n",
      "Iteration 6000 - Loss: 8.7073, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.2168, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 9.0448, Train Accuracy: 0.7042, Val Loss: 8.3984, Val Accuracy: 0.7073\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.63       0.73       0.68      \n",
      "2          0.55       0.52       0.53      \n",
      "3          0.81       0.64       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/3f4cb1ad856347c288eeba3760d366da\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 12.3062, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 7.3287, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 6.9323, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 8.8157, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 8.8492, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 8.5257, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.4475, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.6399, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.0612, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.1633, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.7033, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 7.7870, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 7.7467, Accuracy: 0.8125\n",
      "Iteration 6500 - Loss: 8.5782, Accuracy: 0.5938\n",
      "Fold: 0, Train Loss: 8.1508, Train Accuracy: 0.7039, Val Loss: 7.9389, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.1478, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 8.2813, Accuracy: 0.5625\n",
      "Iteration 1000 - Loss: 7.4642, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 9.4926, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 7.7833, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.0578, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 5.5823, Accuracy: 0.8750\n",
      "Iteration 3500 - Loss: 9.4350, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 6.9438, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 7.5434, Accuracy: 0.8125\n",
      "Iteration 5000 - Loss: 9.0021, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 6.3188, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 6.9089, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 7.7353, Accuracy: 0.6875\n",
      "Fold: 1, Train Loss: 8.0449, Train Accuracy: 0.7100, Val Loss: 8.3646, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 17.6013, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 8.2447, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.1802, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 8.1398, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.3370, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 9.2796, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 7.6736, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 9.4324, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 7.9737, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 9.5010, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.6172, Accuracy: 0.6250\n",
      "Iteration 5500 - Loss: 7.7400, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 7.5529, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 6.8425, Accuracy: 0.7812\n",
      "Fold: 2, Train Loss: 8.0134, Train Accuracy: 0.7141, Val Loss: 8.2839, Val Accuracy: 0.6735\n",
      "Iteration 0 - Loss: 14.8307, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 7.6491, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.5129, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 8.2746, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 7.7784, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 7.2896, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 7.5982, Accuracy: 0.8438\n",
      "Iteration 3500 - Loss: 7.8410, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 6.1961, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 7.7546, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.4512, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 5.6953, Accuracy: 0.8438\n",
      "Iteration 6000 - Loss: 8.2960, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 7.6969, Accuracy: 0.7188\n",
      "Fold: 3, Train Loss: 8.1115, Train Accuracy: 0.7096, Val Loss: 7.7903, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 15.2186, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 7.0702, Accuracy: 0.8125\n",
      "Iteration 1000 - Loss: 7.1221, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 10.0473, Accuracy: 0.5625\n",
      "Iteration 2000 - Loss: 7.7301, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 7.6575, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.1560, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.3668, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 9.4041, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 9.5779, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 10.1222, Accuracy: 0.5312\n",
      "Iteration 5500 - Loss: 7.0505, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.8154, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 10.3717, Accuracy: 0.5000\n",
      "Fold: 4, Train Loss: 8.1248, Train Accuracy: 0.7064, Val Loss: 8.0563, Val Accuracy: 0.6801\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.87       0.70       0.78      \n",
      "1          0.63       0.74       0.68      \n",
      "2          0.53       0.59       0.56      \n",
      "3          0.81       0.59       0.68      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/783f9f3f88044effb087e661f1274a48\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 16.5932, Accuracy: 0.3438\n",
      "Iteration 500 - Loss: 7.8613, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.3622, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 8.9835, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 7.6229, Accuracy: 0.7969\n",
      "Iteration 2500 - Loss: 7.4526, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.3748, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 7.8507, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.7154, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3406, Accuracy: 0.6406\n",
      "Iteration 5000 - Loss: 8.4205, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 6.9144, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 7.2422, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 6.8583, Accuracy: 0.7500\n",
      "Fold: 0, Train Loss: 8.1319, Train Accuracy: 0.7034, Val Loss: 7.7599, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 13.4292, Accuracy: 0.4219\n",
      "Iteration 500 - Loss: 7.0174, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.0376, Accuracy: 0.7344\n",
      "Iteration 1500 - Loss: 8.6852, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.4137, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 8.9138, Accuracy: 0.6406\n",
      "Iteration 3000 - Loss: 6.8798, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 7.8513, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 8.5125, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 8.5442, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.9610, Accuracy: 0.5625\n",
      "Iteration 5500 - Loss: 7.5466, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 7.5972, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 8.5233, Accuracy: 0.6719\n",
      "Fold: 1, Train Loss: 8.0075, Train Accuracy: 0.7096, Val Loss: 8.3009, Val Accuracy: 0.6839\n",
      "Iteration 0 - Loss: 14.6990, Accuracy: 0.3125\n",
      "Iteration 500 - Loss: 6.8562, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.1497, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 7.7459, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.5441, Accuracy: 0.6406\n",
      "Iteration 2500 - Loss: 7.9696, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.1169, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.7070, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.2996, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 7.3936, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 6.9370, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 6.8686, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.8920, Accuracy: 0.5938\n",
      "Iteration 6500 - Loss: 8.0123, Accuracy: 0.7500\n",
      "Fold: 2, Train Loss: 7.9724, Train Accuracy: 0.7121, Val Loss: 8.1379, Val Accuracy: 0.6923\n",
      "Iteration 0 - Loss: 14.5651, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 7.9062, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 7.9397, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.4122, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.5513, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 9.0671, Accuracy: 0.6250\n",
      "Iteration 3000 - Loss: 8.6125, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.6962, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 7.7794, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 7.4362, Accuracy: 0.7656\n",
      "Iteration 5000 - Loss: 7.8678, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 7.5096, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 7.2042, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 7.9954, Accuracy: 0.7500\n",
      "Fold: 3, Train Loss: 8.1065, Train Accuracy: 0.7071, Val Loss: 8.1141, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 17.3315, Accuracy: 0.1562\n",
      "Iteration 500 - Loss: 8.9380, Accuracy: 0.6406\n",
      "Iteration 1000 - Loss: 7.8300, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.3813, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.6288, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.5716, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 9.5154, Accuracy: 0.5781\n",
      "Iteration 3500 - Loss: 7.2644, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 7.0180, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 8.8824, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 7.9483, Accuracy: 0.6406\n",
      "Iteration 5500 - Loss: 7.2257, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 6.6861, Accuracy: 0.7969\n",
      "Iteration 6500 - Loss: 9.0065, Accuracy: 0.6562\n",
      "Fold: 4, Train Loss: 8.0636, Train Accuracy: 0.7081, Val Loss: 8.5319, Val Accuracy: 0.6651\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.67       0.75      \n",
      "1          0.65       0.64       0.64      \n",
      "2          0.53       0.72       0.61      \n",
      "3          0.81       0.67       0.73      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/a4c04f8d9796491aa3069b225910aa0e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 16.3477, Accuracy: 0.2656\n",
      "Iteration 500 - Loss: 7.9601, Accuracy: 0.6797\n",
      "Iteration 1000 - Loss: 8.0325, Accuracy: 0.7422\n",
      "Iteration 1500 - Loss: 8.2138, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 8.5736, Accuracy: 0.6016\n",
      "Iteration 2500 - Loss: 7.7643, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 8.1811, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 8.3205, Accuracy: 0.6719\n",
      "Iteration 4000 - Loss: 8.8509, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 7.4192, Accuracy: 0.7109\n",
      "Iteration 5000 - Loss: 9.7698, Accuracy: 0.7109\n",
      "Iteration 5500 - Loss: 7.9042, Accuracy: 0.7422\n",
      "Iteration 6000 - Loss: 7.3029, Accuracy: 0.7656\n",
      "Iteration 6500 - Loss: 8.9878, Accuracy: 0.6641\n",
      "Fold: 0, Train Loss: 8.1165, Train Accuracy: 0.7007, Val Loss: 7.9408, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 16.9521, Accuracy: 0.2109\n",
      "Iteration 500 - Loss: 7.4552, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.6373, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 8.6018, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 8.4332, Accuracy: 0.7266\n",
      "Iteration 2500 - Loss: 8.0903, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.6803, Accuracy: 0.6797\n",
      "Iteration 3500 - Loss: 7.7789, Accuracy: 0.6328\n",
      "Iteration 4000 - Loss: 8.4796, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 7.3810, Accuracy: 0.7266\n",
      "Iteration 5000 - Loss: 6.8661, Accuracy: 0.7422\n",
      "Iteration 5500 - Loss: 8.1729, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 9.0722, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 7.7360, Accuracy: 0.7188\n",
      "Fold: 1, Train Loss: 7.9946, Train Accuracy: 0.7077, Val Loss: 8.4126, Val Accuracy: 0.6473\n",
      "Iteration 0 - Loss: 13.6935, Accuracy: 0.5000\n",
      "Iteration 500 - Loss: 7.6897, Accuracy: 0.7422\n",
      "Iteration 1000 - Loss: 7.7863, Accuracy: 0.6797\n",
      "Iteration 1500 - Loss: 8.1254, Accuracy: 0.6797\n",
      "Iteration 2000 - Loss: 8.3267, Accuracy: 0.6328\n",
      "Iteration 2500 - Loss: 6.8955, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.8158, Accuracy: 0.7266\n",
      "Iteration 3500 - Loss: 8.4261, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.6219, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 7.8774, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 7.8257, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 7.4975, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 7.5397, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 7.8569, Accuracy: 0.6829\n",
      "Fold: 2, Train Loss: 7.9595, Train Accuracy: 0.7093, Val Loss: 8.2934, Val Accuracy: 0.6435\n",
      "Iteration 0 - Loss: 16.7482, Accuracy: 0.2422\n",
      "Iteration 500 - Loss: 8.7138, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.1238, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.9142, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 8.2582, Accuracy: 0.7109\n",
      "Iteration 2500 - Loss: 8.5821, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.9874, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.5137, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 8.8229, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 7.3624, Accuracy: 0.7109\n",
      "Iteration 5000 - Loss: 8.0164, Accuracy: 0.6953\n",
      "Iteration 5500 - Loss: 7.5968, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 7.8675, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 9.0933, Accuracy: 0.6406\n",
      "Fold: 3, Train Loss: 8.1008, Train Accuracy: 0.7026, Val Loss: 8.1035, Val Accuracy: 0.6811\n",
      "Iteration 0 - Loss: 15.1400, Accuracy: 0.5000\n",
      "Iteration 500 - Loss: 8.5452, Accuracy: 0.6484\n",
      "Iteration 1000 - Loss: 8.8064, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 8.0897, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.5552, Accuracy: 0.7344\n",
      "Iteration 2500 - Loss: 7.5254, Accuracy: 0.7578\n",
      "Iteration 3000 - Loss: 7.7378, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 7.9099, Accuracy: 0.7422\n",
      "Iteration 4000 - Loss: 7.8413, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 7.7680, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 8.1480, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 7.8630, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 8.4643, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.9863, Accuracy: 0.7109\n",
      "Fold: 4, Train Loss: 8.0468, Train Accuracy: 0.7058, Val Loss: 8.0984, Val Accuracy: 0.6735\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.87       0.75       0.80      \n",
      "1          0.60       0.82       0.70      \n",
      "2          0.52       0.48       0.50      \n",
      "3          0.89       0.49       0.63      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/e989bb41e1274afaa57602a8cf680b6a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.6594, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 10.3979, Accuracy: 0.7812\n",
      "Iteration 1000 - Loss: 10.4303, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 8.1117, Accuracy: 0.8125\n",
      "Iteration 2000 - Loss: 8.4598, Accuracy: 0.5938\n",
      "Iteration 2500 - Loss: 6.5772, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 9.3190, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 9.4930, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 9.1247, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 8.9806, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.1323, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 8.3215, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 9.6012, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.9418, Accuracy: 0.7812\n",
      "Fold: 0, Train Loss: 8.5996, Train Accuracy: 0.7064, Val Loss: 7.8739, Val Accuracy: 0.7308\n",
      "Iteration 0 - Loss: 14.4917, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 8.8325, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.9684, Accuracy: 0.8125\n",
      "Iteration 1500 - Loss: 8.4112, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.4827, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.7659, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 9.4813, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 7.1241, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 6.9612, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 9.6673, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.0002, Accuracy: 0.5625\n",
      "Iteration 5500 - Loss: 8.0623, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 8.9160, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 7.2474, Accuracy: 0.7188\n",
      "Fold: 1, Train Loss: 8.4950, Train Accuracy: 0.7117, Val Loss: 8.3104, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 14.9406, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 9.1277, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 8.7205, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.3517, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 9.0019, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.4938, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 7.9739, Accuracy: 0.8125\n",
      "Iteration 3500 - Loss: 8.1513, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 6.6882, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 9.3832, Accuracy: 0.5938\n",
      "Iteration 5000 - Loss: 9.0914, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 8.3870, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.3460, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.2605, Accuracy: 0.6562\n",
      "Fold: 2, Train Loss: 8.4810, Train Accuracy: 0.7104, Val Loss: 8.2937, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 15.1643, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 10.0648, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 10.1457, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 8.3919, Accuracy: 0.7812\n",
      "Iteration 2000 - Loss: 9.8603, Accuracy: 0.5312\n",
      "Iteration 2500 - Loss: 10.3072, Accuracy: 0.5312\n",
      "Iteration 3000 - Loss: 9.4409, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 8.3500, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 8.6458, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 10.1861, Accuracy: 0.5625\n",
      "Iteration 5000 - Loss: 8.9174, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 7.6948, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 6.8216, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.2115, Accuracy: 0.7188\n",
      "Fold: 3, Train Loss: 8.6301, Train Accuracy: 0.7051, Val Loss: 7.9624, Val Accuracy: 0.7101\n",
      "Iteration 0 - Loss: 12.4408, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 9.6947, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 9.4036, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 9.2557, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.6589, Accuracy: 0.8125\n",
      "Iteration 2500 - Loss: 9.1150, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.2365, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.5477, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 6.9854, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.4368, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.7102, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.0899, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.8373, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 8.3114, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 8.5181, Train Accuracy: 0.7083, Val Loss: 8.1672, Val Accuracy: 0.7008\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.62       0.72       0.67      \n",
      "2          0.53       0.49       0.51      \n",
      "3          0.78       0.66       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/4747d30a8bcb42cabed9c5a0041de1c3\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 13.7224, Accuracy: 0.3906\n",
      "Iteration 500 - Loss: 9.6353, Accuracy: 0.6719\n",
      "Iteration 1000 - Loss: 9.9889, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 7.4708, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 7.5909, Accuracy: 0.7656\n",
      "Iteration 2500 - Loss: 8.3056, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 7.3968, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 9.0315, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.9600, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 7.8702, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.3577, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 7.8716, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.1833, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.0102, Accuracy: 0.6719\n",
      "Fold: 0, Train Loss: 8.3252, Train Accuracy: 0.7114, Val Loss: 7.9088, Val Accuracy: 0.7308\n",
      "Iteration 0 - Loss: 14.8952, Accuracy: 0.3281\n",
      "Iteration 500 - Loss: 8.4393, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 8.7023, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 8.5001, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 7.9873, Accuracy: 0.8281\n",
      "Iteration 2500 - Loss: 8.1146, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.2676, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 7.5438, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 8.2902, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 7.9111, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 7.9389, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 7.7686, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 8.1004, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 9.0290, Accuracy: 0.6406\n",
      "Fold: 1, Train Loss: 8.2148, Train Accuracy: 0.7176, Val Loss: 8.2708, Val Accuracy: 0.6942\n",
      "Iteration 0 - Loss: 17.4724, Accuracy: 0.0938\n",
      "Iteration 500 - Loss: 8.8019, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.4667, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.6422, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 7.6946, Accuracy: 0.8281\n",
      "Iteration 2500 - Loss: 7.1858, Accuracy: 0.8182\n",
      "Iteration 3000 - Loss: 7.0835, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 7.6509, Accuracy: 0.8125\n",
      "Iteration 4000 - Loss: 8.5205, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 8.1031, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.8963, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 6.8818, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 8.4020, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.6714, Accuracy: 0.7812\n",
      "Fold: 2, Train Loss: 8.1996, Train Accuracy: 0.7165, Val Loss: 8.2164, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 13.2801, Accuracy: 0.4062\n",
      "Iteration 500 - Loss: 8.4674, Accuracy: 0.7656\n",
      "Iteration 1000 - Loss: 8.1183, Accuracy: 0.7969\n",
      "Iteration 1500 - Loss: 8.8147, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.2983, Accuracy: 0.7656\n",
      "Iteration 2500 - Loss: 8.0333, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.0469, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 7.8348, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 6.7700, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 7.1049, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.1206, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 7.9571, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 7.9527, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.1098, Accuracy: 0.7188\n",
      "Fold: 3, Train Loss: 8.3621, Train Accuracy: 0.7117, Val Loss: 7.8921, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 15.9620, Accuracy: 0.2031\n",
      "Iteration 500 - Loss: 9.2676, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 8.9000, Accuracy: 0.5938\n",
      "Iteration 1500 - Loss: 8.3211, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 8.4428, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 7.8979, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.9443, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.5649, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.5130, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 7.6040, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 7.6609, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.3087, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.4178, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.5761, Accuracy: 0.6406\n",
      "Fold: 4, Train Loss: 8.3026, Train Accuracy: 0.7112, Val Loss: 8.1109, Val Accuracy: 0.7214\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.80       0.81      \n",
      "1          0.68       0.67       0.67      \n",
      "2          0.56       0.66       0.61      \n",
      "3          0.83       0.64       0.73      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.7000 | F1-Score: 0.7000\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/852996d2a2c447d88cfa885110ea6213\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 14.3922, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 8.9592, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 7.7617, Accuracy: 0.7344\n",
      "Iteration 1500 - Loss: 7.9189, Accuracy: 0.7031\n",
      "Iteration 2000 - Loss: 7.3316, Accuracy: 0.7578\n",
      "Iteration 2500 - Loss: 8.3730, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.3664, Accuracy: 0.6797\n",
      "Iteration 3500 - Loss: 8.0785, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.0611, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 8.5585, Accuracy: 0.6953\n",
      "Iteration 5000 - Loss: 8.5177, Accuracy: 0.6797\n",
      "Iteration 5500 - Loss: 8.1535, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 6.7684, Accuracy: 0.8182\n",
      "Iteration 6500 - Loss: 7.4883, Accuracy: 0.7344\n",
      "Fold: 0, Train Loss: 8.2156, Train Accuracy: 0.7127, Val Loss: 7.8832, Val Accuracy: 0.7270\n",
      "Iteration 0 - Loss: 14.6711, Accuracy: 0.2891\n",
      "Iteration 500 - Loss: 8.0691, Accuracy: 0.7891\n",
      "Iteration 1000 - Loss: 8.3798, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 7.7065, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 8.4144, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.6125, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.5595, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 7.1667, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 7.7828, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 7.3996, Accuracy: 0.7578\n",
      "Iteration 5000 - Loss: 8.3329, Accuracy: 0.7422\n",
      "Iteration 5500 - Loss: 8.4066, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 7.5123, Accuracy: 0.7578\n",
      "Iteration 6500 - Loss: 7.5126, Accuracy: 0.7031\n",
      "Fold: 1, Train Loss: 8.1027, Train Accuracy: 0.7195, Val Loss: 8.2281, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 14.9850, Accuracy: 0.2969\n",
      "Iteration 500 - Loss: 8.4306, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.7172, Accuracy: 0.6797\n",
      "Iteration 1500 - Loss: 7.3209, Accuracy: 0.7891\n",
      "Iteration 2000 - Loss: 7.5238, Accuracy: 0.7422\n",
      "Iteration 2500 - Loss: 6.8643, Accuracy: 0.7891\n",
      "Iteration 3000 - Loss: 8.1322, Accuracy: 0.7266\n",
      "Iteration 3500 - Loss: 7.5749, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 8.5066, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 7.9882, Accuracy: 0.6797\n",
      "Iteration 5000 - Loss: 8.0354, Accuracy: 0.6953\n",
      "Iteration 5500 - Loss: 8.2447, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.5495, Accuracy: 0.6719\n",
      "Iteration 6500 - Loss: 7.2072, Accuracy: 0.7734\n",
      "Fold: 2, Train Loss: 8.0813, Train Accuracy: 0.7197, Val Loss: 8.1902, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 13.0734, Accuracy: 0.3594\n",
      "Iteration 500 - Loss: 7.6688, Accuracy: 0.7969\n",
      "Iteration 1000 - Loss: 8.4244, Accuracy: 0.6797\n",
      "Iteration 1500 - Loss: 7.8215, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.4341, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.6657, Accuracy: 0.6797\n",
      "Iteration 3000 - Loss: 8.2447, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 7.8665, Accuracy: 0.7266\n",
      "Iteration 4000 - Loss: 8.8314, Accuracy: 0.6641\n",
      "Iteration 4500 - Loss: 7.8184, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.3995, Accuracy: 0.7266\n",
      "Iteration 5500 - Loss: 7.9458, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 8.2846, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.5891, Accuracy: 0.6875\n",
      "Fold: 3, Train Loss: 8.2028, Train Accuracy: 0.7146, Val Loss: 8.0191, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 12.1792, Accuracy: 0.4531\n",
      "Iteration 500 - Loss: 8.1348, Accuracy: 0.7109\n",
      "Iteration 1000 - Loss: 8.1551, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 7.6832, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 8.2111, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 7.0299, Accuracy: 0.7891\n",
      "Iteration 3000 - Loss: 8.7304, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 7.9757, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 7.3477, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 7.9332, Accuracy: 0.7109\n",
      "Iteration 5000 - Loss: 8.5076, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 7.2989, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 7.7557, Accuracy: 0.7266\n",
      "Iteration 6500 - Loss: 7.6578, Accuracy: 0.7422\n",
      "Fold: 4, Train Loss: 8.1167, Train Accuracy: 0.7154, Val Loss: 8.0525, Val Accuracy: 0.7064\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.62       0.72       0.67      \n",
      "2          0.55       0.48       0.51      \n",
      "3          0.76       0.70       0.73      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/d348fcebef7943628dee0a782562ea9e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 13.6224, Accuracy: 0.3438\n",
      "Iteration 500 - Loss: 12.0177, Accuracy: 0.5000\n",
      "Iteration 1000 - Loss: 10.2230, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 9.4798, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.6540, Accuracy: 0.5938\n",
      "Iteration 2500 - Loss: 11.4066, Accuracy: 0.5000\n",
      "Iteration 3000 - Loss: 9.5651, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 9.9894, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 9.2875, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 9.2913, Accuracy: 0.8125\n",
      "Iteration 5000 - Loss: 9.0314, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.0348, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.9566, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.9411, Accuracy: 0.7812\n",
      "Fold: 0, Train Loss: 10.0486, Train Accuracy: 0.6488, Val Loss: 9.1061, Val Accuracy: 0.7251\n",
      "Iteration 0 - Loss: 17.5801, Accuracy: 0.1562\n",
      "Iteration 500 - Loss: 11.8496, Accuracy: 0.5000\n",
      "Iteration 1000 - Loss: 11.5518, Accuracy: 0.5000\n",
      "Iteration 1500 - Loss: 10.4300, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 11.3423, Accuracy: 0.5312\n",
      "Iteration 2500 - Loss: 10.8786, Accuracy: 0.5938\n",
      "Iteration 3000 - Loss: 10.3794, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 10.0104, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 7.5822, Accuracy: 0.8750\n",
      "Iteration 4500 - Loss: 9.1400, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 9.2898, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 9.0614, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.1225, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.8984, Accuracy: 0.7188\n",
      "Fold: 1, Train Loss: 10.1195, Train Accuracy: 0.6580, Val Loss: 9.5822, Val Accuracy: 0.6764\n",
      "Iteration 0 - Loss: 16.0163, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 10.8610, Accuracy: 0.7812\n",
      "Iteration 1000 - Loss: 11.6614, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 8.9960, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 10.1510, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 9.6729, Accuracy: 0.7059\n",
      "Iteration 3000 - Loss: 9.4542, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 10.2851, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 10.2811, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 9.5551, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.8849, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 9.8292, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 7.5078, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 10.6067, Accuracy: 0.6250\n",
      "Fold: 2, Train Loss: 9.9818, Train Accuracy: 0.6585, Val Loss: 9.3768, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 17.3329, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 13.0356, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 10.4883, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.5363, Accuracy: 0.5312\n",
      "Iteration 2000 - Loss: 10.6877, Accuracy: 0.6250\n",
      "Iteration 2500 - Loss: 9.7707, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 10.5879, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 9.4118, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 9.8718, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 10.4819, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 8.9290, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 9.1412, Accuracy: 0.7812\n",
      "Iteration 6000 - Loss: 9.4433, Accuracy: 0.5938\n",
      "Iteration 6500 - Loss: 8.7417, Accuracy: 0.7500\n",
      "Fold: 3, Train Loss: 10.6590, Train Accuracy: 0.6231, Val Loss: 9.1201, Val Accuracy: 0.7176\n",
      "Iteration 0 - Loss: 16.6226, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 12.5255, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 11.5103, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 9.1495, Accuracy: 0.8125\n",
      "Iteration 2000 - Loss: 9.3039, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.7476, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 11.0509, Accuracy: 0.4688\n",
      "Iteration 3500 - Loss: 10.3151, Accuracy: 0.5625\n",
      "Iteration 4000 - Loss: 8.7626, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 9.0087, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 9.6179, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 9.7915, Accuracy: 0.6250\n",
      "Iteration 6000 - Loss: 7.7998, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 7.7684, Accuracy: 0.6875\n",
      "Fold: 4, Train Loss: 10.0109, Train Accuracy: 0.6568, Val Loss: 9.1747, Val Accuracy: 0.7054\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.82       0.82      \n",
      "1          0.64       0.72       0.68      \n",
      "2          0.52       0.53       0.52      \n",
      "3          0.79       0.57       0.66      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/7076f5c9e8a8482db507b1b4e9787601\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 13.1577, Accuracy: 0.3594\n",
      "Iteration 500 - Loss: 10.8955, Accuracy: 0.6094\n",
      "Iteration 1000 - Loss: 9.0179, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 9.2532, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 8.5715, Accuracy: 0.8750\n",
      "Iteration 2500 - Loss: 8.3677, Accuracy: 0.7656\n",
      "Iteration 3000 - Loss: 10.0028, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 8.7605, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 9.8406, Accuracy: 0.6719\n",
      "Iteration 4500 - Loss: 9.4664, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 9.5308, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.7670, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.5802, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.8166, Accuracy: 0.7031\n",
      "Fold: 0, Train Loss: 9.3268, Train Accuracy: 0.6910, Val Loss: 8.5399, Val Accuracy: 0.7345\n",
      "Iteration 0 - Loss: 16.2558, Accuracy: 0.1875\n",
      "Iteration 500 - Loss: 11.8462, Accuracy: 0.5469\n",
      "Iteration 1000 - Loss: 10.0457, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.4973, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 10.3232, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 9.1000, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 9.7206, Accuracy: 0.6406\n",
      "Iteration 3500 - Loss: 9.2575, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 9.7104, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 8.6213, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.4235, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 9.4025, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 8.5673, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.9273, Accuracy: 0.6719\n",
      "Fold: 1, Train Loss: 9.5145, Train Accuracy: 0.6789, Val Loss: 9.0182, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 15.8901, Accuracy: 0.2188\n",
      "Iteration 500 - Loss: 10.8420, Accuracy: 0.6250\n",
      "Iteration 1000 - Loss: 9.0096, Accuracy: 0.7656\n",
      "Iteration 1500 - Loss: 10.2754, Accuracy: 0.6562\n",
      "Iteration 2000 - Loss: 9.2096, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 9.7678, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 8.9792, Accuracy: 0.7656\n",
      "Iteration 3500 - Loss: 8.9030, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 8.5292, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 9.4067, Accuracy: 0.6094\n",
      "Iteration 5000 - Loss: 8.9147, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 9.1252, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 7.6641, Accuracy: 0.8125\n",
      "Iteration 6500 - Loss: 8.9638, Accuracy: 0.7031\n",
      "Fold: 2, Train Loss: 9.5645, Train Accuracy: 0.6755, Val Loss: 8.9517, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.7865, Accuracy: 0.2500\n",
      "Iteration 500 - Loss: 11.6234, Accuracy: 0.5938\n",
      "Iteration 1000 - Loss: 10.0956, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 9.2820, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.6324, Accuracy: 0.6719\n",
      "Iteration 2500 - Loss: 9.6458, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 10.4884, Accuracy: 0.6094\n",
      "Iteration 3500 - Loss: 8.0522, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 9.9657, Accuracy: 0.6094\n",
      "Iteration 4500 - Loss: 7.9611, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 9.2595, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 9.3615, Accuracy: 0.5938\n",
      "Iteration 6000 - Loss: 9.3122, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 9.1784, Accuracy: 0.7031\n",
      "Fold: 3, Train Loss: 9.6341, Train Accuracy: 0.6823, Val Loss: 8.5516, Val Accuracy: 0.7167\n",
      "Iteration 0 - Loss: 14.5689, Accuracy: 0.2812\n",
      "Iteration 500 - Loss: 11.6928, Accuracy: 0.5312\n",
      "Iteration 1000 - Loss: 9.6368, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.9534, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 9.9188, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.6772, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.2498, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.7531, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 9.1953, Accuracy: 0.6406\n",
      "Iteration 4500 - Loss: 8.5627, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.6772, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 8.8666, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.6573, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.4873, Accuracy: 0.7188\n",
      "Fold: 4, Train Loss: 9.6190, Train Accuracy: 0.6771, Val Loss: 8.8247, Val Accuracy: 0.7120\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.79       0.81      \n",
      "1          0.63       0.73       0.68      \n",
      "2          0.52       0.51       0.51      \n",
      "3          0.78       0.59       0.67      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/f0c609e0745a4944a14ac4f7e4352954\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | normal | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 13.3802, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 11.0320, Accuracy: 0.6328\n",
      "Iteration 1000 - Loss: 10.2953, Accuracy: 0.6953\n",
      "Iteration 1500 - Loss: 9.3151, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 9.9004, Accuracy: 0.6172\n",
      "Iteration 2500 - Loss: 8.2919, Accuracy: 0.7891\n",
      "Iteration 3000 - Loss: 8.2259, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.6682, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.9373, Accuracy: 0.7891\n",
      "Iteration 4500 - Loss: 9.2052, Accuracy: 0.6596\n",
      "Iteration 5000 - Loss: 8.6699, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 8.7040, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 9.0182, Accuracy: 0.6797\n",
      "Iteration 6500 - Loss: 8.5584, Accuracy: 0.6875\n",
      "Fold: 0, Train Loss: 9.1116, Train Accuracy: 0.6949, Val Loss: 8.2741, Val Accuracy: 0.7251\n",
      "Iteration 0 - Loss: 14.5740, Accuracy: 0.3203\n",
      "Iteration 500 - Loss: 9.7313, Accuracy: 0.6719\n",
      "Iteration 1000 - Loss: 9.7289, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 9.1126, Accuracy: 0.7422\n",
      "Iteration 2000 - Loss: 9.4459, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 9.2282, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 8.6716, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.4485, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.6943, Accuracy: 0.7266\n",
      "Iteration 4500 - Loss: 7.9330, Accuracy: 0.7578\n",
      "Iteration 5000 - Loss: 8.8226, Accuracy: 0.7422\n",
      "Iteration 5500 - Loss: 9.0775, Accuracy: 0.6797\n",
      "Iteration 6000 - Loss: 8.0148, Accuracy: 0.7656\n",
      "Iteration 6500 - Loss: 7.8015, Accuracy: 0.7422\n",
      "Fold: 1, Train Loss: 8.9319, Train Accuracy: 0.7053, Val Loss: 8.6173, Val Accuracy: 0.7017\n",
      "Iteration 0 - Loss: 17.7112, Accuracy: 0.1484\n",
      "Iteration 500 - Loss: 10.7239, Accuracy: 0.5859\n",
      "Iteration 1000 - Loss: 9.6003, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 8.7578, Accuracy: 0.7734\n",
      "Iteration 2000 - Loss: 9.5209, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 9.0311, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 8.8368, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.9276, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.9577, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 8.7357, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 9.0624, Accuracy: 0.6953\n",
      "Iteration 5500 - Loss: 7.9849, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.9503, Accuracy: 0.7109\n",
      "Iteration 6500 - Loss: 7.7923, Accuracy: 0.7656\n",
      "Fold: 2, Train Loss: 9.0035, Train Accuracy: 0.6983, Val Loss: 8.5755, Val Accuracy: 0.7036\n",
      "Iteration 0 - Loss: 13.1032, Accuracy: 0.3828\n",
      "Iteration 500 - Loss: 10.5495, Accuracy: 0.6172\n",
      "Iteration 1000 - Loss: 9.6517, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.1128, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 9.7169, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.7489, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.3156, Accuracy: 0.6797\n",
      "Iteration 3500 - Loss: 8.8172, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 8.6640, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 9.1733, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.8425, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 8.4802, Accuracy: 0.7266\n",
      "Iteration 6000 - Loss: 8.2299, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 7.6814, Accuracy: 0.8125\n",
      "Fold: 3, Train Loss: 9.0745, Train Accuracy: 0.6982, Val Loss: 8.2716, Val Accuracy: 0.7261\n",
      "Iteration 0 - Loss: 15.5766, Accuracy: 0.2422\n",
      "Iteration 500 - Loss: 10.7520, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 9.1333, Accuracy: 0.7578\n",
      "Iteration 1500 - Loss: 8.8931, Accuracy: 0.7422\n",
      "Iteration 2000 - Loss: 8.6000, Accuracy: 0.7422\n",
      "Iteration 2500 - Loss: 8.4506, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 9.2545, Accuracy: 0.6797\n",
      "Iteration 3500 - Loss: 9.3349, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 8.5908, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 9.4042, Accuracy: 0.6406\n",
      "Iteration 5000 - Loss: 8.1447, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.8972, Accuracy: 0.6953\n",
      "Iteration 6000 - Loss: 8.5513, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.4102, Accuracy: 0.7422\n",
      "Fold: 4, Train Loss: 9.0555, Train Accuracy: 0.7008, Val Loss: 8.4033, Val Accuracy: 0.7111\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.81       0.81      \n",
      "1          0.64       0.70       0.67      \n",
      "2          0.54       0.55       0.55      \n",
      "3          0.82       0.62       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/a299e956455243d682a90f6f069b37a7\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5762, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 8.6257, Accuracy: 0.5938\n",
      "Iteration 1000 - Loss: 7.9844, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.9803, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.5284, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.4362, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.4864, Accuracy: 0.7500\n",
      "Iteration 3500 - Loss: 8.2482, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.3933, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 7.2939, Accuracy: 0.8125\n",
      "Iteration 5000 - Loss: 6.2748, Accuracy: 0.8750\n",
      "Iteration 5500 - Loss: 7.8590, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.2112, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 10.4067, Accuracy: 0.5312\n",
      "Fold: 0, Train Loss: 8.1330, Train Accuracy: 0.7048, Val Loss: 7.7524, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 15.5197, Accuracy: 0.6250\n",
      "Iteration 500 - Loss: 6.7334, Accuracy: 0.7812\n",
      "Iteration 1000 - Loss: 7.5008, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 9.1729, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 6.1439, Accuracy: 0.8125\n",
      "Iteration 2500 - Loss: 8.6613, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 8.9708, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 8.3006, Accuracy: 0.8438\n",
      "Iteration 4000 - Loss: 7.8927, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.0706, Accuracy: 0.8438\n",
      "Iteration 5000 - Loss: 8.5035, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.4131, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 9.1669, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 8.1501, Accuracy: 0.6250\n",
      "Fold: 1, Train Loss: 8.0505, Train Accuracy: 0.7103, Val Loss: 8.2509, Val Accuracy: 0.6679\n",
      "Iteration 0 - Loss: 15.5515, Accuracy: 0.6875\n",
      "Iteration 500 - Loss: 8.8469, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 6.4393, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 8.4440, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 8.5041, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.8427, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 7.3800, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 6.5865, Accuracy: 0.8438\n",
      "Iteration 4000 - Loss: 7.3555, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.1225, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 7.1822, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 9.0727, Accuracy: 0.5312\n",
      "Iteration 6000 - Loss: 9.4214, Accuracy: 0.5625\n",
      "Iteration 6500 - Loss: 7.9049, Accuracy: 0.6875\n",
      "Fold: 2, Train Loss: 8.0380, Train Accuracy: 0.7116, Val Loss: 8.2178, Val Accuracy: 0.6773\n",
      "Iteration 0 - Loss: 15.5596, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 8.4053, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.6335, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 8.4021, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.8115, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.1560, Accuracy: 0.6562\n",
      "Iteration 3000 - Loss: 6.4746, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 8.0037, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.9926, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.6924, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 7.2217, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 9.6372, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 7.5185, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 6.5354, Accuracy: 0.7812\n",
      "Fold: 3, Train Loss: 8.1496, Train Accuracy: 0.7075, Val Loss: 8.0054, Val Accuracy: 0.6876\n",
      "Iteration 0 - Loss: 15.5502, Accuracy: 0.5625\n",
      "Iteration 500 - Loss: 8.0641, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 7.8841, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 7.3258, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.3150, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 7.5141, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 6.9451, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.1038, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 9.5511, Accuracy: 0.5000\n",
      "Iteration 4500 - Loss: 7.2645, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 9.9903, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 6.9593, Accuracy: 0.8438\n",
      "Iteration 6000 - Loss: 6.3574, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 8.8818, Accuracy: 0.7188\n",
      "Fold: 4, Train Loss: 8.0817, Train Accuracy: 0.7090, Val Loss: 8.0701, Val Accuracy: 0.7054\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.75       0.80      \n",
      "1          0.64       0.73       0.68      \n",
      "2          0.56       0.45       0.50      \n",
      "3          0.66       0.78       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/a9a65f7363d24c8aa87e2f1be14eae00\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5540, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 7.1805, Accuracy: 0.8125\n",
      "Iteration 1000 - Loss: 8.2554, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.9961, Accuracy: 0.7969\n",
      "Iteration 2000 - Loss: 9.1317, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 8.4636, Accuracy: 0.6406\n",
      "Iteration 3000 - Loss: 8.6804, Accuracy: 0.5957\n",
      "Iteration 3500 - Loss: 8.6496, Accuracy: 0.7500\n",
      "Iteration 4000 - Loss: 8.3652, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 6.8109, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.5489, Accuracy: 0.6800\n",
      "Iteration 5500 - Loss: 7.6108, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.2994, Accuracy: 0.6094\n",
      "Iteration 6500 - Loss: 7.9204, Accuracy: 0.7344\n",
      "Fold: 0, Train Loss: 8.1332, Train Accuracy: 0.7031, Val Loss: 8.1443, Val Accuracy: 0.6895\n",
      "Iteration 0 - Loss: 15.5339, Accuracy: 0.5469\n",
      "Iteration 500 - Loss: 6.9008, Accuracy: 0.7656\n",
      "Iteration 1000 - Loss: 8.4799, Accuracy: 0.6406\n",
      "Iteration 1500 - Loss: 8.8666, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.1881, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.0432, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.2359, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 8.7517, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.8954, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.8393, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.9454, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 6.7471, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.4527, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 8.3541, Accuracy: 0.6406\n",
      "Fold: 1, Train Loss: 7.9979, Train Accuracy: 0.7112, Val Loss: 8.2282, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 15.5103, Accuracy: 0.4444\n",
      "Iteration 500 - Loss: 7.5587, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.0818, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 8.1473, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 7.1023, Accuracy: 0.7344\n",
      "Iteration 2500 - Loss: 7.6280, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 7.9235, Accuracy: 0.6719\n",
      "Iteration 3500 - Loss: 6.6703, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 8.6091, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.9446, Accuracy: 0.6250\n",
      "Iteration 5000 - Loss: 7.9690, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 7.0347, Accuracy: 0.7656\n",
      "Iteration 6000 - Loss: 7.8833, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 7.2227, Accuracy: 0.7656\n",
      "Fold: 2, Train Loss: 7.9829, Train Accuracy: 0.7117, Val Loss: 8.1088, Val Accuracy: 0.6951\n",
      "Iteration 0 - Loss: 15.5599, Accuracy: 0.5625\n",
      "Iteration 500 - Loss: 8.9022, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.8816, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.5897, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.3923, Accuracy: 0.7391\n",
      "Iteration 2500 - Loss: 8.0686, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 7.9995, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 6.8095, Accuracy: 0.7969\n",
      "Iteration 4000 - Loss: 6.8700, Accuracy: 0.8125\n",
      "Iteration 4500 - Loss: 8.6470, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 7.3975, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.1916, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.2759, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.4109, Accuracy: 0.6094\n",
      "Fold: 3, Train Loss: 8.1238, Train Accuracy: 0.7065, Val Loss: 8.3427, Val Accuracy: 0.6679\n",
      "Iteration 0 - Loss: 15.5383, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 8.9776, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.3261, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 8.0571, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 7.3941, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.4043, Accuracy: 0.6875\n",
      "Iteration 3000 - Loss: 8.2646, Accuracy: 0.7344\n",
      "Iteration 3500 - Loss: 8.4740, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.2833, Accuracy: 0.7812\n",
      "Iteration 4500 - Loss: 7.5235, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.3713, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 7.6293, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.5530, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 9.0766, Accuracy: 0.7031\n",
      "Fold: 4, Train Loss: 8.0708, Train Accuracy: 0.7080, Val Loss: 7.8740, Val Accuracy: 0.7139\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.63       0.72       0.67      \n",
      "2          0.54       0.46       0.50      \n",
      "3          0.74       0.70       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/daac9b8e00b047a4b29af3a39a64fb23\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.4932, Accuracy: 0.4453\n",
      "Iteration 500 - Loss: 7.7631, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.2857, Accuracy: 0.7121\n",
      "Iteration 1500 - Loss: 7.7797, Accuracy: 0.7109\n",
      "Iteration 2000 - Loss: 8.8537, Accuracy: 0.6641\n",
      "Iteration 2500 - Loss: 7.4606, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 7.5345, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.5505, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 8.2459, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3840, Accuracy: 0.6953\n",
      "Iteration 5000 - Loss: 8.3443, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 8.7160, Accuracy: 0.6406\n",
      "Iteration 6000 - Loss: 8.2716, Accuracy: 0.6484\n",
      "Iteration 6500 - Loss: 8.2814, Accuracy: 0.6641\n",
      "Fold: 0, Train Loss: 8.1023, Train Accuracy: 0.7011, Val Loss: 7.9205, Val Accuracy: 0.7092\n",
      "Iteration 0 - Loss: 15.4923, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 7.9371, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 7.2039, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 8.3968, Accuracy: 0.6698\n",
      "Iteration 2000 - Loss: 7.2362, Accuracy: 0.8047\n",
      "Iteration 2500 - Loss: 7.5583, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 7.1184, Accuracy: 0.7734\n",
      "Iteration 3500 - Loss: 8.4494, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 7.5529, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 8.7617, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 7.8088, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.9330, Accuracy: 0.6719\n",
      "Iteration 6000 - Loss: 8.0087, Accuracy: 0.6328\n",
      "Iteration 6500 - Loss: 8.0812, Accuracy: 0.6875\n",
      "Fold: 1, Train Loss: 7.9996, Train Accuracy: 0.7075, Val Loss: 8.2170, Val Accuracy: 0.7036\n",
      "Iteration 0 - Loss: 15.5454, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 8.0012, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.3774, Accuracy: 0.6406\n",
      "Iteration 1500 - Loss: 7.4253, Accuracy: 0.7578\n",
      "Iteration 2000 - Loss: 7.3108, Accuracy: 0.7422\n",
      "Iteration 2500 - Loss: 7.9998, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.1446, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 8.4199, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 8.0174, Accuracy: 0.6953\n",
      "Iteration 4500 - Loss: 6.9111, Accuracy: 0.7734\n",
      "Iteration 5000 - Loss: 8.2437, Accuracy: 0.6953\n",
      "Iteration 5500 - Loss: 7.6683, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 8.3159, Accuracy: 0.6641\n",
      "Iteration 6500 - Loss: 8.0616, Accuracy: 0.7422\n",
      "Fold: 2, Train Loss: 7.9685, Train Accuracy: 0.7095, Val Loss: 8.1575, Val Accuracy: 0.6792\n",
      "Iteration 0 - Loss: 15.5378, Accuracy: 0.4844\n",
      "Iteration 500 - Loss: 7.8784, Accuracy: 0.7266\n",
      "Iteration 1000 - Loss: 8.7766, Accuracy: 0.6641\n",
      "Iteration 1500 - Loss: 8.3670, Accuracy: 0.6953\n",
      "Iteration 2000 - Loss: 8.8836, Accuracy: 0.6484\n",
      "Iteration 2500 - Loss: 7.8014, Accuracy: 0.6641\n",
      "Iteration 3000 - Loss: 8.0916, Accuracy: 0.6641\n",
      "Iteration 3500 - Loss: 7.5673, Accuracy: 0.7109\n",
      "Iteration 4000 - Loss: 6.3836, Accuracy: 0.7891\n",
      "Iteration 4500 - Loss: 7.2658, Accuracy: 0.7266\n",
      "Iteration 5000 - Loss: 7.9411, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.8876, Accuracy: 0.6797\n",
      "Iteration 6000 - Loss: 7.9147, Accuracy: 0.7344\n",
      "Iteration 6500 - Loss: 8.9552, Accuracy: 0.6484\n",
      "Fold: 3, Train Loss: 8.0858, Train Accuracy: 0.7047, Val Loss: 8.3947, Val Accuracy: 0.6567\n",
      "Iteration 0 - Loss: 15.5669, Accuracy: 0.4375\n",
      "Iteration 500 - Loss: 7.9306, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 8.4711, Accuracy: 0.6484\n",
      "Iteration 1500 - Loss: 7.9487, Accuracy: 0.7734\n",
      "Iteration 2000 - Loss: 7.5004, Accuracy: 0.7266\n",
      "Iteration 2500 - Loss: 7.5300, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.4118, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 7.9052, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 7.4496, Accuracy: 0.7656\n",
      "Iteration 4500 - Loss: 8.5390, Accuracy: 0.6797\n",
      "Iteration 5000 - Loss: 8.4461, Accuracy: 0.6484\n",
      "Iteration 5500 - Loss: 8.4181, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.8130, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 8.2288, Accuracy: 0.7422\n",
      "Fold: 4, Train Loss: 8.0485, Train Accuracy: 0.7057, Val Loss: 7.9737, Val Accuracy: 0.6942\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.80       0.82      \n",
      "1          0.64       0.75       0.69      \n",
      "2          0.55       0.45       0.49      \n",
      "3          0.74       0.70       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.01-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/fd5832910bcf4e52b9a9428866d0842b\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5419, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 9.9399, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 7.5928, Accuracy: 0.6667\n",
      "Iteration 1500 - Loss: 8.1773, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.3460, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.5459, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 8.5275, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 6.8295, Accuracy: 0.8750\n",
      "Iteration 4000 - Loss: 8.3033, Accuracy: 0.6875\n",
      "Iteration 4500 - Loss: 7.5629, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.2491, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 6.7782, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.6565, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 9.1244, Accuracy: 0.5938\n",
      "Fold: 0, Train Loss: 8.6187, Train Accuracy: 0.7073, Val Loss: 7.9092, Val Accuracy: 0.7345\n",
      "Iteration 0 - Loss: 15.5393, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 9.0265, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.4924, Accuracy: 0.7500\n",
      "Iteration 1500 - Loss: 8.3820, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 8.1082, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 7.6721, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.1042, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 8.1248, Accuracy: 0.6875\n",
      "Iteration 4000 - Loss: 7.2904, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 7.6111, Accuracy: 0.8438\n",
      "Iteration 5000 - Loss: 8.5409, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 6.4926, Accuracy: 0.8438\n",
      "Iteration 6000 - Loss: 8.3285, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.4199, Accuracy: 0.7188\n",
      "Fold: 1, Train Loss: 8.4806, Train Accuracy: 0.7146, Val Loss: 8.3352, Val Accuracy: 0.7111\n",
      "Iteration 0 - Loss: 15.5398, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 9.7236, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 9.9351, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.9364, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 9.2769, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.5590, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 9.4919, Accuracy: 0.5938\n",
      "Iteration 3500 - Loss: 9.5013, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 7.8445, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 9.1393, Accuracy: 0.5312\n",
      "Iteration 5000 - Loss: 8.6849, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 7.6055, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 9.1041, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 5.8396, Accuracy: 0.8438\n",
      "Fold: 2, Train Loss: 8.4709, Train Accuracy: 0.7156, Val Loss: 8.3452, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5439, Accuracy: 0.4062\n",
      "Iteration 500 - Loss: 8.4938, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 9.1298, Accuracy: 0.8125\n",
      "Iteration 1500 - Loss: 7.8809, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.3318, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 7.1459, Accuracy: 0.8750\n",
      "Iteration 3000 - Loss: 7.1563, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 8.1194, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 9.2611, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 8.8235, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.1133, Accuracy: 0.5938\n",
      "Iteration 5500 - Loss: 8.6015, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 7.2917, Accuracy: 0.8438\n",
      "Iteration 6500 - Loss: 7.2274, Accuracy: 0.8125\n",
      "Fold: 3, Train Loss: 8.5999, Train Accuracy: 0.7096, Val Loss: 8.0325, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 15.5391, Accuracy: 0.6250\n",
      "Iteration 500 - Loss: 11.5249, Accuracy: 0.4688\n",
      "Iteration 1000 - Loss: 7.6891, Accuracy: 0.7812\n",
      "Iteration 1500 - Loss: 9.7955, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.2670, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 7.5538, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 8.7624, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 9.2263, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 8.4733, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 6.9302, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.3292, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 7.2407, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 8.0077, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 7.2056, Accuracy: 0.8125\n",
      "Fold: 4, Train Loss: 8.5185, Train Accuracy: 0.7106, Val Loss: 8.1185, Val Accuracy: 0.7129\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.83       0.82      \n",
      "1          0.64       0.70       0.67      \n",
      "2          0.54       0.54       0.54      \n",
      "3          0.84       0.64       0.73      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6900 | F1-Score: 0.6900\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/aeadacb913a14b05a0aa7209004ec010\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5444, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 9.0480, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 9.1787, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 8.5705, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 7.5733, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 8.2077, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 8.7125, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 7.9396, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 7.4213, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 8.4114, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.0077, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.7881, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 7.8688, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 7.6285, Accuracy: 0.7344\n",
      "Fold: 0, Train Loss: 8.3596, Train Accuracy: 0.7109, Val Loss: 7.9491, Val Accuracy: 0.7298\n",
      "Iteration 0 - Loss: 15.5431, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 9.1139, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 8.9981, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.1393, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 9.2749, Accuracy: 0.6562\n",
      "Iteration 2500 - Loss: 7.4087, Accuracy: 0.7969\n",
      "Iteration 3000 - Loss: 8.3316, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 7.9729, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 8.0417, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3506, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 8.6823, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 8.4253, Accuracy: 0.7031\n",
      "Iteration 6000 - Loss: 9.1600, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.8174, Accuracy: 0.6875\n",
      "Fold: 1, Train Loss: 8.2353, Train Accuracy: 0.7192, Val Loss: 8.2333, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5402, Accuracy: 0.4844\n",
      "Iteration 500 - Loss: 8.1671, Accuracy: 0.7969\n",
      "Iteration 1000 - Loss: 8.6330, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 7.9630, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 8.5425, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 8.2343, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 7.4146, Accuracy: 0.7969\n",
      "Iteration 3500 - Loss: 8.8381, Accuracy: 0.6406\n",
      "Iteration 4000 - Loss: 8.9233, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 7.9703, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 7.2661, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 8.7914, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 8.5792, Accuracy: 0.6406\n",
      "Iteration 6500 - Loss: 7.6450, Accuracy: 0.7812\n",
      "Fold: 2, Train Loss: 8.2210, Train Accuracy: 0.7183, Val Loss: 8.2973, Val Accuracy: 0.6848\n",
      "Iteration 0 - Loss: 15.5383, Accuracy: 0.5161\n",
      "Iteration 500 - Loss: 9.6904, Accuracy: 0.6719\n",
      "Iteration 1000 - Loss: 9.0428, Accuracy: 0.6406\n",
      "Iteration 1500 - Loss: 9.1152, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 8.2621, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 7.8114, Accuracy: 0.7656\n",
      "Iteration 3000 - Loss: 7.7955, Accuracy: 0.7656\n",
      "Iteration 3500 - Loss: 8.5428, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 9.1180, Accuracy: 0.6250\n",
      "Iteration 4500 - Loss: 8.4321, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 7.8359, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.0243, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 8.3193, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 7.1078, Accuracy: 0.7812\n",
      "Fold: 3, Train Loss: 8.3509, Train Accuracy: 0.7131, Val Loss: 7.9848, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 15.5445, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 9.1401, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.6633, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 8.7394, Accuracy: 0.6719\n",
      "Iteration 2000 - Loss: 9.6853, Accuracy: 0.6094\n",
      "Iteration 2500 - Loss: 7.9476, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.9221, Accuracy: 0.6719\n",
      "Iteration 3500 - Loss: 7.6670, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 6.8447, Accuracy: 0.7969\n",
      "Iteration 4500 - Loss: 8.7536, Accuracy: 0.7031\n",
      "Iteration 5000 - Loss: 8.0000, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 8.3485, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 7.4143, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 7.4207, Accuracy: 0.7500\n",
      "Fold: 4, Train Loss: 8.2813, Train Accuracy: 0.7124, Val Loss: 8.1364, Val Accuracy: 0.7017\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.63       0.72       0.67      \n",
      "2          0.54       0.49       0.52      \n",
      "3          0.77       0.66       0.71      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/dd9aacaead8a4effa2a8266d1c9189f8\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5415, Accuracy: 0.4453\n",
      "Iteration 500 - Loss: 9.0567, Accuracy: 0.6641\n",
      "Iteration 1000 - Loss: 8.1790, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 8.5328, Accuracy: 0.6953\n",
      "Iteration 2000 - Loss: 7.3545, Accuracy: 0.7734\n",
      "Iteration 2500 - Loss: 8.3730, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 6.6383, Accuracy: 0.8667\n",
      "Iteration 3500 - Loss: 7.9097, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.2129, Accuracy: 0.7407\n",
      "Iteration 4500 - Loss: 8.7313, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 7.9470, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.9130, Accuracy: 0.6328\n",
      "Iteration 6000 - Loss: 8.9787, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 7.9959, Accuracy: 0.7031\n",
      "Fold: 0, Train Loss: 8.2095, Train Accuracy: 0.7139, Val Loss: 7.8196, Val Accuracy: 0.7336\n",
      "Iteration 0 - Loss: 15.5419, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 8.5786, Accuracy: 0.7344\n",
      "Iteration 1000 - Loss: 8.0891, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 7.6253, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 7.7011, Accuracy: 0.7656\n",
      "Iteration 2500 - Loss: 8.5208, Accuracy: 0.7188\n",
      "Iteration 3000 - Loss: 7.8526, Accuracy: 0.7422\n",
      "Iteration 3500 - Loss: 7.9303, Accuracy: 0.6953\n",
      "Iteration 4000 - Loss: 7.9105, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.3086, Accuracy: 0.6797\n",
      "Iteration 5000 - Loss: 7.8747, Accuracy: 0.6875\n",
      "Iteration 5500 - Loss: 7.2909, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 8.2891, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 7.9939, Accuracy: 0.7109\n",
      "Fold: 1, Train Loss: 8.0887, Train Accuracy: 0.7209, Val Loss: 8.2508, Val Accuracy: 0.6979\n",
      "Iteration 0 - Loss: 15.5438, Accuracy: 0.5547\n",
      "Iteration 500 - Loss: 8.8393, Accuracy: 0.6953\n",
      "Iteration 1000 - Loss: 8.1005, Accuracy: 0.7266\n",
      "Iteration 1500 - Loss: 8.0091, Accuracy: 0.7422\n",
      "Iteration 2000 - Loss: 8.7761, Accuracy: 0.6719\n",
      "Iteration 2500 - Loss: 7.8192, Accuracy: 0.7344\n",
      "Iteration 3000 - Loss: 8.3942, Accuracy: 0.7356\n",
      "Iteration 3500 - Loss: 8.5340, Accuracy: 0.6953\n",
      "Iteration 4000 - Loss: 7.9812, Accuracy: 0.7266\n",
      "Iteration 4500 - Loss: 8.2127, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 8.5596, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 7.5572, Accuracy: 0.7578\n",
      "Iteration 6000 - Loss: 7.4121, Accuracy: 0.7656\n",
      "Iteration 6500 - Loss: 8.0691, Accuracy: 0.7031\n",
      "Fold: 2, Train Loss: 8.0809, Train Accuracy: 0.7202, Val Loss: 8.1992, Val Accuracy: 0.7017\n",
      "Iteration 0 - Loss: 15.5456, Accuracy: 0.5391\n",
      "Iteration 500 - Loss: 8.3202, Accuracy: 0.7734\n",
      "Iteration 1000 - Loss: 8.2839, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 8.7130, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 7.2430, Accuracy: 0.7578\n",
      "Iteration 2500 - Loss: 8.1623, Accuracy: 0.6953\n",
      "Iteration 3000 - Loss: 8.3160, Accuracy: 0.6953\n",
      "Iteration 3500 - Loss: 8.1793, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 7.7489, Accuracy: 0.7422\n",
      "Iteration 4500 - Loss: 8.7791, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 6.8066, Accuracy: 0.8667\n",
      "Iteration 5500 - Loss: 8.2058, Accuracy: 0.7266\n",
      "Iteration 6000 - Loss: 7.7403, Accuracy: 0.7734\n",
      "Iteration 6500 - Loss: 8.2780, Accuracy: 0.7188\n",
      "Fold: 3, Train Loss: 8.2001, Train Accuracy: 0.7159, Val Loss: 7.8906, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 15.5436, Accuracy: 0.5000\n",
      "Iteration 500 - Loss: 8.6344, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 8.3890, Accuracy: 0.7109\n",
      "Iteration 1500 - Loss: 8.1564, Accuracy: 0.7344\n",
      "Iteration 2000 - Loss: 8.1430, Accuracy: 0.6797\n",
      "Iteration 2500 - Loss: 8.6255, Accuracy: 0.6484\n",
      "Iteration 3000 - Loss: 8.6846, Accuracy: 0.6484\n",
      "Iteration 3500 - Loss: 7.8008, Accuracy: 0.6641\n",
      "Iteration 4000 - Loss: 7.8587, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 7.4051, Accuracy: 0.7578\n",
      "Iteration 5000 - Loss: 8.1210, Accuracy: 0.7031\n",
      "Iteration 5500 - Loss: 7.8194, Accuracy: 0.7578\n",
      "Iteration 6000 - Loss: 7.9279, Accuracy: 0.6641\n",
      "Iteration 6500 - Loss: 7.7533, Accuracy: 0.7578\n",
      "Fold: 4, Train Loss: 8.1448, Train Accuracy: 0.7151, Val Loss: 8.0387, Val Accuracy: 0.7101\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.82       0.82      \n",
      "1          0.63       0.71       0.67      \n",
      "2          0.54       0.50       0.52      \n",
      "3          0.79       0.66       0.72      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/1f32f2b8e9f24776aa1528e44cbda3e1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 13.0167, Accuracy: 0.4375\n",
      "Iteration 1000 - Loss: 11.2255, Accuracy: 0.6875\n",
      "Iteration 1500 - Loss: 11.2124, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 10.3000, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 10.9599, Accuracy: 0.5625\n",
      "Iteration 3000 - Loss: 11.2587, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 9.7967, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 10.5647, Accuracy: 0.5625\n",
      "Iteration 4500 - Loss: 8.9029, Accuracy: 0.6562\n",
      "Iteration 5000 - Loss: 9.9350, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 8.3902, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.5304, Accuracy: 0.6562\n",
      "Iteration 6500 - Loss: 7.8373, Accuracy: 0.7500\n",
      "Fold: 0, Train Loss: 10.2724, Train Accuracy: 0.6740, Val Loss: 9.1427, Val Accuracy: 0.7308\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5938\n",
      "Iteration 500 - Loss: 12.5974, Accuracy: 0.5625\n",
      "Iteration 1000 - Loss: 12.1599, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 10.6793, Accuracy: 0.5312\n",
      "Iteration 2000 - Loss: 9.6585, Accuracy: 0.7812\n",
      "Iteration 2500 - Loss: 9.2547, Accuracy: 0.8125\n",
      "Iteration 3000 - Loss: 8.9156, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 10.2752, Accuracy: 0.6562\n",
      "Iteration 4000 - Loss: 9.9055, Accuracy: 0.5938\n",
      "Iteration 4500 - Loss: 11.1736, Accuracy: 0.5312\n",
      "Iteration 5000 - Loss: 9.1957, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.7778, Accuracy: 0.7500\n",
      "Iteration 6000 - Loss: 10.4281, Accuracy: 0.5312\n",
      "Iteration 6500 - Loss: 8.7309, Accuracy: 0.6562\n",
      "Fold: 1, Train Loss: 10.1498, Train Accuracy: 0.6784, Val Loss: 9.5580, Val Accuracy: 0.6782\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 13.0249, Accuracy: 0.5000\n",
      "Iteration 1000 - Loss: 11.6630, Accuracy: 0.5312\n",
      "Iteration 1500 - Loss: 12.3925, Accuracy: 0.5312\n",
      "Iteration 2000 - Loss: 9.8932, Accuracy: 0.7500\n",
      "Iteration 2500 - Loss: 9.1253, Accuracy: 0.7812\n",
      "Iteration 3000 - Loss: 9.8556, Accuracy: 0.5625\n",
      "Iteration 3500 - Loss: 10.5292, Accuracy: 0.5312\n",
      "Iteration 4000 - Loss: 9.7283, Accuracy: 0.7500\n",
      "Iteration 4500 - Loss: 9.7873, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 8.5577, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 9.9415, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.1341, Accuracy: 0.7500\n",
      "Iteration 6500 - Loss: 8.8305, Accuracy: 0.6875\n",
      "Fold: 2, Train Loss: 10.1794, Train Accuracy: 0.6796, Val Loss: 9.4508, Val Accuracy: 0.6895\n",
      "Iteration 0 - Loss: 15.5426, Accuracy: 0.6667\n",
      "Iteration 500 - Loss: 12.8003, Accuracy: 0.4688\n",
      "Iteration 1000 - Loss: 11.0950, Accuracy: 0.7188\n",
      "Iteration 1500 - Loss: 10.8423, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 10.5802, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 11.4325, Accuracy: 0.5625\n",
      "Iteration 3000 - Loss: 11.6719, Accuracy: 0.6250\n",
      "Iteration 3500 - Loss: 9.9899, Accuracy: 0.5938\n",
      "Iteration 4000 - Loss: 9.3927, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 10.5838, Accuracy: 0.6250\n",
      "Iteration 5000 - Loss: 9.6065, Accuracy: 0.7500\n",
      "Iteration 5500 - Loss: 8.5806, Accuracy: 0.8125\n",
      "Iteration 6000 - Loss: 10.8984, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.7369, Accuracy: 0.8438\n",
      "Fold: 3, Train Loss: 10.3250, Train Accuracy: 0.6745, Val Loss: 9.0704, Val Accuracy: 0.7214\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.2560, Accuracy: 0.7188\n",
      "Iteration 1000 - Loss: 11.5549, Accuracy: 0.5625\n",
      "Iteration 1500 - Loss: 11.4953, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 10.8179, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 10.8523, Accuracy: 0.5312\n",
      "Iteration 3000 - Loss: 10.6981, Accuracy: 0.6562\n",
      "Iteration 3500 - Loss: 11.0062, Accuracy: 0.6250\n",
      "Iteration 4000 - Loss: 9.4906, Accuracy: 0.6562\n",
      "Iteration 4500 - Loss: 8.8484, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 8.5542, Accuracy: 0.6250\n",
      "Iteration 5500 - Loss: 9.4888, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 8.5694, Accuracy: 0.7812\n",
      "Iteration 6500 - Loss: 9.2203, Accuracy: 0.7812\n",
      "Fold: 4, Train Loss: 10.2298, Train Accuracy: 0.6762, Val Loss: 9.2806, Val Accuracy: 0.6998\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.63       0.72       0.67      \n",
      "2          0.50       0.52       0.51      \n",
      "3          0.77       0.55       0.64      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6600 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/ddb35bebc79341f487bacb0720f589ed\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 15.5422, Accuracy: 0.4844\n",
      "Iteration 500 - Loss: 10.8563, Accuracy: 0.6875\n",
      "Iteration 1000 - Loss: 10.9202, Accuracy: 0.6250\n",
      "Iteration 1500 - Loss: 10.8509, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 10.6038, Accuracy: 0.7188\n",
      "Iteration 2500 - Loss: 8.5023, Accuracy: 0.8438\n",
      "Iteration 3000 - Loss: 9.3256, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 9.3520, Accuracy: 0.7344\n",
      "Iteration 4000 - Loss: 9.9252, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.9544, Accuracy: 0.6875\n",
      "Iteration 5000 - Loss: 9.5821, Accuracy: 0.6562\n",
      "Iteration 5500 - Loss: 8.8222, Accuracy: 0.7188\n",
      "Iteration 6000 - Loss: 9.1428, Accuracy: 0.7031\n",
      "Iteration 6500 - Loss: 9.3353, Accuracy: 0.6406\n",
      "Fold: 0, Train Loss: 9.6488, Train Accuracy: 0.6901, Val Loss: 8.6432, Val Accuracy: 0.7355\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.4679, Accuracy: 0.7500\n",
      "Iteration 1000 - Loss: 10.7273, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 10.2988, Accuracy: 0.6250\n",
      "Iteration 2000 - Loss: 10.0939, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 10.0105, Accuracy: 0.6094\n",
      "Iteration 3000 - Loss: 9.5889, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 9.1534, Accuracy: 0.7031\n",
      "Iteration 4000 - Loss: 8.3802, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 9.6540, Accuracy: 0.7188\n",
      "Iteration 5000 - Loss: 9.0114, Accuracy: 0.7344\n",
      "Iteration 5500 - Loss: 8.7030, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.6411, Accuracy: 0.6719\n",
      "Iteration 6500 - Loss: 9.6369, Accuracy: 0.6562\n",
      "Fold: 1, Train Loss: 9.5191, Train Accuracy: 0.6972, Val Loss: 9.0239, Val Accuracy: 0.6970\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.8172, Accuracy: 0.5938\n",
      "Iteration 1000 - Loss: 10.5329, Accuracy: 0.6406\n",
      "Iteration 1500 - Loss: 10.4146, Accuracy: 0.5938\n",
      "Iteration 2000 - Loss: 10.1578, Accuracy: 0.5625\n",
      "Iteration 2500 - Loss: 10.4822, Accuracy: 0.6094\n",
      "Iteration 3000 - Loss: 9.6450, Accuracy: 0.7031\n",
      "Iteration 3500 - Loss: 9.1742, Accuracy: 0.6719\n",
      "Iteration 4000 - Loss: 9.6469, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 9.3088, Accuracy: 0.7656\n",
      "Iteration 5000 - Loss: 8.5306, Accuracy: 0.7656\n",
      "Iteration 5500 - Loss: 9.0108, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.4041, Accuracy: 0.7188\n",
      "Iteration 6500 - Loss: 9.0555, Accuracy: 0.6562\n",
      "Fold: 2, Train Loss: 9.5510, Train Accuracy: 0.6957, Val Loss: 8.9884, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.5000\n",
      "Iteration 500 - Loss: 10.9982, Accuracy: 0.6719\n",
      "Iteration 1000 - Loss: 9.9552, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.7731, Accuracy: 0.7188\n",
      "Iteration 2000 - Loss: 10.7499, Accuracy: 0.5938\n",
      "Iteration 2500 - Loss: 9.0854, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 9.0763, Accuracy: 0.7812\n",
      "Iteration 3500 - Loss: 9.1413, Accuracy: 0.7812\n",
      "Iteration 4000 - Loss: 8.7457, Accuracy: 0.7188\n",
      "Iteration 4500 - Loss: 8.3655, Accuracy: 0.7656\n",
      "Iteration 5000 - Loss: 10.0475, Accuracy: 0.5312\n",
      "Iteration 5500 - Loss: 8.4583, Accuracy: 0.7344\n",
      "Iteration 6000 - Loss: 8.8516, Accuracy: 0.6250\n",
      "Iteration 6500 - Loss: 8.9518, Accuracy: 0.7031\n",
      "Fold: 3, Train Loss: 9.6841, Train Accuracy: 0.6916, Val Loss: 8.5799, Val Accuracy: 0.7186\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 10.9895, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 10.7172, Accuracy: 0.6094\n",
      "Iteration 1500 - Loss: 11.4048, Accuracy: 0.5781\n",
      "Iteration 2000 - Loss: 9.4608, Accuracy: 0.7031\n",
      "Iteration 2500 - Loss: 9.2350, Accuracy: 0.6719\n",
      "Iteration 3000 - Loss: 9.4564, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.6872, Accuracy: 0.7656\n",
      "Iteration 4000 - Loss: 9.3419, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.8871, Accuracy: 0.7500\n",
      "Iteration 5000 - Loss: 9.4354, Accuracy: 0.6250\n",
      "Iteration 5500 - Loss: 10.2029, Accuracy: 0.6562\n",
      "Iteration 6000 - Loss: 8.6566, Accuracy: 0.7656\n",
      "Iteration 6500 - Loss: 8.1778, Accuracy: 0.7500\n",
      "Fold: 4, Train Loss: 9.6022, Train Accuracy: 0.6950, Val Loss: 8.7937, Val Accuracy: 0.7111\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.81       0.81      \n",
      "1          0.64       0.71       0.67      \n",
      "2          0.52       0.53       0.53      \n",
      "3          0.79       0.59       0.68      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6700 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/23807d3e403e484794d88f772e0fd2f4\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | mini_batch | uniform | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 15.5424, Accuracy: 0.4688\n",
      "Iteration 500 - Loss: 10.8406, Accuracy: 0.6328\n",
      "Iteration 1000 - Loss: 9.8680, Accuracy: 0.7031\n",
      "Iteration 1500 - Loss: 9.7211, Accuracy: 0.6484\n",
      "Iteration 2000 - Loss: 8.4884, Accuracy: 0.7578\n",
      "Iteration 2500 - Loss: 8.8668, Accuracy: 0.7031\n",
      "Iteration 3000 - Loss: 9.0084, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 9.0913, Accuracy: 0.7188\n",
      "Iteration 4000 - Loss: 8.5432, Accuracy: 0.7578\n",
      "Iteration 4500 - Loss: 8.0956, Accuracy: 0.7812\n",
      "Iteration 5000 - Loss: 8.7008, Accuracy: 0.7188\n",
      "Iteration 5500 - Loss: 7.7086, Accuracy: 0.8047\n",
      "Iteration 6000 - Loss: 7.8957, Accuracy: 0.7692\n",
      "Iteration 6500 - Loss: 9.1115, Accuracy: 0.7109\n",
      "Fold: 0, Train Loss: 9.1355, Train Accuracy: 0.7011, Val Loss: 8.2596, Val Accuracy: 0.7345\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5234\n",
      "Iteration 500 - Loss: 10.0272, Accuracy: 0.7031\n",
      "Iteration 1000 - Loss: 9.9461, Accuracy: 0.6719\n",
      "Iteration 1500 - Loss: 9.2374, Accuracy: 0.7109\n",
      "Iteration 2000 - Loss: 9.4270, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 9.4886, Accuracy: 0.6484\n",
      "Iteration 3000 - Loss: 8.8551, Accuracy: 0.7109\n",
      "Iteration 3500 - Loss: 8.4651, Accuracy: 0.7422\n",
      "Iteration 4000 - Loss: 8.9201, Accuracy: 0.7031\n",
      "Iteration 4500 - Loss: 8.5475, Accuracy: 0.7422\n",
      "Iteration 5000 - Loss: 8.6849, Accuracy: 0.7109\n",
      "Iteration 5500 - Loss: 8.5843, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 8.2514, Accuracy: 0.6953\n",
      "Iteration 6500 - Loss: 7.9895, Accuracy: 0.7578\n",
      "Fold: 1, Train Loss: 9.0041, Train Accuracy: 0.7089, Val Loss: 8.6134, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 15.5425, Accuracy: 0.4922\n",
      "Iteration 500 - Loss: 11.0464, Accuracy: 0.6328\n",
      "Iteration 1000 - Loss: 8.8827, Accuracy: 0.7422\n",
      "Iteration 1500 - Loss: 9.1335, Accuracy: 0.7656\n",
      "Iteration 2000 - Loss: 8.6737, Accuracy: 0.7109\n",
      "Iteration 2500 - Loss: 9.0085, Accuracy: 0.7109\n",
      "Iteration 3000 - Loss: 8.9448, Accuracy: 0.6875\n",
      "Iteration 3500 - Loss: 8.6668, Accuracy: 0.7266\n",
      "Iteration 4000 - Loss: 8.5601, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 7.9834, Accuracy: 0.7344\n",
      "Iteration 5000 - Loss: 7.8105, Accuracy: 0.7812\n",
      "Iteration 5500 - Loss: 8.8192, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 9.1720, Accuracy: 0.6797\n",
      "Iteration 6500 - Loss: 8.3164, Accuracy: 0.7422\n",
      "Fold: 2, Train Loss: 8.9974, Train Accuracy: 0.7083, Val Loss: 8.6001, Val Accuracy: 0.6998\n",
      "Iteration 0 - Loss: 15.5423, Accuracy: 0.5312\n",
      "Iteration 500 - Loss: 10.5408, Accuracy: 0.6328\n",
      "Iteration 1000 - Loss: 10.2210, Accuracy: 0.6562\n",
      "Iteration 1500 - Loss: 9.5952, Accuracy: 0.6875\n",
      "Iteration 2000 - Loss: 9.2347, Accuracy: 0.7109\n",
      "Iteration 2500 - Loss: 8.9580, Accuracy: 0.7266\n",
      "Iteration 3000 - Loss: 9.3470, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 8.9208, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 8.9826, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 9.0348, Accuracy: 0.6719\n",
      "Iteration 5000 - Loss: 8.4742, Accuracy: 0.7734\n",
      "Iteration 5500 - Loss: 8.6134, Accuracy: 0.6875\n",
      "Iteration 6000 - Loss: 8.8664, Accuracy: 0.6875\n",
      "Iteration 6500 - Loss: 8.6795, Accuracy: 0.6797\n",
      "Fold: 3, Train Loss: 9.1378, Train Accuracy: 0.7034, Val Loss: 8.2414, Val Accuracy: 0.7251\n",
      "Iteration 0 - Loss: 15.5427, Accuracy: 0.5156\n",
      "Iteration 500 - Loss: 10.8412, Accuracy: 0.6562\n",
      "Iteration 1000 - Loss: 9.1721, Accuracy: 0.7266\n",
      "Iteration 1500 - Loss: 8.7599, Accuracy: 0.7500\n",
      "Iteration 2000 - Loss: 9.2173, Accuracy: 0.6875\n",
      "Iteration 2500 - Loss: 8.3016, Accuracy: 0.7500\n",
      "Iteration 3000 - Loss: 9.3549, Accuracy: 0.7188\n",
      "Iteration 3500 - Loss: 9.1456, Accuracy: 0.6797\n",
      "Iteration 4000 - Loss: 8.3340, Accuracy: 0.7344\n",
      "Iteration 4500 - Loss: 8.8274, Accuracy: 0.7422\n",
      "Iteration 5000 - Loss: 8.9196, Accuracy: 0.6719\n",
      "Iteration 5500 - Loss: 8.5810, Accuracy: 0.7109\n",
      "Iteration 6000 - Loss: 8.5829, Accuracy: 0.7246\n",
      "Iteration 6500 - Loss: 7.8185, Accuracy: 0.7422\n",
      "Fold: 4, Train Loss: 9.0589, Train Accuracy: 0.7061, Val Loss: 8.4123, Val Accuracy: 0.7167\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.81       0.82      \n",
      "1          0.64       0.70       0.67      \n",
      "2          0.54       0.55       0.55      \n",
      "3          0.81       0.62       0.70      \n",
      " Model: MyLogisticRegression | Method: mini_batch | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-mini_batch-lr-0.0001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/2918971136a64d179c0ba24cb3ade3ec\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6773, Val Loss: 8.3472, Val Accuracy: 0.7129\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6853, Val Loss: 8.9502, Val Accuracy: 0.6660\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6839, Val Loss: 8.5689, Val Accuracy: 0.6735\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6793, Val Loss: 8.1167, Val Accuracy: 0.6857\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6857, Val Loss: 8.5079, Val Accuracy: 0.7026\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.82       0.82      \n",
      "1          0.68       0.63       0.65      \n",
      "2          0.54       0.60       0.57      \n",
      "3          0.73       0.68       0.70      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6800 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/dd627290a48b49339341d8cc5870d2fa\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6796, Val Loss: 8.5420, Val Accuracy: 0.6932\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6870, Val Loss: 8.7014, Val Accuracy: 0.6961\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6871, Val Loss: 8.8047, Val Accuracy: 0.6454\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6824, Val Loss: 8.3314, Val Accuracy: 0.7008\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6811, Val Loss: 8.4663, Val Accuracy: 0.7008\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.77       0.85       0.81      \n",
      "1          0.67       0.58       0.63      \n",
      "2          0.51       0.70       0.59      \n",
      "3          0.88       0.48       0.62      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6600 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/aeb54588180040f5ba69ffb933bd76d9\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6714, Val Loss: 8.3407, Val Accuracy: 0.7073\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6814, Val Loss: 8.7972, Val Accuracy: 0.6773\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6867, Val Loss: 8.6009, Val Accuracy: 0.6820\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6864, Val Loss: 8.3851, Val Accuracy: 0.7045\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6784, Val Loss: 8.4742, Val Accuracy: 0.7017\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.79       0.84       0.82      \n",
      "1          0.68       0.65       0.66      \n",
      "2          0.54       0.43       0.48      \n",
      "3          0.63       0.80       0.70      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6700 | F1-Score: 0.6700\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/fece87cb8b8e4f2796811364df3961cb\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6183, Val Loss: 10.1478, Val Accuracy: 0.7064\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6059, Val Loss: 10.7128, Val Accuracy: 0.6238\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6030, Val Loss: 10.5561, Val Accuracy: 0.6341\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5881, Val Loss: 10.1636, Val Accuracy: 0.6717\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6310, Val Loss: 10.2806, Val Accuracy: 0.6501\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.75       0.78      \n",
      "1          0.57       0.78       0.66      \n",
      "2          0.49       0.31       0.38      \n",
      "3          0.69       0.60       0.64      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/49dc80a9ddae4d60856ece16ed9ed2f8\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6094, Val Loss: 10.1785, Val Accuracy: 0.6979\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5927, Val Loss: 10.6985, Val Accuracy: 0.6304\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5994, Val Loss: 10.5764, Val Accuracy: 0.6388\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5993, Val Loss: 10.1437, Val Accuracy: 0.6848\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6150, Val Loss: 10.2283, Val Accuracy: 0.6529\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.78       0.81      \n",
      "1          0.57       0.81       0.67      \n",
      "2          0.46       0.27       0.34      \n",
      "3          0.71       0.58       0.64      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6400 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/ab624145b9bf495eb0c82927eba190be\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6036, Val Loss: 10.3542, Val Accuracy: 0.6989\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6169, Val Loss: 10.6506, Val Accuracy: 0.6182\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6039, Val Loss: 10.5133, Val Accuracy: 0.6295\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6187, Val Loss: 10.0464, Val Accuracy: 0.6773\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6070, Val Loss: 10.3684, Val Accuracy: 0.6623\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.79       0.81      \n",
      "1          0.59       0.73       0.65      \n",
      "2          0.46       0.34       0.39      \n",
      "3          0.66       0.60       0.63      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6300\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/ca4a460a1c7d4413a232ebf1ca22ea08\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.4051, Val Loss: 12.9007, Val Accuracy: 0.4953\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3700, Val Loss: 13.3270, Val Accuracy: 0.4803\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.3661, Val Loss: 13.2807, Val Accuracy: 0.5169\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.4029, Val Loss: 13.0436, Val Accuracy: 0.5563\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.4281, Val Loss: 13.0147, Val Accuracy: 0.5141\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.77       0.67       0.72      \n",
      "1          0.42       0.72       0.53      \n",
      "2          0.31       0.10       0.15      \n",
      "3          0.45       0.30       0.36      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5000 | F1-Score: 0.4600\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-xavier-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/f248a91f39904063a1242f68dba1cd67\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.3833, Val Loss: 13.1971, Val Accuracy: 0.4953\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3973, Val Loss: 13.1353, Val Accuracy: 0.4615\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.4080, Val Loss: 13.0467, Val Accuracy: 0.4869\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.4391, Val Loss: 12.9144, Val Accuracy: 0.5563\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.4566, Val Loss: 12.7126, Val Accuracy: 0.5450\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.68       0.67       0.67      \n",
      "1          0.48       0.64       0.54      \n",
      "2          0.37       0.24       0.29      \n",
      "3          0.57       0.45       0.50      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5200 | F1-Score: 0.5100\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-xavier-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/525a0b4dc3a04aefa38fbf743330ed96\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | xavier | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.3994, Val Loss: 13.1030, Val Accuracy: 0.5272\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.4233, Val Loss: 13.0274, Val Accuracy: 0.4934\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.3987, Val Loss: 12.7381, Val Accuracy: 0.5000\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.4654, Val Loss: 12.7462, Val Accuracy: 0.5553\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.4609, Val Loss: 13.1170, Val Accuracy: 0.5469\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.56       0.66      \n",
      "1          0.48       0.77       0.59      \n",
      "2          0.36       0.22       0.27      \n",
      "3          0.49       0.38       0.43      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5300 | F1-Score: 0.5100\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-xavier-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/dd0be8d158e14081a8fc17a791e3c061\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6681, Val Loss: 8.3114, Val Accuracy: 0.6923\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6951, Val Loss: 8.5848, Val Accuracy: 0.6961\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6836, Val Loss: 9.0515, Val Accuracy: 0.6548\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6700, Val Loss: 8.4770, Val Accuracy: 0.6848\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6777, Val Loss: 8.4089, Val Accuracy: 0.6998\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.74       0.87       0.80      \n",
      "1          0.60       0.69       0.64      \n",
      "2          0.53       0.32       0.40      \n",
      "3          0.72       0.70       0.71      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6500 | F1-Score: 0.6400\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/978608b4131645f98d296f6c3a183f62\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6674, Val Loss: 8.1351, Val Accuracy: 0.7233\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6784, Val Loss: 8.7239, Val Accuracy: 0.6642\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6891, Val Loss: 8.6716, Val Accuracy: 0.6754\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6691, Val Loss: 8.5212, Val Accuracy: 0.6801\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6809, Val Loss: 8.6224, Val Accuracy: 0.6773\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.76       0.82       0.79      \n",
      "1          0.64       0.64       0.64      \n",
      "2          0.52       0.51       0.52      \n",
      "3          0.72       0.62       0.67      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6500 | F1-Score: 0.6500\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/6e9974fb015e44a8bd9227b2aee92d16\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6740, Val Loss: 8.1886, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6857, Val Loss: 8.8158, Val Accuracy: 0.6313\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6861, Val Loss: 8.5260, Val Accuracy: 0.6820\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6717, Val Loss: 8.2955, Val Accuracy: 0.7036\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6753, Val Loss: 8.5086, Val Accuracy: 0.6820\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.77       0.80      \n",
      "1          0.58       0.77       0.66      \n",
      "2          0.52       0.34       0.41      \n",
      "3          0.74       0.67       0.70      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6500 | F1-Score: 0.6400\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/f993b8b847094ba2b5649fd4e5b82369\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5581, Val Loss: 9.8715, Val Accuracy: 0.6604\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5739, Val Loss: 10.5997, Val Accuracy: 0.6088\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5560, Val Loss: 10.4282, Val Accuracy: 0.6538\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5379, Val Loss: 10.0213, Val Accuracy: 0.6435\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5223, Val Loss: 10.4724, Val Accuracy: 0.6445\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.81       0.79       0.80      \n",
      "1          0.56       0.76       0.65      \n",
      "2          0.49       0.32       0.39      \n",
      "3          0.72       0.55       0.62      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/51f23e0fa35f4b1aad3eb1c05de4852a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5214, Val Loss: 10.3806, Val Accuracy: 0.6417\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6149, Val Loss: 10.2452, Val Accuracy: 0.6323\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5486, Val Loss: 10.4497, Val Accuracy: 0.6098\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6050, Val Loss: 9.9712, Val Accuracy: 0.6820\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5346, Val Loss: 10.4915, Val Accuracy: 0.6276\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.74       0.83       0.78      \n",
      "1          0.57       0.60       0.59      \n",
      "2          0.45       0.51       0.48      \n",
      "3          0.81       0.42       0.55      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6100 | F1-Score: 0.6100\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/8ec6f752a9ed47168401231c47be30d6\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5277, Val Loss: 10.4702, Val Accuracy: 0.6126\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5150, Val Loss: 10.6648, Val Accuracy: 0.5882\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5639, Val Loss: 10.5827, Val Accuracy: 0.6163\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5141, Val Loss: 10.3239, Val Accuracy: 0.6482\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5413, Val Loss: 10.6020, Val Accuracy: 0.6220\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.74       0.79      \n",
      "1          0.53       0.84       0.65      \n",
      "2          0.46       0.22       0.30      \n",
      "3          0.68       0.50       0.58      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6100 | F1-Score: 0.5900\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/1d0c7b08cf974447bcaf4c3e38cb4e3a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.2969, Val Loss: 13.1020, Val Accuracy: 0.4165\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3260, Val Loss: 13.0151, Val Accuracy: 0.4174\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.3064, Val Loss: 13.2069, Val Accuracy: 0.3996\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.2563, Val Loss: 13.5995, Val Accuracy: 0.3565\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.2574, Val Loss: 13.6885, Val Accuracy: 0.4081\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.55       0.49       0.52      \n",
      "1          0.39       0.32       0.35      \n",
      "2          0.28       0.46       0.35      \n",
      "3          0.40       0.27       0.32      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.3900 | F1-Score: 0.3900\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-normal-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/010d712ad772472f9b4ccf2497125077\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.3349, Val Loss: 13.4316, Val Accuracy: 0.3705\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3101, Val Loss: 13.6835, Val Accuracy: 0.3846\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.2440, Val Loss: 13.9688, Val Accuracy: 0.3668\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.3204, Val Loss: 13.8040, Val Accuracy: 0.4522\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.3633, Val Loss: 13.9038, Val Accuracy: 0.3884\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.76       0.26       0.39      \n",
      "1          0.38       0.80       0.51      \n",
      "2          0.29       0.13       0.18      \n",
      "3          0.40       0.16       0.23      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.4000 | F1-Score: 0.3600\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-normal-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/71c8ee66388047d7944397c6fc4c1457\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | normal | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.3294, Val Loss: 13.6584, Val Accuracy: 0.3912\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.3877, Val Loss: 12.8280, Val Accuracy: 0.4756\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.3250, Val Loss: 13.2832, Val Accuracy: 0.4268\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.1784, Val Loss: 14.6940, Val Accuracy: 0.2964\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.2434, Val Loss: 13.8118, Val Accuracy: 0.3321\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.19       0.16       0.17      \n",
      "1          0.38       0.44       0.41      \n",
      "2          0.31       0.36       0.33      \n",
      "3          0.72       0.45       0.55      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.3500 | F1-Score: 0.3500\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-normal-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/c046e841225442f986e1ceadfec6a4ef\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.01 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6797, Val Loss: 8.2946, Val Accuracy: 0.7120\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6796, Val Loss: 8.8706, Val Accuracy: 0.6689\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6920, Val Loss: 8.8008, Val Accuracy: 0.6614\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6856, Val Loss: 8.3569, Val Accuracy: 0.6886\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6821, Val Loss: 9.1303, Val Accuracy: 0.6501\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.89       0.67       0.76      \n",
      "1          0.61       0.75       0.67      \n",
      "2          0.47       0.64       0.54      \n",
      "3          0.86       0.32       0.47      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6400 | F1-Score: 0.6300\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/d04cb6c8d8ac48678921c0d755f87d19\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.01 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6804, Val Loss: 8.1819, Val Accuracy: 0.7176\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6884, Val Loss: 8.5817, Val Accuracy: 0.6904\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6923, Val Loss: 8.6592, Val Accuracy: 0.6426\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6876, Val Loss: 8.6252, Val Accuracy: 0.6529\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6797, Val Loss: 8.4927, Val Accuracy: 0.7026\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.85       0.76       0.80      \n",
      "1          0.65       0.70       0.67      \n",
      "2          0.53       0.56       0.55      \n",
      "3          0.70       0.64       0.67      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6700 | F1-Score: 0.6800\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/7dcceb8f25f74f1c982623abe9e884a6\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.01 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6853, Val Loss: 8.7602, Val Accuracy: 0.7026\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6917, Val Loss: 9.2328, Val Accuracy: 0.6651\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.7014, Val Loss: 8.6477, Val Accuracy: 0.6998\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6827, Val Loss: 8.3293, Val Accuracy: 0.6876\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6971, Val Loss: 8.4538, Val Accuracy: 0.7026\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.82       0.81      \n",
      "1          0.68       0.63       0.65      \n",
      "2          0.50       0.73       0.60      \n",
      "3          0.94       0.40       0.56      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6700 | F1-Score: 0.6600\n",
      "ðŸƒ View run method-stochastic-lr-0.01-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/44fc3fd11df54c1fa9ef581c5da8937b\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6286, Val Loss: 10.1728, Val Accuracy: 0.6717\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6417, Val Loss: 10.6265, Val Accuracy: 0.6276\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6363, Val Loss: 10.5223, Val Accuracy: 0.6707\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6281, Val Loss: 10.2721, Val Accuracy: 0.6801\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6279, Val Loss: 10.4214, Val Accuracy: 0.6585\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.80       0.82       0.81      \n",
      "1          0.58       0.71       0.64      \n",
      "2          0.46       0.34       0.39      \n",
      "3          0.68       0.59       0.63      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6200\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/9d137d63a33147dfa24cfcec30e20bdd\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6303, Val Loss: 10.1495, Val Accuracy: 0.6689\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6444, Val Loss: 10.7174, Val Accuracy: 0.6257\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6284, Val Loss: 10.5332, Val Accuracy: 0.6510\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6271, Val Loss: 10.1068, Val Accuracy: 0.6726\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6273, Val Loss: 10.4188, Val Accuracy: 0.6370\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.84       0.74       0.79      \n",
      "1          0.54       0.86       0.66      \n",
      "2          0.50       0.18       0.26      \n",
      "3          0.71       0.58       0.64      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6300 | F1-Score: 0.6000\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/3c912a1701674da7a6cabc4f3339c7fc\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.6299, Val Loss: 10.1477, Val Accuracy: 0.6773\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.6413, Val Loss: 10.6114, Val Accuracy: 0.6323\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.6357, Val Loss: 10.4900, Val Accuracy: 0.6492\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.6327, Val Loss: 10.1449, Val Accuracy: 0.6961\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.6343, Val Loss: 10.4344, Val Accuracy: 0.6548\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.80       0.81      \n",
      "1          0.60       0.76       0.67      \n",
      "2          0.47       0.39       0.42      \n",
      "3          0.74       0.49       0.59      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.6400 | F1-Score: 0.6400\n",
      "ðŸƒ View run method-stochastic-lr-0.001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/063a3405e8364a1b9f8b5a6c0882fab1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.0001 | l2: None | Batch Size: 32\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5400, Val Loss: 12.9189, Val Accuracy: 0.5872\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5617, Val Loss: 13.0430, Val Accuracy: 0.5553\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5544, Val Loss: 12.9998, Val Accuracy: 0.5685\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5514, Val Loss: 12.8463, Val Accuracy: 0.6257\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5494, Val Loss: 13.0305, Val Accuracy: 0.5910\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.82       0.71       0.76      \n",
      "1          0.48       0.82       0.60      \n",
      "2          0.43       0.10       0.16      \n",
      "3          0.62       0.47       0.54      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5700 | F1-Score: 0.5300\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-uniform-batch-32 at: http://127.0.0.1:5000/#/experiments/1/runs/15895ded07f74e88a8fde5b6b1a39f0a\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.0001 | l2: None | Batch Size: 64\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5534, Val Loss: 12.8747, Val Accuracy: 0.6041\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5597, Val Loss: 13.0772, Val Accuracy: 0.5460\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5606, Val Loss: 12.9783, Val Accuracy: 0.5826\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5517, Val Loss: 12.7880, Val Accuracy: 0.6201\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5471, Val Loss: 13.0358, Val Accuracy: 0.5966\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.83       0.71       0.76      \n",
      "1          0.49       0.83       0.61      \n",
      "2          0.45       0.12       0.19      \n",
      "3          0.62       0.49       0.55      \n",
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5800 | F1-Score: 0.5400\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-uniform-batch-64 at: http://127.0.0.1:5000/#/experiments/1/runs/9ac3606873984199a1819324f285b19c\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Running MyLogisticRegression | stochastic | uniform | LR: 0.0001 | l2: None | Batch Size: 128\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 0, Train Loss: 0.2500, Train Accuracy: 0.5496, Val Loss: 12.8875, Val Accuracy: 0.6041\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 1, Train Loss: 0.2500, Train Accuracy: 0.5511, Val Loss: 13.0902, Val Accuracy: 0.5497\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 2, Train Loss: 0.2500, Train Accuracy: 0.5613, Val Loss: 12.9888, Val Accuracy: 0.5600\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 3, Train Loss: 0.2500, Train Accuracy: 0.5613, Val Loss: 12.8326, Val Accuracy: 0.6060\n",
      "Iteration 0 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 1000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 1500 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 2000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 2500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 3500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 4000 - Loss: 0.2500, Accuracy: 0.0000\n",
      "Iteration 4500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 5500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6000 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Iteration 6500 - Loss: 0.2500, Accuracy: 1.0000\n",
      "Fold: 4, Train Loss: 0.2500, Train Accuracy: 0.5487, Val Loss: 13.0131, Val Accuracy: 0.6023\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "Class      Precision  Recall     F1-score  \n",
      "----------------------------------------\n",
      "0          0.77       0.75       0.76      \n",
      "1          0.48       0.77       0.59      \n",
      "2          0.41       0.13       0.20      \n",
      "3          0.63       0.49       0.55      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'st124783-a3-model'.\n",
      "2025/03/17 23:19:46 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: st124783-a3-model, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model: MyLogisticRegression | Method: stochastic | Accuracy: 0.5700 | F1-Score: 0.5400\n",
      "ðŸƒ View run method-stochastic-lr-0.0001-weight-uniform-batch-128 at: http://127.0.0.1:5000/#/experiments/1/runs/9143f88ef54f4ddcae05b4dc3833cf9f\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Best Model by Accuracy: <__main__.MyLogisticRegression object at 0x16595b470> with Accuracy: 0.7000\n",
      "\n",
      " Registered Model: st124783-a3-model (Version: <ModelVersion: aliases=[], creation_timestamp=1742228386680, current_stage='None', description='', last_updated_timestamp=1742228386680, name='st124783-a3-model', run_id='1d7dc3517be246b1ba404c4fa7ae335e', run_link='', source='/Users/binit/PycharmProjects/ML_AIT_A3/mlruns/1/1d7dc3517be246b1ba404c4fa7ae335e/artifacts/model', status='READY', status_message=None, tags={}, user_id='', version='1'>)\n",
      "Model Version 1 is now in 'Staging'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'st124783-a3-model'.\n",
      "/var/folders/8j/ndxdwkv11mg2yfgyrfgqxrwr0000gn/T/ipykernel_15615/3042286121.py:89: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest_version = client.get_latest_versions(model_name, stages=[\"None\"])[0].version\n",
      "/var/folders/8j/ndxdwkv11mg2yfgyrfgqxrwr0000gn/T/ipykernel_15615/3042286121.py:91: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(name=model_name, version=latest_version, stage=\"Staging\")\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:23:56.925021Z",
     "start_time": "2025-03-17T16:23:56.923304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Best Model \n",
    "print(best_acc, best_model_acc, best_params_acc)"
   ],
   "id": "2a208c5be0180403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 <__main__.MyLogisticRegression object at 0x16595b470> {'method': 'batch', 'lr': 0.001, 'weight_init': 'xavier', 'l2': None, 'batch_size': 64}\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:38:56.691384Z",
     "start_time": "2025-03-18T17:38:56.687760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cloudpickle\n",
    "\n",
    "# Suppose 'preprocessor' is your pipeline or other object\n",
    "with open(\"preprocess_test.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(preprocessor, f)\n",
    "\n",
    "print(\"Successfully saved preprocessor to preprocess_test.pkl with cloudpickle.\")"
   ],
   "id": "22ec1581a8d8e497",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved preprocessor to preprocess_test.pkl with cloudpickle.\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:39:38.126645Z",
     "start_time": "2025-03-18T17:39:38.118524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"model_test.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(model, f)\n",
    "\n",
    "print(\"Successfully saved model to model_test.pkl with cloudpickle.\")"
   ],
   "id": "2d06fc30e3283aec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model to model_test.pkl with cloudpickle.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:55:44.903322Z",
     "start_time": "2025-03-18T17:55:44.901439Z"
    }
   },
   "cell_type": "code",
   "source": "print(cloudpickle.__version__)",
   "id": "67724ceced97d66e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook implements a custom logistic regression model for car price classification. It covers:\n",
    "\t- Data preprocessing (feature engineering, scaling, encoding)\n",
    "\t- Model training using various gradient descent methods\n",
    "\t- Performance evaluation with metrics\n",
    "\t- Hyperparameter tuning and model tracking with MLflow\n",
    "\t- Model serialization for deployment\n",
    "\n",
    "This structured pipeline ensures an end-to-end workflow for building and optimizing a machine learning classification model. The custom MyLogisticRegression class implements a multinomial logistic regression model with gradient-based optimization techniques. It supports three training methods: mini-batch, batch, and stochastic gradient descent, allowing flexible learning strategies. The model includes custom weight initialization (uniform, normal, Xavier) and L2 regularization to prevent overfitting. It uses one-hot encoding for categorical target variables and applies softmax activation for multi-class classification. The training process incorporates cross-validation (KFold), tracks losses and accuracies per fold, and logs performance metrics for evaluation. Finally, MLflow is used for hyperparameter tuning and model tracking, ensuring systematic experimentation and model selection."
   ],
   "id": "7a93aeb79c1736af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ml Flow\n",
    "Experiment Tracking â†’ I Logged hyperparameters and metrics to compare models that helped me to see my best model\n",
    "Easy Deployment â†’ Models in Staging can be transitioned to Production automatically.But here i did till staging only. I found out mlflow Tracks model sources, making debugging easier too this helps to improve model performance overtime too. "
   ],
   "id": "ed05fbd640600662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "557a950ed0ecaf7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
